{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913d2a2-f87f-4505-abb4-49e6b2291e52",
   "metadata": {},
   "source": [
    "# Part 1: Querying\n",
    "\n",
    "PyData Amsterdam 2023\n",
    "\n",
    "* Tutorial: Building a personal search engine with llama-index\n",
    "* Speakers: Judith van Stegeren and Yorick van Pelt\n",
    "* Company: [Datakami](www.datakami.nl)\n",
    "\n",
    "## Prep\n",
    "\n",
    "- Install all the prerequisities using TODO\n",
    "- If you want to use the OpenAPI API during this tutorial, make a file `secret.py` with `openai_api_key = \"YOURKEYHERE\"`\n",
    "- Run all cells under 'Setup'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb43be-6be1-4cd1-933f-cd3a77052e33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb21f09-827a-412c-8b19-c40fd14825a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd39ab-b6ba-4d69-8e8f-39f382d7712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# logging for lazy people :)\n",
    "from loguru import logger\n",
    "\n",
    "# we're not importing specific methods or classes so it's clear when we actually call llama_index!\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b056809-10bc-4f4d-90bf-a9b8e8acd34e",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44daa1-a3e6-45bf-bc6e-8c84fad646b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"{time} - {level} - {message}\", level=\"DEBUG\")\n",
    "logger.add(\"tutorial_part_1.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e8b1-5b94-4dcf-b109-9e8351b087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/pydata/schedule.json\")\n",
    "INDEX_PATH = Path(\"indices/pydata_schedule_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f10b1-759c-459b-bece-1495c099cf62",
   "metadata": {},
   "source": [
    "### Tell `llama-index` to use a local embeddings model for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b220f-e86c-48e6-a069-ec4d84a2b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More information about this embeddings model: https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "# all-minilm-l6-v2 has a maximum size of 256 tokens\n",
    "embed_model = \"local:sentence-transformers/all-minilm-l6-v2\"\n",
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531cae-9ec3-47f3-b4c1-e9df4cea39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=embed_model, chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b317df6-0c8c-4ec8-beb9-ce932bc34a50",
   "metadata": {},
   "source": [
    "### Load a vector index with the PyData Amsterdam 2023 schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bc3e5-6bc7-4970-9a1f-c722ba931956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-23T16:19:43.182350+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "# load vector index from file\n",
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede9a40-f6e1-42f3-92bf-4519bfad87ef",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e4d89-3dea-4809-b073-a4c5efc03ab4",
   "metadata": {},
   "source": [
    "### Create a search engine from the vector index `index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d0cad-f3bb-4467-b3ed-ec87e5fa966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a search engine\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27822ef-0958-4460-abf5-c98a2c9a7dfb",
   "metadata": {},
   "source": [
    "### Retrieve talks that mention llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343d465-8b22-4778-91e7-b33f3c63e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the search engine\n",
    "results = retriever.retrieve(\"llama_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741e34d-0edb-4aae-97d6-f1760ba8e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node=TextNode(id_='0790cffb-67fa-4be4-b668-61b6f75b195a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f71f1eae-c12e-5b4f-8902-b8d0b8a38d72', node_type=None, metadata={}, hash='06987f3dd83522f1b914565902298eb9eed00f77d5fa47d3d52c5f4301959f54'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7d194c54-99e5-4011-8cd0-51b77b1274e6', node_type=None, metadata={}, hash='3afcb14eb9ca291a920e6ec82ff895103024cb798781880f8f09ee4e80c725c0')}, hash='c852556384af48ec95a06b58d93753903d7a40cf65713b2ce35ef85583de0ac8', text='Building a personal search engine with llama-index\\n\\nWouldn’t it be great to have a Google-like search engine, but then for your own text files and completely private?In this tutorial we’ll build a small personal search engine using open source library llama-index.In this tutorial we will build a small personal search engine using open source library `llama-index`.Llama-index provides utility functions for ingesting various kinds of data, breaking the data up in chunks, building an index of that data using vector embeddings, and retrieving data from the index based on queries.We can even use llama-index to post-process the retrieval results for us using large language models such as GPT.The target audience is people that are already familiar with Python.Participants will experience working with unstructured data, vector embeddings, and explore the possibilities of the recent developments in natural language processing.Workshop materials will be provided via Github before the start of the workshop.For the demo application, we will only use open-source software and models that are light enough to run on an average laptop without a GPU.Using llama-index with OpenAI’s API is optional.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.35425657295491564\n",
      "\n",
      "node=TextNode(id_='fafa26d4-a1ee-4fcc-9bf9-ff0a3d4857ad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3b32bca8-108d-5c64-9dd4-d577b0d284f6', node_type=None, metadata={}, hash='7d3cd326f2833ac4115a0d9a2c614b25cfdcfc7ec2eaebb6c382e32a2424ed78')}, hash='7d3cd326f2833ac4115a0d9a2c614b25cfdcfc7ec2eaebb6c382e32a2424ed78', text='Unconference #1\\n\\nLorem ipsum dolor\\n\\n1143', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.24314365635632973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4201f4-1388-47ee-a5ef-52b83c47f538",
   "metadata": {},
   "source": [
    "### Retrieve talks about causal machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01462a67-ebcc-4282-a7c0-5fadb0cc75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node=TextNode(id_='3667cdc1-476c-45bc-b991-af941fb56858', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8', node_type=None, metadata={}, hash='5a52c60ee9f4c8bd3f85be362bf7abaf161cf24ca442c226d83ca7ec2f307177'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='468f946b-6bdf-445d-b45b-c342364ac4af', node_type=None, metadata={}, hash='62eacf013c2fd781932f8ab4ae72aabd9021bfd4292803138792b918fdbcc883')}, hash='939bab04cf1ba76e562e5c9f7aa37bc9550dd61f6a7555cbf72554a260ea115b', text=\"The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls\\n\\nData scientists and analysts are using quasi-experimental methods to make recommendations based on causality instead of randomized control trials.While these methods are easy to use, their assumptions can be complex to explain.This talk will explain these assumptions for data scientists and analysts without in-depth training of causal inference so they can use and explain these methods more confidently to change people's minds using data.Instead of relying solely on randomized control trials (also known as A/B tests), which are considered the gold standard for inferring causality, data scientists and analysts are increasingly turning to quasi-experimental methods to make recommendations based on causality.These methods, including open-source libraries such as CausalImpact (originally an R package but with numerous Python ports), are easy to use, but their assumptions can be complex to explain.I will break down these assumptions and explain how they can help practitioners determine when to use these methods (and when not to use them), using examples from the world of digital language learning.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.510425310010518\n",
      "\n",
      "node=TextNode(id_='8109ce79-60c3-451b-8bef-eebe71169740', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7b6015d4-f08c-5df1-a904-965815a3180b', node_type=None, metadata={}, hash='ce6783e463d1f15130807c86717e3b9b41031d0530956a662a0db6db0e28aa65'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fa28e565-77ae-4cfd-835c-eea1fbb2a944', node_type=None, metadata={}, hash='cd20a1e92aa66dd08209952dea4f5bad2e3aa2e2caa82da98b239637381467b4')}, hash='14f478ef8710e0ccf68d2bcca03ac5d3fa4826380aeec0c29e8183c6ab68d333', text='Attendees will gain an understanding of causal inference through observational studies and specifically how the new DiD methods are used through an interesting and original case study.The talk will be structured as follows:\\r\\n\\r\\n1.Intro to the case (e.g., background on music features on TV, dataset) (5 mins)\\r\\n2.Explanation of the DiD approach and its limitations.(5 mins)\\r\\n3.Introduction to the Staggered DiD method.(5 mins)\\r\\n4.Application of staggered DiD for the case study from the music industry (10 mins)\\r\\n5.Conclusions (5 mins)\\r\\n\\r\\nTarget Audience: The talk would be beneficial for data scientists, researchers, and practitioners interested in causal inference, marketing analytics, and quasi-experimental design.Attendees should have a basic understanding of statistical methods used in data science.Key Takeaways:\\r\\n\\r\\n1.Understanding of the DiD approach and its limitations in the context of analyses with observational data.2.Insights into the Staggered DiD method and its application.3.Practical knowledge about executing and evaluating DiD studies effectively.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.43695903577868206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = retriever.retrieve(\"causal\")\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f7501-8e58-463d-acf1-0a1c37274238",
   "metadata": {},
   "source": [
    "### Create a new retriever that retrieves more than 2 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b98123-b560-4bc9-b611-3f2db8705262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: https://gpt-index.readthedocs.io/en/v0.8.5.post2/api_reference/query/retrievers/vector_store.html\n",
    "retriever = index.as_retriever(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e9016-74d1-477c-be7c-d1ea43ac6456",
   "metadata": {},
   "source": [
    "### Find all talks about causal inference at PyData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f418a4-19d4-4ec0-b102-714de283c7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls\n",
      "\n",
      "Data scientists and analysts are using quasi-experimental methods to make recommendations based on causality instead of randomized control trials.While these methods are easy to use, their assumptions can be complex to explain.This talk will explain these assumptions for data scientists and analysts without in-depth training of causal inference so they can use and explain these methods more confidently to change people's minds using data.Instead of relying solely on randomized control trials (also known as A/B tests), which are considered the gold standard for inferring causality, data scientists and analysts are increasingly turning to quasi-experimental methods to make recommendations based on causality.These methods, including open-source libraries such as CausalImpact (originally an R package but with numerous Python ports), are easy to use, but their assumptions can be complex to explain.I will break down these assumptions and explain how they can help practitioners determine when to use these methods (and when not to use them), using examples from the world of digital language learning.\n",
      "---------------\n",
      "Attendees will gain an understanding of causal inference through observational studies and specifically how the new DiD methods are used through an interesting and original case study.The talk will be structured as follows:\n",
      "\n",
      "1.Intro to the case (e.g., background on music features on TV, dataset) (5 mins)\n",
      "2.Explanation of the DiD approach and its limitations.(5 mins)\n",
      "3.Introduction to the Staggered DiD method.(5 mins)\n",
      "4.Application of staggered DiD for the case study from the music industry (10 mins)\n",
      "5.Conclusions (5 mins)\n",
      "\n",
      "Target Audience: The talk would be beneficial for data scientists, researchers, and practitioners interested in causal inference, marketing analytics, and quasi-experimental design.Attendees should have a basic understanding of statistical methods used in data science.Key Takeaways:\n",
      "\n",
      "1.Understanding of the DiD approach and its limitations in the context of analyses with observational data.2.Insights into the Staggered DiD method and its application.3.Practical knowledge about executing and evaluating DiD studies effectively.\n",
      "---------------\n",
      "We will then dive into the various libraries available for Python, discussing their strengths and weaknesses and providing real-world examples of their usage.Specifically, we will cover:\n",
      "- EconML: An open-source library for general Causal Inference purposes, by Microsoft Research\n",
      "- CausalML: An open-source library for uplift modeling in particular, by Uber\n",
      "\n",
      "We will compare and contrast these libraries with respect to CATE estimation, discussing which methods they use, which assumptions they make, and which types of data they are best suited for.We will also provide code examples to illustrate how to use each library in practice.Moreover, we will discuss what we think is missing from both of them.By the end of the talk, attendees will have a solid understanding of the Python tooling and ecosystem for estimating CATEs in a causal inference setting.They will know which libraries to use for different types of data and which methods are most appropriate for different scenarios.This talk could be particularly relevant for Data Scientists wishing to analyze experiments, such as A/B tests, or trying to derive causal statements from observational, non-experimental data.Participants are not expected to have Causal Inference expertise.\n",
      "---------------\n",
      "Yet, a fundamental understanding of Machine Learning and Probability Theory will be beneficial.0-5’: Why Causal Inference and why CATE estimation?5-10’: What are some conceptual ways of estimating CATEs?10-20’: How can we use EconML and CausalML for CATE estimation on a real dataset?20-30’: What are we missing from EconML and CausalML?\n",
      "---------------\n",
      "Additionally, we explain how the open source Python package developed at Uber, CausalML, can help others in successfully making the transition from correlation-driven machine learning to causal-driven machine learning.\n",
      "---------------\n",
      "Causal Inference Libraries: What They Do, What I'd Like Them To Do\n",
      "\n",
      "This talk will explore the Python tooling and ecosystem for estimating conditional average treatment effects (CATEs) in a Causal Inference setting.Using real world-examples, it will compare and contrast the pros and cons of various existing libraries as well as outline desirable functionalities not currently offered by any public library.Conditional average treatment effects (CATEs) are a fundamental concept in Causal Inference, allowing for the estimation of the effect of a particular treatment or intervention.For CATEs, the effect estimation is not only with respect to an entire population, e.g.all experiment participants, but rather with respect to units, e.g.a single experiment participant, with individual characteristics.This can be very important to meaningfully personalize services and products.In this talk, we will explore the Python tooling and ecosystem for estimating CATEs, including libraries such as EconML and CausalML.We will begin by providing an overview of the theory behind CATE estimation, how it fits into the broader field of causal inference and how Machine Learning has recently broken into CATE estimation.\n",
      "---------------\n",
      "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry\n",
      "\n",
      "The Difference-in-Differences (DiD) methodology is a popular causal inference method utilized by leading tech firms such as Microsoft Research, LinkedIn, Meta, and Uber.Yet recent studies suggest that traditional DiD methods may have significant limitations when treatment timings differ.An effective alternative is the implementation of the staggered DiD design.We exemplify this by investigating an interesting question in the music industry: Does featuring a song in TV shows influence its popularity, and are there specific factors that could moderate this impact?Difference-in-differences (DiD) is a causal inference method frequently used in empirical research in industry and academia.However, standard DiD has limitations when interventions occur at different times or affect varying groups.This talk will highlight the application of the Staggered DiD method, a more nuanced approach that addresses these limitations, in the context of the music industry.We will try to answer the question of how music features in TV shows affect music popularity and how this effect might change for different types of music using the staggered DiD method.\n",
      "---------------\n",
      "Personalization at Uber scale via causal-driven machine learning\n",
      "\n",
      "In this talk, we outline how we introduced causality into our machine learning models within the core checkout and onboarding experiences globally, thereby strongly improving our key business metrics.We discuss case studies, where experimental data were combined with machine learning in order to create value for our users and personalize their experiences, and we share our lessons learned with the goal to inspire attendees to start incorporating causality into their machine learning solutions.Additionally, we explain how the open source Python package developed at Uber, CausalML, can help others in successfully making the transition from correlation-driven machine learning to causal-driven machine learning.In this talk, we outline how we introduced causality into our machine learning models within the core checkout and onboarding experiences globally, thereby strongly improving our key business metrics.We discuss case studies, where experimental data were combined with machine learning in order to create value for our users and personalize their experiences, and we share our lessons learned with the goal to inspire attendees to start incorporating causality into their machine learning solutions.\n",
      "---------------\n",
      "The key takeaway is that when it comes to changing people's minds using data, explaining assumptions to decision-makers is just as important as understanding the underlying statistics.**Outline**\n",
      "- Minute 0-5: Introduction and Motivation\n",
      "- Minute 5-10: Difference-in-Difference / Bayesian Structural Time-Series\n",
      "- Minute 10-15: Case - Conversion effects of content changes based language-pair specific releases at Babbel\n",
      "- Minute 15-20: Regression Discontinuity Design\n",
      "- Minute 20-25: Case - Estimating motivational effects of language assessment\n",
      "- Minute 25-30: Wrap-up / Take-Aways\n",
      "\n",
      "Attendees should have basic knowledge of statistics and causality to get the most out of this talk.\n",
      "---------------\n",
      "Our talk is a must-watch for data scientists, product managers, supply chain wizards, and anyone who has ever been curious about the new innovations in number-crunching that gets your favorite snack from the factory to your front door.If you're in the e-commerce or retail industries, this talk will be as essential as oatmilk and bread in a shopping list.Don’t worry if words like multimodal, temporal, and fusion sound intimidating; They will be explained in a way that is informative and entertaining if you have seen them before but also if you have not.We promise it’s not all graphs and matrices – expect an unexpected rollercoaster ride through the aisle of our digital store.With each turn, you'll discover how our multimodal method uses product images, textual descriptions, and additional contextual information to predict if potatoes will overtake pasta in popularity next month.We'll show you the ‘cart’ loads of data behind these predictions, putting a fun spin on the world of groceries.In the grand finale, we’ll take you behind the scenes of our model's showdown with traditional methods.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "results = retriever.retrieve('causal')\n",
    "for r in results:\n",
    "    print(r.node.text)\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ee400-d0c5-4ad2-b68e-09c84ce4044c",
   "metadata": {},
   "source": [
    "## Querying the vector index with an external LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ced37-8de0-4d5e-8abb-6fae18bf1803",
   "metadata": {},
   "source": [
    "### Set OpenAI API key (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5b922-be03-435f-b581-053dfb47e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret import openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fc70c-f18f-412c-bae5-3ac06a090a92",
   "metadata": {},
   "source": [
    "### Use OpenAI's gpt-3.5-turbo for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6ce8a-85d1-4d29-a182-a749782d14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llama_index.llms.OpenAI(model=\"gpt-3.5-turbo\", api_key=openai_api_key)\n",
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=embed_model, chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c91e61-7b94-4a1c-a68b-b22553e8b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-23T16:20:45.797800+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "# load vector index from file\n",
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1c213-e75b-4bf4-bd25-90d9afbcf01d",
   "metadata": {},
   "source": [
    "### Create a query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d00b1-c133-4af2-b9f7-0e82113797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139131c-f152-4d01-94cb-0099ba6d4548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a4e9960-51ab-4b53-b33e-6c79d16e972a",
   "metadata": {},
   "source": [
    "### Which talks might be interesting for startup founders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64136237-b60d-48b5-89d7-e749b71e061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Which talks are probably interesting for startup founders?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad3806-ad31-4774-a2f4-fbf085d48a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The talks that are probably interesting for startup founders are \"Setting The Right KPIs\" and \"Data-Driven Decision Making.\" These talks discuss topics such as setting realistic and challenging KPIs and leveraging data for informed decision-making and product strategy adjustments, which are relevant for startup founders involved in shaping product strategy and making data-driven decisions.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525644b7-4fe3-4ab9-a95d-8fa8589ab7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e121fe9-762f-4c0a-9501-a8332ec1cedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
