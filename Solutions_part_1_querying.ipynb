{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913d2a2-f87f-4505-abb4-49e6b2291e52",
   "metadata": {},
   "source": [
    "# Part 1: Querying\n",
    "\n",
    "PyData Amsterdam 2023\n",
    "\n",
    "* Tutorial: Building a personal search engine with llama-index\n",
    "* Speakers: Judith van Stegeren and Yorick van Pelt\n",
    "* Company: [Datakami](www.datakami.nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effd39ab-b6ba-4d69-8e8f-39f382d7712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/judith/git/pydata-llama-index-tutorial/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pprint # dev\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# loguru: logging for lazy people :)\n",
    "from loguru import logger\n",
    "\n",
    "# we're using a local embeddings model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# llama_index: the topic of this tutorial\n",
    "# we're not importing specific methods or classes so it's clear when we actually call llama_index!\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f5b922-be03-435f-b581-053dfb47e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret import openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f44daa1-a3e6-45bf-bc6e-8c84fad646b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log to stdout and local file\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"{time} - {level} - {message}\", level=\"DEBUG\")\n",
    "logger.add(\"tutorial_part_1.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e2e8b1-5b94-4dcf-b109-9e8351b087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_PATH = Path(\"data/pydata/schedule.json\")\n",
    "INDEX_PATH = Path(\"indices/pydata_schedule_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb43be-6be1-4cd1-933f-cd3a77052e33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f10b1-759c-459b-bece-1495c099cf62",
   "metadata": {},
   "source": [
    "### Use a local embeddings model\n",
    "\n",
    "(So no calls to OpenAI APIs :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4b220f-e86c-48e6-a069-ec4d84a2b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =llama_index.llms.OpenAI(model=\"gpt-3.5-turbo\", api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85531cae-9ec3-47f3-b4c1-e9df4cea39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all-minilm-l6-v2 has a maximum size of 256 tokens\n",
    "# source: https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=\"local:sentence-transformers/all-minilm-l6-v2\", chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a1ce4-48b3-494b-9ee3-f0b0f20c67a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c821a64-7322-48f2-a183-225c90a31a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_index.global_service_context = service_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b968eeb-82c6-49c4-88bf-3b776dd1ec49",
   "metadata": {},
   "source": [
    "### Load PyData schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c312d-62ed-492c-9466-4aa7f96f0a89",
   "metadata": {},
   "source": [
    "**Load JSON file with the PyData Amsterdam 2023 schedule**\n",
    "* source: https://amsterdam2023.pydata.org/cfp/schedule/export/schedule.json\n",
    "* retrieved: 2023-08-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70f8ada1-48d2-4ab1-982d-177727d7d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22T12:08:14.040315+0200 - INFO - Loaded the PyData schedule JSON from file data/pydata/schedule.json\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'r') as infile:\n",
    "    schedule = json.loads(infile.read())\n",
    "    logger.info(f\"Loaded the PyData schedule JSON from file {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd308a6-84e7-4964-aa90-205cc6b2556c",
   "metadata": {},
   "source": [
    "**Extract the talks from the schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02ae6ab9-adc3-4456-9212-6a50a37fdb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22T12:08:15.480534+0200 - INFO - Loaded 67 talks from the PyData schedule JSON!\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "talks = []\n",
    "for day in schedule['schedule']['conference']['days']:\n",
    "    for room in day['rooms']:\n",
    "        for talk in day['rooms'][room]:\n",
    "            talk['filename'] = str(DATA_PATH)\n",
    "            talk['category'] = \"Conference talk at PyData Amsterdam 2023\"\n",
    "            talks.append(talk)\n",
    "\n",
    "logger.info(f\"Loaded {len(talks)} talks from the PyData schedule JSON!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "867fe918-339e-4de3-a614-f73a44f4b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a PyData talk:\n",
      "{'abstract': 'Lorem ipsum dolor',\n",
      " 'answers': [],\n",
      " 'attachments': [],\n",
      " 'category': 'Conference talk at PyData Amsterdam 2023',\n",
      " 'date': '2023-09-14T09:30:00+02:00',\n",
      " 'description': 'Lorem ipsum dolor',\n",
      " 'do_not_record': False,\n",
      " 'duration': '00:50',\n",
      " 'filename': 'data/pydata/schedule.json',\n",
      " 'guid': 'e82f37c8-03f9-5cb5-92f2-8f157924b59d',\n",
      " 'id': 267,\n",
      " 'language': 'en',\n",
      " 'links': [],\n",
      " 'logo': '',\n",
      " 'persons': [{'answers': [],\n",
      "              'biography': 'Vicki Boykis works on end-to-end ML applications. '\n",
      "                           'Her interests include the intersection of '\n",
      "                           'information retrieval and large language models, '\n",
      "                           'applying engineering best practices to machine '\n",
      "                           'learning, and Nutella.  She works at Duo Security '\n",
      "                           'and she lives in Philadelphia with her family. Her '\n",
      "                           'favorite hobby was making terrible jokes on '\n",
      "                           'Twitter when it was still good. She recently wrote '\n",
      "                           'a deep dive on embeddings and put together '\n",
      "                           'Normconf, celebrating normcore workflows in ML.',\n",
      "              'code': '3G8JD7',\n",
      "              'id': 283,\n",
      "              'public_name': 'Vicki Boykis'}],\n",
      " 'recording_license': '',\n",
      " 'room': 'Room 3',\n",
      " 'slug': 'cfp-267-keynote-vicki-boykis',\n",
      " 'start': '09:30',\n",
      " 'subtitle': '',\n",
      " 'title': 'Keynote Vicki Boykis',\n",
      " 'track': None,\n",
      " 'type': 'Keynote',\n",
      " 'url': 'https://amsterdam2023.pydata.org/cfp/talk/QLDEXB/'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a PyData talk:\")\n",
    "pprint.pprint(talks[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaeac5c-91f1-4899-b1dc-714c45cbe17e",
   "metadata": {},
   "source": [
    "**Turn the talks data into llama_index Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4b91d79-a54f-4f1a-aaf3-8ce1c01dbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for talk in talks:\n",
    "    talk_text = f\"{talk['title']}\\n\\n{talk['abstract']}\\n\\n{talk['description']}\"\n",
    "    doc = llama_index.Document(text = talk_text)\n",
    "    #doc.extra_info = talk\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f68d2b6-23ea-4a9d-bc9c-87743dbec032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a PyData talk Document:\n",
      "{'embedding': None,\n",
      " 'end_char_idx': None,\n",
      " 'excluded_embed_metadata_keys': [],\n",
      " 'excluded_llm_metadata_keys': [],\n",
      " 'hash': 'baf26eaa727340ec583dbc2b3d661646147352bb711318674ab72326813efbe6',\n",
      " 'id_': 'c8268bf8-c1d4-4420-b345-7094bd1eb19b',\n",
      " 'metadata': {},\n",
      " 'metadata_seperator': '\\n',\n",
      " 'metadata_template': '{key}: {value}',\n",
      " 'relationships': {},\n",
      " 'start_char_idx': None,\n",
      " 'text': 'Keynote Vicki Boykis\\n\\nLorem ipsum dolor\\n\\nLorem ipsum dolor',\n",
      " 'text_template': '{metadata_str}\\n\\n{content}'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a PyData talk Document:\")\n",
    "pprint.pprint(dict(documents[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151cd83-2133-48e6-a7b2-6c152d14a323",
   "metadata": {},
   "source": [
    "### Create vector index from PyData schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2fc0d-2d3b-4911-bb2f-f85cc80cf9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cfe9102-054b-4cc2-b0c0-8d37233b4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22T12:08:21.411280+0200 - INFO - Building a VectorStoreIndex from 67 documents\n",
      "2023-08-22T12:08:32.213701+0200 - INFO - Saved VectorStoreIndex to indices/pydata_schedule_index\n"
     ]
    }
   ],
   "source": [
    "# create vector index from PyData schedule\n",
    "logger.info(f\"Building a VectorStoreIndex from {len(documents)} documents\")\n",
    "index = llama_index.VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# store index to disk\n",
    "index.storage_context.persist(INDEX_PATH)\n",
    "logger.info(f\"Saved VectorStoreIndex to {INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b317df6-0c8c-4ec8-beb9-ce932bc34a50",
   "metadata": {},
   "source": [
    "## Load vector index with PyData Amsterdam 2023 schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8bc3e5-6bc7-4970-9a1f-c722ba931956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22T12:12:27.556688+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "# load vector index from file\n",
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        # rebuild storage context from disk                                          \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        # load index                                                                 \n",
    "        #index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        index = llama_index.load_index_from_storage(storage_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e4d89-3dea-4809-b073-a4c5efc03ab4",
   "metadata": {},
   "source": [
    "## Create a search engine from vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4d0cad-f3bb-4467-b3ed-ec87e5fa966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a search engine\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede9a40-f6e1-42f3-92bf-4519bfad87ef",
   "metadata": {},
   "source": [
    "## Query the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f343d465-8b22-4778-91e7-b33f3c63e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the search engine\n",
    "results = retriever.retrieve(\"llama_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6741e34d-0edb-4aae-97d6-f1760ba8e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_='97863ede-b6a9-4fc2-8c7d-bf09017fdf64' embedding=None metadata={} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0a4e1eca-81a8-4212-a397-a25a3e12a12c', node_type=None, metadata={}, hash='06987f3dd83522f1b914565902298eb9eed00f77d5fa47d3d52c5f4301959f54'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ac962050-8632-48fc-8be6-5bffc2c2eef9', node_type=None, metadata={}, hash='3afcb14eb9ca291a920e6ec82ff895103024cb798781880f8f09ee4e80c725c0')} hash='c852556384af48ec95a06b58d93753903d7a40cf65713b2ce35ef85583de0ac8' text='Building a personal search engine with llama-index\\n\\nWouldn’t it be great to have a Google-like search engine, but then for your own text files and completely private?In this tutorial we’ll build a small personal search engine using open source library llama-index.In this tutorial we will build a small personal search engine using open source library `llama-index`.Llama-index provides utility functions for ingesting various kinds of data, breaking the data up in chunks, building an index of that data using vector embeddings, and retrieving data from the index based on queries.We can even use llama-index to post-process the retrieval results for us using large language models such as GPT.The target audience is people that are already familiar with Python.Participants will experience working with unstructured data, vector embeddings, and explore the possibilities of the recent developments in natural language processing.Workshop materials will be provided via Github before the start of the workshop.For the demo application, we will only use open-source software and models that are light enough to run on an average laptop without a GPU.Using llama-index with OpenAI’s API is optional.' start_char_idx=None end_char_idx=None text_template='{metadata_str}\\n\\n{content}' metadata_template='{key}: {value}' metadata_seperator='\\n'\n",
      "---\n",
      "\n",
      "id_='c24af771-b9f2-46ef-825b-7b49c8d7a74a' embedding=None metadata={} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5e8625af-c56a-4989-ac05-ffbe6c247d5d', node_type=None, metadata={}, hash='7d3cd326f2833ac4115a0d9a2c614b25cfdcfc7ec2eaebb6c382e32a2424ed78')} hash='7d3cd326f2833ac4115a0d9a2c614b25cfdcfc7ec2eaebb6c382e32a2424ed78' text='Unconference #1\\n\\nLorem ipsum dolor\\n\\n1143' start_char_idx=None end_char_idx=None text_template='{metadata_str}\\n\\n{content}' metadata_template='{key}: {value}' metadata_seperator='\\n'\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result.node)\n",
    "    print(\"---\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01462a67-ebcc-4282-a7c0-5fadb0cc75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_='e032a29f-3bdf-479e-9cf7-965f14d55f50' embedding=None metadata={} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6d2f4965-8bb3-4e9a-9f7f-47799dedee38', node_type=None, metadata={}, hash='5f9805edfef93dacfcb40f26094eed9f7811fed2523d7810e2606e74e945155b')} hash='5f9805edfef93dacfcb40f26094eed9f7811fed2523d7810e2606e74e945155b' text='Kickstart AI sponsored drinks [time & location TBD]\\n\\nKickstart AI is a foundation powered by a coalition of iconic Dutch brands (Ahold Delhaize, ING, KLM and NS). Their mission is to accelerate AI adoption in the Netherlands, and improve society through the use of AI.\\n\\nLorem ipsum dolor' start_char_idx=None end_char_idx=None text_template='{metadata_str}\\n\\n{content}' metadata_template='{key}: {value}' metadata_seperator='\\n'\n",
      "---\n",
      "\n",
      "id_='764c2331-2c5f-414a-8a97-fe614114bcd3' embedding=None metadata={} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5a12faea-b5c4-4a51-a501-0112f267f72a', node_type=None, metadata={}, hash='6bdf83af063b48e726e9a7e7f0c219d0ab7239a40c6100df8853e02aad1a4306'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fa44306d-1f93-4f75-a05b-49cdb494fa7e', node_type=None, metadata={}, hash='fa618cee034119e8f2333c7ea97da0597f629ce6df29bd00ca4ab81fb491fe0a')} hash='945f803bbc6a463def683687d203216e7b70ba233a9e08ebc2b5e1da0cb2fb7e' text=\"[7 mins]\\r\\n\\r\\n3.Setting The Right KPIs: Once metrics are defined, we'll venture into setting the correct KPIs - the small set of top line numbers that say if our venture is doing the job.[7 mins]\\r\\n\\r\\n4.Data-Driven Decision Making: Lastly, we'll elucidate how to leverage the data you've gathered to make informed, strategic decisions.This necessitates interpreting your metrics and KPIs, spotting trends, and making necessary adjustments to stay on course.[7 mins]\\r\\n\\r\\nIncorporating real-world case studies, we'll demonstrate how these concepts intertwine to contribute to product success.Learning Objectives:\\r\\n* Appreciate the multifaceted role of a data scientist in driving product strategies.* Learn to set realistic and challenging KPIs that align with your company's overarching objectives.* Gain insights into leveraging data for informed decision-making and product strategy adjustments.Who Should Attend:\\r\\nThis talk is aimed for data professionals, however anyone involved in shaping product strategy and making data-driven decisions could find this useful.\" start_char_idx=None end_char_idx=None text_template='{metadata_str}\\n\\n{content}' metadata_template='{key}: {value}' metadata_seperator='\\n'\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = retriever.retrieve(\"startups\")\n",
    "for result in results:\n",
    "    print(result.node)\n",
    "    print(\"---\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ee400-d0c5-4ad2-b68e-09c84ce4044c",
   "metadata": {},
   "source": [
    "## Querying the vector index with an external LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2aa750-c144-422f-8e25-4a7d05c12f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de9d00b1-c133-4af2-b9f7-0e82113797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64136237-b60d-48b5-89d7-e749b71e061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22T12:12:55.725596+0200 - ERROR - No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = query_engine.query(\"Which talks are probably interesting for startup founders?\")\n",
    "except openai.error.AuthenticationError as auth_error:\n",
    "    logger.error(auth_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9ad3806-ad31-4774-a2f4-fbf085d48a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241m.\u001b[39mresponse\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e121fe9-762f-4c0a-9501-a8332ec1cedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
