{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913d2a2-f87f-4505-abb4-49e6b2291e52",
   "metadata": {},
   "source": [
    "# Part 1: Querying\n",
    "\n",
    "PyData Amsterdam 2023\n",
    "\n",
    "* Tutorial: Building a personal search engine with llama-index\n",
    "* Speakers: Judith van Stegeren and Yorick van Pelt\n",
    "* Company: [Datakami](www.datakami.nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "effd39ab-b6ba-4d69-8e8f-39f382d7712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pprint # dev\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# loguru: logging for lazy people :)\n",
    "from loguru import logger\n",
    "\n",
    "# we're using a local embeddings model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# llama_index: the topic of this tutorial\n",
    "# we're not importing specific methods or classes so it's clear when we actually call llama_index!\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f44daa1-a3e6-45bf-bc6e-8c84fad646b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log to stdout and local file\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"{time} - {level} - {message}\", level=\"DEBUG\")\n",
    "logger.add(\"tutorial_part_1.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "84e2e8b1-5b94-4dcf-b109-9e8351b087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_PATH = Path(\"data/pydata/schedule.json\")\n",
    "INDEX_PATH = Path(\"indices/pydata_schedule_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb43be-6be1-4cd1-933f-cd3a77052e33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f10b1-759c-459b-bece-1495c099cf62",
   "metadata": {},
   "source": [
    "### Use a local embeddings model\n",
    "\n",
    "(So no calls to OpenAI APIs :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "85531cae-9ec3-47f3-b4c1-e9df4cea39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all-minilm-l6-v2 has a maximum size of 256 tokens\n",
    "# source: https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "\n",
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=\"local:sentence-transformers/all-minilm-l6-v2\", chunk_size=256\n",
    ")\n",
    "\n",
    "llama_index.global_service_context = service_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b968eeb-82c6-49c4-88bf-3b776dd1ec49",
   "metadata": {},
   "source": [
    "### Load PyData schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c312d-62ed-492c-9466-4aa7f96f0a89",
   "metadata": {},
   "source": [
    "**Load JSON file with the PyData Amsterdam 2023 schedule**\n",
    "* source: https://amsterdam2023.pydata.org/cfp/schedule/export/schedule.json\n",
    "* retrieved: 2023-08-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "70f8ada1-48d2-4ab1-982d-177727d7d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-10T16:08:22.152109+0200 - INFO - Loaded the PyData schedule JSON from file data/pydata/schedule.json\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'r') as infile:\n",
    "    schedule = json.loads(infile.read())\n",
    "    logger.info(f\"Loaded the PyData schedule JSON from file {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd308a6-84e7-4964-aa90-205cc6b2556c",
   "metadata": {},
   "source": [
    "**Extract the talks from the schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "02ae6ab9-adc3-4456-9212-6a50a37fdb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-10T16:08:32.275500+0200 - INFO - Loaded 67 talks from the PyData schedule JSON!\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "talks = []\n",
    "for day in schedule['schedule']['conference']['days']:\n",
    "    for room in day['rooms']:\n",
    "        for talk in day['rooms'][room]:\n",
    "            talk['filename'] = str(DATA_PATH)\n",
    "            talk['category'] = \"Conference talk at PyData Amsterdam 2023\"\n",
    "            talks.append(talk)\n",
    "\n",
    "logger.info(f\"Loaded {len(talks)} talks from the PyData schedule JSON!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaeac5c-91f1-4899-b1dc-714c45cbe17e",
   "metadata": {},
   "source": [
    "**Turn the talks data into llama_index Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f4b91d79-a54f-4f1a-aaf3-8ce1c01dbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for talk in talks:\n",
    "    talk_text = f\"{talk['title']}\\n\\n{talk['abstract']}\\n\\n{talk['description']}\"\n",
    "    doc = llama_index.Document(text = talk_text)\n",
    "    #doc.extra_info = talk\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151cd83-2133-48e6-a7b2-6c152d14a323",
   "metadata": {},
   "source": [
    "### Create vector index from PyData schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2fc0d-2d3b-4911-bb2f-f85cc80cf9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5cfe9102-054b-4cc2-b0c0-8d37233b4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-10T16:08:33.453421+0200 - INFO - Building a VectorStoreIndex from 67 documents\n",
      "2023-08-10T16:08:42.219739+0200 - INFO - Saved VectorStoreIndex to indices/pydata_schedule_index\n"
     ]
    }
   ],
   "source": [
    "# create vector index from PyData schedule\n",
    "logger.info(f\"Building a VectorStoreIndex from {len(documents)} documents\")\n",
    "# create a vector store index\n",
    "# uses OpenAI API by default!\n",
    "index = llama_index.VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# store index to disk\n",
    "index.storage_context.persist(INDEX_PATH)\n",
    "logger.info(f\"Saved VectorStoreIndex to {INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e078c0-b4fe-4f21-9a6b-1555d814f6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3b8bc3e5-6bc7-4970-9a1f-c722ba931956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-10T16:08:42.589211+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "# load vector index from file\n",
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        # rebuild storage context from disk                                          \n",
    "        storage_context = StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        # load index                                                                 \n",
    "        index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4b4d0cad-f3bb-4467-b3ed-ec87e5fa966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()\n",
    "results = retriever.retrieve(\"llama_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6741e34d-0edb-4aae-97d6-f1760ba8e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_='a64c9da7-86b1-4782-af65-30cd1aa9f5cf' embedding=None metadata={} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1f00b54b-a024-4493-83ff-c6408f8b498c', node_type=None, metadata={}, hash='06987f3dd83522f1b914565902298eb9eed00f77d5fa47d3d52c5f4301959f54'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7da999ef-e4a3-4f0f-ae88-0c8a500c5135', node_type=None, metadata={}, hash='e03d84fc9854a169e62cc820db376263518300e815b5b95fe6642903037612e1')} hash='6a94850a7452390034c74ec335f631cc1c25a2753a78508cf6242f98bffe8538' text='Building a personal search engine with llama-index\\n\\nWouldn’t it be great to have a Google-like search engine, but then for your own text files and completely private? In this tutorial we’ll build a small personal search engine using open source library llama-index.\\n\\nIn this tutorial we will build a small personal search engine using open source library `llama-index`. Llama-index provides utility functions for ingesting various kinds of data, breaking the data up in chunks, building an index of that data using vector embeddings, and retrieving data from the index based on queries. We can even use llama-index to post-process the retrieval results for us using large language models such as GPT.\\r\\n\\r\\nThe target audience is people that are already familiar with Python. Participants will experience working with unstructured data, vector embeddings, and explore the possibilities of the recent developments in natural language processing. \\r\\n\\r\\nWorkshop materials will be provided via Github before the start of the workshop. For the demo application, we will only use' start_char_idx=0 end_char_idx=1069 text_template='{metadata_str}\\n\\n{content}' metadata_template='{key}: {value}' metadata_seperator='\\n'\n",
      "---\n",
      "\n",
      "id_='7da999ef-e4a3-4f0f-ae88-0c8a500c5135' embedding=None metadata={} excluded_embed_metadata_keys=[] excluded_llm_metadata_keys=[] relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1f00b54b-a024-4493-83ff-c6408f8b498c', node_type=None, metadata={}, hash='06987f3dd83522f1b914565902298eb9eed00f77d5fa47d3d52c5f4301959f54'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a64c9da7-86b1-4782-af65-30cd1aa9f5cf', node_type=None, metadata={}, hash='6a94850a7452390034c74ec335f631cc1c25a2753a78508cf6242f98bffe8538')} hash='e03d84fc9854a169e62cc820db376263518300e815b5b95fe6642903037612e1' text='via Github before the start of the workshop. For the demo application, we will only use open-source software and models that are light enough to run on an average laptop without a GPU. \\r\\nUsing llama-index with OpenAI’s API is optional. If you want to explore postprocessing your results with GPT-3.5, we recommend registering an OpenAI account and making sure you have your OpenAI API key ready.' start_char_idx=982 end_char_idx=1377 text_template='{metadata_str}\\n\\n{content}' metadata_template='{key}: {value}' metadata_seperator='\\n'\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result.node)\n",
    "    print(\"---\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "01462a67-ebcc-4282-a7c0-5fadb0cc75a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1f00b54b-a024-4493-83ff-c6408f8b498c', node_type=None, metadata={}, hash='06987f3dd83522f1b914565902298eb9eed00f77d5fa47d3d52c5f4301959f54'),\n",
       " <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7da999ef-e4a3-4f0f-ae88-0c8a500c5135', node_type=None, metadata={}, hash='e03d84fc9854a169e62cc820db376263518300e815b5b95fe6642903037612e1')}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].node.relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14977579-0616-477a-964e-514527210211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
