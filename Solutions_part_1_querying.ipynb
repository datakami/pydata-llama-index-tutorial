{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913d2a2-f87f-4505-abb4-49e6b2291e52",
   "metadata": {},
   "source": [
    "# Part 1: Querying\n",
    "\n",
    "PyData Amsterdam 2023\n",
    "\n",
    "* Tutorial: Building a personal search engine with llama-index\n",
    "* Speakers: Judith van Stegeren and Yorick van Pelt\n",
    "* Company: [Datakami](www.datakami.nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd39ab-b6ba-4d69-8e8f-39f382d7712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pprint # dev\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# loguru: logging for lazy people :)\n",
    "from loguru import logger\n",
    "\n",
    "# llama_index: the topic of this tutorial\n",
    "# we're not importing specific methods or classes so it's clear when we actually call llama_index!\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5b922-be03-435f-b581-053dfb47e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret import openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44daa1-a3e6-45bf-bc6e-8c84fad646b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log to stdout and local file\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"{time} - {level} - {message}\", level=\"DEBUG\")\n",
    "logger.add(\"tutorial_part_1.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e8b1-5b94-4dcf-b109-9e8351b087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_PATH = Path(\"data/pydata/schedule.json\")\n",
    "INDEX_PATH = Path(\"indices/pydata_schedule_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb43be-6be1-4cd1-933f-cd3a77052e33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f10b1-759c-459b-bece-1495c099cf62",
   "metadata": {},
   "source": [
    "### Use a local embeddings model\n",
    "\n",
    "(So no calls to OpenAI APIs :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b220f-e86c-48e6-a069-ec4d84a2b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llama_index.llms.OpenAI(model=\"gpt-3.5-turbo\", api_key=openai_api_key) # todo: requires API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531cae-9ec3-47f3-b4c1-e9df4cea39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all-minilm-l6-v2 has a maximum size of 256 tokens\n",
    "# source: https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=\"local:sentence-transformers/all-minilm-l6-v2\", chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c821a64-7322-48f2-a183-225c90a31a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_index.global_service_context = service_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b968eeb-82c6-49c4-88bf-3b776dd1ec49",
   "metadata": {},
   "source": [
    "### Load PyData schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c312d-62ed-492c-9466-4aa7f96f0a89",
   "metadata": {},
   "source": [
    "**Load JSON file with the PyData Amsterdam 2023 schedule**\n",
    "* source: https://amsterdam2023.pydata.org/cfp/schedule/export/schedule.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8ada1-48d2-4ab1-982d-177727d7d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-30T17:19:37.543473+0200 - INFO - Loaded the PyData schedule JSON from file data/pydata/schedule.json\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'r') as infile:\n",
    "    schedule = json.load(infile)\n",
    "    logger.info(f\"Loaded the PyData schedule JSON from file {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd308a6-84e7-4964-aa90-205cc6b2556c",
   "metadata": {},
   "source": [
    "**Extract the talks from the schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae6ab9-adc3-4456-9212-6a50a37fdb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-30T17:19:38.431570+0200 - INFO - Loaded 68 talks from the PyData schedule JSON!\n"
     ]
    }
   ],
   "source": [
    "talks = {}\n",
    "for day in schedule['schedule']['conference']['days']:\n",
    "    for room in day['rooms'].values():\n",
    "        for talk in room:\n",
    "            talk['filename'] = str(DATA_PATH)\n",
    "            talk['category'] = \"Conference talk at PyData Amsterdam 2023\"\n",
    "            talks[talk['guid']] = talk\n",
    "\n",
    "logger.info(f\"Loaded {len(talks)} talks from the PyData schedule JSON!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe918-339e-4de3-a614-f73a44f4b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a PyData talk:\n",
      "{'abstract': 'Working on ML serving for couple of years we learned a lot. I '\n",
      "             'would like to share a set of best practices / learnings with the '\n",
      "             'community',\n",
      " 'answers': [],\n",
      " 'attachments': [],\n",
      " 'category': 'Conference talk at PyData Amsterdam 2023',\n",
      " 'date': '2023-09-14T12:00:00+02:00',\n",
      " 'description': 'At Adyen we deploy a lot of models for online inference in '\n",
      "                'the payment flow. Working in the MLOps team to streamline '\n",
      "                'this process, I learned a lot about best practices / things '\n",
      "                'to consider before (after) putting a model online. These are '\n",
      "                'small things but they do contribute to a production and '\n",
      "                'reliable setup for online inference. Some examples:\\r\\n'\n",
      "                '\\r\\n'\n",
      "                '- Adding meta data & creating a self contained archive\\r\\n'\n",
      "                '- Separating serving sources from training sources\\r\\n'\n",
      "                '- Choosing the requirements of model\\r\\n'\n",
      "                '- Adding an example input & output request\\r\\n'\n",
      "                '- Adding schemas for input and output\\r\\n'\n",
      "                '- Common issues when putting models online: memory leaks, '\n",
      "                'concurrency\\r\\n'\n",
      "                '- Which server is best? Process based or thread based\\r\\n'\n",
      "                '- How different python versions affect inference (execution) '\n",
      "                'time',\n",
      " 'do_not_record': False,\n",
      " 'duration': '00:30',\n",
      " 'filename': 'data/pydata/schedule.json',\n",
      " 'guid': '80309313-5f6a-58dd-b81f-730483b2dd6b',\n",
      " 'id': 28,\n",
      " 'language': 'en',\n",
      " 'links': [],\n",
      " 'logo': '',\n",
      " 'persons': [{'answers': [],\n",
      "              'biography': 'Staff Engineer @ Adyen. I am passionate about high '\n",
      "                           'performance distributed systems. Recently I was '\n",
      "                           \"working on scaling Adyen's Data & ML \"\n",
      "                           'infrastructure.',\n",
      "              'code': 'RBKUVL',\n",
      "              'id': 42,\n",
      "              'public_name': 'Ziad Al Moubayed'}],\n",
      " 'recording_license': '',\n",
      " 'room': 'Bar',\n",
      " 'slug': 'cfp-28-reliable-and-scalable-ml-serving-best-practices-for-online-model-deployment',\n",
      " 'start': '12:00',\n",
      " 'subtitle': '',\n",
      " 'title': 'Reliable and Scalable ML Serving: Best Practices for Online Model '\n",
      "          'Deployment',\n",
      " 'track': None,\n",
      " 'type': 'Talk',\n",
      " 'url': 'https://amsterdam2023.pydata.org/cfp/talk/NSZFRB/'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a PyData talk:\")\n",
    "pprint.pprint(list(talks.values())[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaeac5c-91f1-4899-b1dc-714c45cbe17e",
   "metadata": {},
   "source": [
    "**Turn the talks data into llama_index Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b91d79-a54f-4f1a-aaf3-8ce1c01dbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for talk in talks.values():\n",
    "    talk_text = f\"{talk['title']}\\n\\n{talk['abstract']}\\n\\n{talk['description']}\"\n",
    "    speakers = \", \".join([p['public_name'] for p in talk['persons']])\n",
    "    doc = llama_index.Document(text = talk_text, id_ = talk[\"guid\"], extra_info={'title': talk['title'], 'speakers' : speakers})\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68d2b6-23ea-4a9d-bc9c-87743dbec032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a PyData talk Document:\n",
      "{'embedding': None,\n",
      " 'end_char_idx': None,\n",
      " 'excluded_embed_metadata_keys': [],\n",
      " 'excluded_llm_metadata_keys': [],\n",
      " 'hash': 'e1a90272b9b8d29972d9674b7c1ab976aca6fdf7ce73f49b1ca7432c90123c74',\n",
      " 'id_': '80309313-5f6a-58dd-b81f-730483b2dd6b',\n",
      " 'metadata': {'speakers': 'Ziad Al Moubayed',\n",
      "              'title': 'Reliable and Scalable ML Serving: Best Practices for '\n",
      "                       'Online Model Deployment'},\n",
      " 'metadata_seperator': '\\n',\n",
      " 'metadata_template': '{key}: {value}',\n",
      " 'relationships': {},\n",
      " 'start_char_idx': None,\n",
      " 'text': 'Reliable and Scalable ML Serving: Best Practices for Online Model '\n",
      "         'Deployment\\n'\n",
      "         '\\n'\n",
      "         'Working on ML serving for couple of years we learned a lot. I would '\n",
      "         'like to share a set of best practices / learnings with the '\n",
      "         'community\\n'\n",
      "         '\\n'\n",
      "         'At Adyen we deploy a lot of models for online inference in the '\n",
      "         'payment flow. Working in the MLOps team to streamline this process, '\n",
      "         'I learned a lot about best practices / things to consider before '\n",
      "         '(after) putting a model online. These are small things but they do '\n",
      "         'contribute to a production and reliable setup for online inference. '\n",
      "         'Some examples:\\r\\n'\n",
      "         '\\r\\n'\n",
      "         '- Adding meta data & creating a self contained archive\\r\\n'\n",
      "         '- Separating serving sources from training sources\\r\\n'\n",
      "         '- Choosing the requirements of model\\r\\n'\n",
      "         '- Adding an example input & output request\\r\\n'\n",
      "         '- Adding schemas for input and output\\r\\n'\n",
      "         '- Common issues when putting models online: memory leaks, '\n",
      "         'concurrency\\r\\n'\n",
      "         '- Which server is best? Process based or thread based\\r\\n'\n",
      "         '- How different python versions affect inference (execution) time',\n",
      " 'text_template': '{metadata_str}\\n\\n{content}'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a PyData talk Document:\")\n",
    "pprint.pprint(dict(documents[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151cd83-2133-48e6-a7b2-6c152d14a323",
   "metadata": {},
   "source": [
    "### Create vector index from PyData schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe9102-054b-4cc2-b0c0-8d37233b4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-30T17:19:43.632450+0200 - INFO - Building a VectorStoreIndex from 68 documents\n",
      "2023-08-30T17:19:54.067441+0200 - INFO - Saved VectorStoreIndex to indices/pydata_schedule_index\n"
     ]
    }
   ],
   "source": [
    "# create vector index from PyData schedule\n",
    "logger.info(f\"Building a VectorStoreIndex from {len(documents)} documents\")\n",
    "index = llama_index.VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# store index to disk\n",
    "index.storage_context.persist(INDEX_PATH)\n",
    "logger.info(f\"Saved VectorStoreIndex to {INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b317df6-0c8c-4ec8-beb9-ce932bc34a50",
   "metadata": {},
   "source": [
    "## Load vector index with PyData Amsterdam 2023 schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bc3e5-6bc7-4970-9a1f-c722ba931956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-23T16:22:05.678105+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "# load vector index from file\n",
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        # rebuild storage context from disk                                          \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        # load index                                                                 \n",
    "        #index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        index = llama_index.load_index_from_storage(storage_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e4d89-3dea-4809-b073-a4c5efc03ab4",
   "metadata": {},
   "source": [
    "## Create a search engine from vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d0cad-f3bb-4467-b3ed-ec87e5fa966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a search engine\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede9a40-f6e1-42f3-92bf-4519bfad87ef",
   "metadata": {},
   "source": [
    "## Query the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994fdd4-a17d-4cbd-b003-9cc0769b8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- score: 0.35 title: Building a personal search engine with llama-index\n",
      "- score: 0.24 title: Unconference #1\n"
     ]
    }
   ],
   "source": [
    "# query the search engine\n",
    "results = retriever.retrieve(\"llama_index\")\n",
    "for result in results:\n",
    "    talk = talks[result.node.source_node.node_id]\n",
    "    print(f\"- score: {result.score:.2f} title: {talk['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9341a-d56f-4f9c-a372-b46c0e51bacb",
   "metadata": {},
   "source": [
    "### Startups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2d5e8-e67a-4bff-8e89-04f555ee682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- score: 0.32 title: Kickstart AI sponsored drinks [time & location TBD]\n",
      "- score: 0.26 title: Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy\n"
     ]
    }
   ],
   "source": [
    "results = retriever.retrieve(\"startups\")\n",
    "for result in results:\n",
    "    talk = talks[result.node.source_node.node_id]\n",
    "    print(f\"- score: {result.score:.2f} title: {talk['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ee400-d0c5-4ad2-b68e-09c84ce4044c",
   "metadata": {},
   "source": [
    "## Querying the vector index with an external LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2aa750-c144-422f-8e25-4a7d05c12f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d00b1-c133-4af2-b9f7-0e82113797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64136237-b60d-48b5-89d7-e749b71e061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = query_engine.query(\"Which talks are probably interesting for startup founders?\")\n",
    "except openai.error.AuthenticationError as auth_error:\n",
    "    logger.error(auth_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad3806-ad31-4774-a2f4-fbf085d48a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The talks that are probably interesting for startup founders are \"Setting The Right KPIs\" and \"Data-Driven Decision Making.\" These talks discuss topics such as setting realistic and challenging KPIs and leveraging data for informed decision-making and product strategy adjustments, which are relevant for startup founders involved in shaping product strategy and making data-driven decisions.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e121fe9-762f-4c0a-9501-a8332ec1cedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources:\n",
      "- Kickstart AI sponsored drinks [time & location TBD]\n",
      "- Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy\n"
     ]
    }
   ],
   "source": [
    "print(\"Sources:\")\n",
    "for source in response.source_nodes:\n",
    "    print(\"-\", talks[source.node.source_node.node_id]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5ad99-9516-4d7f-913f-b50ec5506cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
