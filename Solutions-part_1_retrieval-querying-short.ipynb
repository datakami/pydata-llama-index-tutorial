{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913d2a2-f87f-4505-abb4-49e6b2291e52",
   "metadata": {},
   "source": [
    "# Part 1: Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e084dce-bd60-43c1-a13d-cd9636ea2856",
   "metadata": {},
   "source": [
    "If you want to use the OpenAPI API during this tutorial, make a file `secret.py` with `openai_api_key = \"YOURKEYHERE\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb43be-6be1-4cd1-933f-cd3a77052e33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb21f09-827a-412c-8b19-c40fd14825a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd39ab-b6ba-4d69-8e8f-39f382d7712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# logging for lazy people :)\n",
    "from loguru import logger\n",
    "\n",
    "# we're not importing specific methods or classes so it's clear when we actually call llama_index!\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b056809-10bc-4f4d-90bf-a9b8e8acd34e",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44daa1-a3e6-45bf-bc6e-8c84fad646b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"{time} - {level} - {message}\", level=\"DEBUG\")\n",
    "logger.add(\"tutorial_part_1.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee221890-7faa-4b1a-b593-ffe15d5976dc",
   "metadata": {},
   "source": [
    "### Tell the notebook where files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e8b1-5b94-4dcf-b109-9e8351b087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/pydata/schedule.json\")\n",
    "INDEX_PATH = Path(\"indices/pydata_schedule_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f10b1-759c-459b-bece-1495c099cf62",
   "metadata": {},
   "source": [
    "### Tell `llama-index` to use a local embeddings model for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420c723-b6f0-4c1f-8a5f-9da6b875747b",
   "metadata": {},
   "source": [
    "More information about this embeddings model: https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "\n",
    "Note that all-minilm-l6-v2 has a maximum size of 256 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b220f-e86c-48e6-a069-ec4d84a2b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"local:sentence-transformers/all-minilm-l6-v2\"\n",
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531cae-9ec3-47f3-b4c1-e9df4cea39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=embed_model, chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b317df6-0c8c-4ec8-beb9-ce932bc34a50",
   "metadata": {},
   "source": [
    "### Load a vector index with the PyData Amsterdam 2023 schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81545f8f-f540-4b5a-bc39-eac70da0771a",
   "metadata": {},
   "source": [
    "Load the index from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bc3e5-6bc7-4970-9a1f-c722ba931956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-30T16:37:54.330814+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede9a40-f6e1-42f3-92bf-4519bfad87ef",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d0cad-f3bb-4467-b3ed-ec87e5fa966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector index `index`\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343d465-8b22-4778-91e7-b33f3c63e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve talks that mention llama-index\n",
    "results = retriever.retrieve(\"llama_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741e34d-0edb-4aae-97d6-f1760ba8e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node=TextNode(id_='e6dd1229-39dd-40db-b8cf-56d1c0bd49b1', embedding=None, metadata={'title': 'Building a personal search engine with llama-index'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f71f1eae-c12e-5b4f-8902-b8d0b8a38d72', node_type=None, metadata={'title': 'Building a personal search engine with llama-index'}, hash='3048db44e02fa581744056cb7a31a225dfdd5eb0ef6d99ecc4c82dc85e0e2872'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='daecb07c-9247-4bca-9dc8-89cfc0250d93', node_type=None, metadata={'title': 'Building a personal search engine with llama-index'}, hash='18dddb7f30eb0d9e79afef5ea3a3fd74a04a8353cbc332d0f5009af31813fdab')}, hash='e6b09c77ade2a5bb7b6c5afc3c9851bc0689fa2625cf85be8831fcc85a0e85ec', text='Using llama-index with OpenAI’s API is optional.If you want to explore postprocessing your results with GPT-3.5, we recommend registering an OpenAI account and making sure you have your OpenAI API key ready.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.5608457858608932\n",
      "\n",
      "node=TextNode(id_='daecb07c-9247-4bca-9dc8-89cfc0250d93', embedding=None, metadata={'title': 'Building a personal search engine with llama-index'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f71f1eae-c12e-5b4f-8902-b8d0b8a38d72', node_type=None, metadata={'title': 'Building a personal search engine with llama-index'}, hash='3048db44e02fa581744056cb7a31a225dfdd5eb0ef6d99ecc4c82dc85e0e2872'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e6dd1229-39dd-40db-b8cf-56d1c0bd49b1', node_type=None, metadata={'title': 'Building a personal search engine with llama-index'}, hash='e6b09c77ade2a5bb7b6c5afc3c9851bc0689fa2625cf85be8831fcc85a0e85ec')}, hash='18dddb7f30eb0d9e79afef5ea3a3fd74a04a8353cbc332d0f5009af31813fdab', text='Building a personal search engine with llama-index\\n\\nWouldn’t it be great to have a Google-like search engine, but then for your own text files and completely private?In this tutorial we’ll build a small personal search engine using open source library llama-index.In this tutorial we will build a small personal search engine using open source library `llama-index`.Llama-index provides utility functions for ingesting various kinds of data, breaking the data up in chunks, building an index of that data using vector embeddings, and retrieving data from the index based on queries.We can even use llama-index to post-process the retrieval results for us using large language models such as GPT.The target audience is people that are already familiar with Python.Participants will experience working with unstructured data, vector embeddings, and explore the possibilities of the recent developments in natural language processing.Workshop materials will be provided via Github before the start of the workshop.For the demo application, we will only use open-source software and models that are light enough to run on an average laptop without a GPU.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.4102764063197005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01462a67-ebcc-4282-a7c0-5fadb0cc75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node=TextNode(id_='3aa39d86-22c4-4215-8263-d4fee2f3a041', embedding=None, metadata={'title': 'The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8', node_type=None, metadata={'title': 'The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls'}, hash='09d3ccd7fff13f4a78184786a5d02a94ebb301f0e20b6ad873d1d070d4066989'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d68f7a36-fc89-44f1-bb4e-41095158b490', node_type=None, metadata={'title': 'The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls'}, hash='c15d2084f0b21e8e4a21b5ded4791bc95fcdc1c399e31eb370cf170423d6d13b')}, hash='c742c9696f5116d28be0742413be8c5802df9b578d6152546874cd13b0dd1b80', text=\"The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls\\n\\nData scientists and analysts are using quasi-experimental methods to make recommendations based on causality instead of randomized control trials.While these methods are easy to use, their assumptions can be complex to explain.This talk will explain these assumptions for data scientists and analysts without in-depth training of causal inference so they can use and explain these methods more confidently to change people's minds using data.Instead of relying solely on randomized control trials (also known as A/B tests), which are considered the gold standard for inferring causality, data scientists and analysts are increasingly turning to quasi-experimental methods to make recommendations based on causality.These methods, including open-source libraries such as CausalImpact (originally an R package but with numerous Python ports), are easy to use, but their assumptions can be complex to explain.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.5411455843020827\n",
      "\n",
      "node=TextNode(id_='dea587a7-93d4-4061-afa7-12856d76e08b', embedding=None, metadata={'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\"}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0b21795f-fb6f-583d-a77d-bceee45fd768', node_type=None, metadata={'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\"}, hash='7a91c5802ae5d6b1f852ab755a02fa2c32823673782773c83ce35e6ded5f3b3f'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='12285416-baab-407e-b1e8-45626a98d769', node_type=None, metadata={'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\"}, hash='0e3fb8eb6fb247e646bb2a884ccd05f76645a5b7a97ddf1d77a9c3f2c246288e')}, hash='06bf2a5debb317c148d053852050648893e2e15db9c9a96567ee99e01981ec9a', text='They will know which libraries to use for different types of data and which methods are most appropriate for different scenarios.This talk could be particularly relevant for Data Scientists wishing to analyze experiments, such as A/B tests, or trying to derive causal statements from observational, non-experimental data.Participants are not expected to have Causal Inference expertise.Yet, a fundamental understanding of Machine Learning and Probability Theory will be beneficial.0-5’: Why Causal Inference and why CATE estimation?5-10’: What are some conceptual ways of estimating CATEs?10-20’: How can we use EconML and CausalML for CATE estimation on a real dataset?20-30’: What are we missing from EconML and CausalML?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.488732702630359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve talks about causal machine learning\n",
    "results = retriever.retrieve(\"causal\")\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b98123-b560-4bc9-b611-3f2db8705262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new retriever that retrieves more than 2 results.\n",
    "retriever = index.as_retriever(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f418a4-19d4-4ec0-b102-714de283c7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls\n",
      "\n",
      "Data scientists and analysts are using quasi-experimental methods to make recommendations based on causality instead of randomized control trials.While these methods are easy to use, their assumptions can be complex to explain.This talk will explain these assumptions for data scientists and analysts without in-depth training of causal inference so they can use and explain these methods more confidently to change people's minds using data.Instead of relying solely on randomized control trials (also known as A/B tests), which are considered the gold standard for inferring causality, data scientists and analysts are increasingly turning to quasi-experimental methods to make recommendations based on causality.These methods, including open-source libraries such as CausalImpact (originally an R package but with numerous Python ports), are easy to use, but their assumptions can be complex to explain.\n",
      "---------------\n",
      "They will know which libraries to use for different types of data and which methods are most appropriate for different scenarios.This talk could be particularly relevant for Data Scientists wishing to analyze experiments, such as A/B tests, or trying to derive causal statements from observational, non-experimental data.Participants are not expected to have Causal Inference expertise.Yet, a fundamental understanding of Machine Learning and Probability Theory will be beneficial.0-5’: Why Causal Inference and why CATE estimation?5-10’: What are some conceptual ways of estimating CATEs?10-20’: How can we use EconML and CausalML for CATE estimation on a real dataset?20-30’: What are we missing from EconML and CausalML?\n",
      "---------------\n",
      "We will begin by providing an overview of the theory behind CATE estimation, how it fits into the broader field of causal inference and how Machine Learning has recently broken into CATE estimation.We will then dive into the various libraries available for Python, discussing their strengths and weaknesses and providing real-world examples of their usage.Specifically, we will cover:\n",
      "- EconML: An open-source library for general Causal Inference purposes, by Microsoft Research\n",
      "- CausalML: An open-source library for uplift modeling in particular, by Uber\n",
      "\n",
      "We will compare and contrast these libraries with respect to CATE estimation, discussing which methods they use, which assumptions they make, and which types of data they are best suited for.We will also provide code examples to illustrate how to use each library in practice.Moreover, we will discuss what we think is missing from both of them.By the end of the talk, attendees will have a solid understanding of the Python tooling and ecosystem for estimating CATEs in a causal inference setting.\n",
      "---------------\n",
      "I will break down these assumptions and explain how they can help practitioners determine when to use these methods (and when not to use them), using examples from the world of digital language learning.The key takeaway is that when it comes to changing people's minds using data, explaining assumptions to decision-makers is just as important as understanding the underlying statistics.**Outline**\n",
      "- Minute 0-5: Introduction and Motivation\n",
      "- Minute 5-10: Difference-in-Difference / Bayesian Structural Time-Series\n",
      "- Minute 10-15: Case - Conversion effects of content changes based language-pair specific releases at Babbel\n",
      "- Minute 15-20: Regression Discontinuity Design\n",
      "- Minute 20-25: Case - Estimating motivational effects of language assessment\n",
      "- Minute 25-30: Wrap-up / Take-Aways\n",
      "\n",
      "Attendees should have basic knowledge of statistics and causality to get the most out of this talk.\n",
      "---------------\n",
      "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry\n",
      "\n",
      "The Difference-in-Differences (DiD) methodology is a popular causal inference method utilized by leading tech firms such as Microsoft Research, LinkedIn, Meta, and Uber.Yet recent studies suggest that traditional DiD methods may have significant limitations when treatment timings differ.An effective alternative is the implementation of the staggered DiD design.We exemplify this by investigating an interesting question in the music industry: Does featuring a song in TV shows influence its popularity, and are there specific factors that could moderate this impact?Difference-in-differences (DiD) is a causal inference method frequently used in empirical research in industry and academia.However, standard DiD has limitations when interventions occur at different times or affect varying groups.This talk will highlight the application of the Staggered DiD method, a more nuanced approach that addresses these limitations, in the context of the music industry.\n",
      "---------------\n",
      "Additionally, we explain how the open source Python package developed at Uber, CausalML, can help others in successfully making the transition from correlation-driven machine learning to causal-driven machine learning.\n",
      "---------------\n",
      "Causal Inference Libraries: What They Do, What I'd Like Them To Do\n",
      "\n",
      "This talk will explore the Python tooling and ecosystem for estimating conditional average treatment effects (CATEs) in a Causal Inference setting.Using real world-examples, it will compare and contrast the pros and cons of various existing libraries as well as outline desirable functionalities not currently offered by any public library.Conditional average treatment effects (CATEs) are a fundamental concept in Causal Inference, allowing for the estimation of the effect of a particular treatment or intervention.For CATEs, the effect estimation is not only with respect to an entire population, e.g.all experiment participants, but rather with respect to units, e.g.a single experiment participant, with individual characteristics.This can be very important to meaningfully personalize services and products.In this talk, we will explore the Python tooling and ecosystem for estimating CATEs, including libraries such as EconML and CausalML.\n",
      "---------------\n",
      "Personalization at Uber scale via causal-driven machine learning\n",
      "\n",
      "In this talk, we outline how we introduced causality into our machine learning models within the core checkout and onboarding experiences globally, thereby strongly improving our key business metrics.We discuss case studies, where experimental data were combined with machine learning in order to create value for our users and personalize their experiences, and we share our lessons learned with the goal to inspire attendees to start incorporating causality into their machine learning solutions.Additionally, we explain how the open source Python package developed at Uber, CausalML, can help others in successfully making the transition from correlation-driven machine learning to causal-driven machine learning.In this talk, we outline how we introduced causality into our machine learning models within the core checkout and onboarding experiences globally, thereby strongly improving our key business metrics.We discuss case studies, where experimental data were combined with machine learning in order to create value for our users and personalize their experiences, and we share our lessons learned with the goal to inspire attendees to start incorporating causality into their machine learning solutions.\n",
      "---------------\n",
      "We will try to answer the question of how music features in TV shows affect music popularity and how this effect might change for different types of music using the staggered DiD method.Attendees will gain an understanding of causal inference through observational studies and specifically how the new DiD methods are used through an interesting and original case study.The talk will be structured as follows:\n",
      "\n",
      "1.Intro to the case (e.g., background on music features on TV, dataset) (5 mins)\n",
      "2.Explanation of the DiD approach and its limitations.(5 mins)\n",
      "3.Introduction to the Staggered DiD method.(5 mins)\n",
      "4.Application of staggered DiD for the case study from the music industry (10 mins)\n",
      "5.Conclusions (5 mins)\n",
      "\n",
      "Target Audience: The talk would be beneficial for data scientists, researchers, and practitioners interested in causal inference, marketing analytics, and quasi-experimental design.Attendees should have a basic understanding of statistical methods used in data science.Key Takeaways:\n",
      "\n",
      "1.\n",
      "---------------\n",
      "Understanding of the DiD approach and its limitations in the context of analyses with observational data.2.Insights into the Staggered DiD method and its application.3.Practical knowledge about executing and evaluating DiD studies effectively.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Find all talks about causal inference at PyData\n",
    "results = retriever.retrieve('causal')\n",
    "for r in results:\n",
    "    print(r.node.text)\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ee400-d0c5-4ad2-b68e-09c84ce4044c",
   "metadata": {},
   "source": [
    "## Querying the vector index with an external LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ced37-8de0-4d5e-8abb-6fae18bf1803",
   "metadata": {},
   "source": [
    "### Set OpenAI API key (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5b922-be03-435f-b581-053dfb47e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret import openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fc70c-f18f-412c-bae5-3ac06a090a92",
   "metadata": {},
   "source": [
    "### Use OpenAI's gpt-3.5-turbo for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6ce8a-85d1-4d29-a182-a749782d14c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m llama_index\u001b[38;5;241m.\u001b[39mllms\u001b[38;5;241m.\u001b[39mOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39m\u001b[43mopenai_api_key\u001b[49m)\n\u001b[1;32m      2\u001b[0m service_context \u001b[38;5;241m=\u001b[39m llama_index\u001b[38;5;241m.\u001b[39mServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m      3\u001b[0m   embed_model\u001b[38;5;241m=\u001b[39membed_model, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, llm\u001b[38;5;241m=\u001b[39mllm\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'openai_api_key' is not defined"
     ]
    }
   ],
   "source": [
    "llm = llama_index.llms.OpenAI(model=\"gpt-3.5-turbo\", api_key=openai_api_key)\n",
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=embed_model, chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c91e61-7b94-4a1c-a68b-b22553e8b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-30T16:38:04.830107+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d00b1-c133-4af2-b9f7-0e82113797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64136237-b60d-48b5-89d7-e749b71e061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find talks that might be interesting for startup founders\n",
    "response = query_engine.query(\"Which talks are probably interesting for startup founders?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad3806-ad31-4774-a2f4-fbf085d48a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "title: Kickstart AI sponsored drinks [time & location TBD]\n",
      "\n",
      "Kickstart AI sponsored drinks [time & location TBD]\n",
      "\n",
      "Kickstart AI is a foundation powered by a coalition of iconic Dutch brands (Ahold Delhaize, ING, KLM and NS). Their mission is to accelerate AI adoption in the Netherlands, and improve society through the use of AI.\n",
      "\n",
      "Lorem ipsum dolor\n",
      "\n",
      "title: LLM Agents 101: How I Gave ChatGPT Access to My To-Do List\n",
      "\n",
      "We'll showcase these solutions through amusing moments and challenges encountered during the development of my to-do list agent.- A summary of what works well and what still needs improvement\n",
      "\n",
      "This talk is designed as an introduction to LLM agents.Throughout the presentation, I will aim to maintain a high-level perspective to ensure that even less technical audience members can grasp the concepts.To achieve this, I will share entertaining situations where my agent did something unexpected and how I resolved those issues.However, the presentation will still be practical enough for technical people to know all the concepts and techniques to develop their LLM agents.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: Which talks are probably interesting for startup founders?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc130808-7ec1-4ec0-aeaf-92b2a4ec72ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
