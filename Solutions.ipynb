{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713706da-8734-4481-8b35-8587ecd1fecc",
   "metadata": {},
   "source": [
    "# Building a personal search engine with llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e232f85-d487-4986-8482-97d054fe5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df87a15c-b98f-4c1d-babe-7bd7f73645e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, download_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222ee787-604b-4ba7-bcfe-0b4e3cbdd464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c7998f-b778-4fa1-a8cf-8ea40d2f7958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(\"tutorial.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b21faf-484c-475c-8cf9-3fd4eb138d15",
   "metadata": {},
   "source": [
    "## 1. Ingesting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b484b5-0379-4b7f-8735-3d0ffc353c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # tutorial: load data from data/ folder\n",
    "    documents = SimpleDirectoryReader('data').load_data()\n",
    "    return documents\n",
    "\n",
    "documents = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97222f8-de96-46f6-a3c0-b63a052c85d3",
   "metadata": {},
   "source": [
    "## 2. Chunking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8229bf-2e4a-47e0-8a0c-3d6b127d5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default stuff is fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb2102-e313-4a8d-aac5-3b226515c365",
   "metadata": {},
   "source": [
    "## 3. Building an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33acf16e-aaa7-417a-b9bb-744a04aa5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(documents):\n",
    "    logger.info(\"Building a GPTVectorStore from documents\")\n",
    "    # create a vector store index\n",
    "    # uses OpenAI API by default!\n",
    "    index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    # store index to disk\n",
    "    index.storage_context.persist()\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247a69b-3337-4c5a-a660-d70dfe5e8b9a",
   "metadata": {},
   "source": [
    "## 4. Querying the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b898d9a-62c6-4997-b2a1-a5328082dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(index, question=\"What does Rumelt write about industry dynamics?\"):\n",
    "    # query the new index with a question\n",
    "    # uses OpenAI API by default\n",
    "    query_engine = index.as_query_engine()\n",
    "    logger.info(f\"Querying index with question {question}\")\n",
    "    response = query_engine.query(question)\n",
    "    logger.info(f\"Received response: {response.response.strip()}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a2069a6-543d-4693-ab48-d9559bb1dbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='9e13c83b-1af7-466e-8c4c-6c3c3c6f05ce', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5bad56206015af5e33bed0782e68fa9023cf08be19be8d26330dd650ebdd105f', text=\"\\n\\nOmissions\\n\\n- In general, our investigation has centered on commercially usable models. There are a lot of fine-tuned llamas that have been omitted, but they are generally not that different.\\n- **Unaligned chat/instruct models**: not a big research focus, as they are all based on llama. However, they may be useful in case you're hitting “Sorry, as an AI language model..” a lot.\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5863df73-c115-4821-8db0-a4e39b4ad09c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8119925119158186114c2c741a6174f943fd0445161a5059467141f965ea38b7', text=\"\\n\\nVarious Restrictions\\n\\nThe Models section uses various tags for usage restrictions:\\n\\n- **Non-commercial** means the weights have been released under a license that prohibits commercial use.\\n- **No-Evil** means the weights have been released under a license that prohibits use for illegal or unethical activities. There are two licenses in wide use.\\n    - **The Together license** Includes some examples of misuse.\\n        \\n        Using the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\\n        \\n        - Generating fake news, misinformation, or propaganda\\n        - Promoting hate speech, discrimination, or violence against individuals or groups\\n        - Impersonating individuals or organizations without their consent\\n        - Engaging in cyberbullying or harassment\\n        - Defamatory content\\n        - Spamming or scamming\\n        - Sharing confidential or sensitive information without proper authorization\\n        - Violating the terms of use of the model or the data used to train it\\n        - Creating automated bots for malicious purposes such as spreading malware, phishing scams, or spamming\\n        \\n    - **The RAIL license**: License - a Hugging Face Space by bigscience\\n        \\n    \\n    While this is (hopefully) not a problem for your intended use, it's good to keep this restriction in mind, and don't consider the model fully open.\\n\\n<!-- comment\\nHow to generate:\\n- notion: export as markdown + csv\\n- pandoc -i Models\\\\ 1f4e7a77004c4aed97864b551b9a00c4.csv -f csv -t gfm | wl-copy\\n- clean up links\\n-->\\n    \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6aa8a6f5-96c5-495b-86b1-44c1774243fe', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='792b4e3a35603d8a5d0d0c2ed4a577c6089b1334c4529ac404bd6780421888fa', text='\\n\\nModel list\\n\\n\\n| Name                      | Tags           | Trained    | Based on              | Company              | Date       | Dataset                                 | Params            |\\n|---------------------------|----------------|------------|-----------------------|----------------------|------------|-----------------------------------------|-------------------|\\n| GPT-2                     |                | Completion |                       | OpenAI               | 2019/02/14 |                                         | 1.5B              |\\n| gpt-j-6b                  |                | Completion |                       | EleutherAI           | 2021/05/01 | The Pile                                | 6B                |\\n| GPT-NeoX-20B              |                | Completion |                       | EleutherAI           | 2022/04/14 | The Pile                                | 20B               |\\n| BLOOM                     | No-Evil        | Completion |                       | BigScience           | 2022/07/06 |                                         | 176B              |\\n| flan-t5                   |                | Completion |                       | Google               | 2022/10/20 |                                         | 11B, 3B           |\\n| RWKV-4-pile               |                | Completion |                       |                      | 2022/10/22 | The Pile                                |                   |\\n| Llama                     | Non-commercial | Completion |                       | Meta                 | 2023/02/24 |                                         | 13B, 33B, 65B, 7B |\\n| oasst-pythia-12b          |                | Assistant  | Pythia                | LAION                | 2023/03/07 | openassistant/oasst1                    | 12B               |\\n| Dolly                     |                | Instruct   | Pythia                | Databricks           | 2023/03/24 | databricks-dolly-15k                    | 12B, 3B, 6B, 7B   |\\n| Cerebras-GPT-13B          |                | Completion |                       | Cerebras             | 2023/03/28 | The Pile                                | 13B               |\\n| Vicuna                    | Non-commercial | Chat       | Llama                 | LMSYS                | 2023/03/30 | ShareGPT                                | 13B, 7B           |\\n| GPT-NeoXT-Chat-Base-20B   | No-Evil        | Chat       | GPT-NeoX-20B          | Together             | 2023/03/30 |                                         |                   |\\n| RWKV-4-raven              |                | Chat       | RWKV-4-pile           | BlinkDL              | 2023/04/01 | ShareGPT                                | 1.5B, 14B, 3B, 7B |\\n| Koala                     | Non-commercial | Chat       | Llama                 | Berkeley AI Research | 2023/04/03 |                                         | 13B, 7B           |\\n| Pythia                    |                | Completion | GPT-NeoX-20B          | EleutherAI           | 2023/04/03 | The Pile                                |                   |\\n| gpt4all-j                 |                | Chat       | gpt-j-6b              | Nomic                | 2023/04/12 | databricks-dolly-15k, ShareGPT          | 6B                |\\n| Fastchat-T5-3b-v1.0       |                | Chat       | flan-t5               | LMSYS                | 2023/05/01 | ShareGPT                                | 3B                |\\n| StarCoder                 |                | Code       |                       |                      | 2023/05/03 | The Stack                               | 16B               |\\n| MPT-7B-storywriter-65k+   |                | Completion | MPT-7B                | MosaicML             | 2023/05/05 |                                         | 7B                |\\n| MPT-7B-Instruct           |                | Instruct   | MPT-7B                | MosaicML             | 2023/05/05 | databricks-dolly-15k, Anthropic/hh-rlhf | 7B                |\\n| MPT-7B-Chat               |                | Chat       | MPT-7B                | MosaicML             | 2023/05/05 |                                         | 7B                |\\n| MPT-7B                    |                | Completion |                       | MosaicML             | 2023/05/05 |                                         | 7B                |\\n| RedPajama-INCITE-Chat     | No-Evil        | Chat       | RedPajama-INCITE-Base | Together             | 2023/05/05 | RedPajama                               | 3B, 7B            |\\n| RedPajama-INCITE-Instruct | No-Evil        | Instruct   | RedPajama-INCITE-Base | Together             | 2023/05/05 | RedPajama                               | 3B, 7B            |\\n| RedPajama-INCITE-Base     | No-Evil        | Completion |                       | Together             | 2023/05/05 | RedPajama                               | 3B, 7B            |\\n| StarCoderPlus             | No-Evil        | Completion | StarCoder             | bigcode              | 2023/05/08 | RefinedWeb                              | 16B               |\\n| Falcon-rw                 |                | Completion |                       | TII UAE              | 2023/06/05 | RefinedWeb                              | 1B, 7B            |\\n| Falcon-Instruct           |                | Chat       | Falcon                | TII UAE              | 2023/06/05 |                                         | 40B, 7B           |\\n| Falcon                    |                | Completion |                       | TII UAE              | 2023/06/05 |                                         | 40B, 7B           |\\n| StarChat-β                | No-Evil        | Chat       | StarCoderPlus         | Huggingface          | 2023/06/07 | openassistant/oasst1                    | 16B               |\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0e7a4bc4-1fe6-4ad8-88dd-e7ffe9701ed6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='180a14b289701d33576487f40a8914ffa5932ee299598b0915053f1be8f85f14', text='\\n\\nDatasets\\n\\n| Name                 | Group      |\\n|----------------------|------------|\\n| The Pile             | EleutherAI |\\n| ShareGPT             |            |\\n| RedPajama            | Together   |\\n| databricks-dolly-15k | Databricks |\\n| Anthropic/hh-rlhf    |            |\\n| openassistant/oasst1 | LAION      |\\n| RefinedWeb           | TII UAE    |\\n| The Stack            | bigcode    |\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b02ff811-819e-4c2c-8cf1-df3a2c41a3da', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='36b7b7dc418143f598bb5cd4d232e8562acedf873a8e1504dcab6a45546ac979', text='\\n\\nGroups\\n\\n| Name                        | Tags           | URL                             |\\n|-----------------------------|----------------|---------------------------------|\\n| EleutherAI                  | Research group | https://www.eleuther.ai/        |\\n| Google                      |                | https://ai.googleblog.com/      |\\n| Meta                        |                | https://ai.facebook.com/        |\\n| Cerebras                    |                | https://www.cerebras.net/       |\\n| LMSYS                       | Research group | https://lmsys.org/              |\\n| Nomic                       |                | https://nomic.ai/               |\\n| OpenAI                      |                | https://openai.com/             |\\n| Together                    |                | https://www.together.xyz/       |\\n| Databricks                  |                | https://www.databricks.com/     |\\n| MosaicML                    |                | https://www.mosaicml.com/       |\\n| Berkeley AI Research (BAIR) | Research group | https://bair.berkeley.edu/blog/ |\\n| BigScience                  |                |                                 |\\n| LAION                       |                |                                 |\\n| BlinkDL                     |                |                                 |\\n| TII UAE                     |                |                                 |\\n| Huggingface                 |                |                                 |\\n| bigcode                     |                |                                 |\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42435d01-1333-4a8e-9c8c-648547a3acf4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='46224c0f52a891d99184fdd28872431d04abfcb77255e3f0338ff981feed7da8', text=\"\\n\\nRising fixed costs\\n\\nWhen a crucial resource or activity becomes increasingly expensive over time, it can lead to changes in consumer and company behaviors.\\n\\nThe costs for training a language model with average performance from scratch have significantly increased, especially since 2020. Training a model that is on par with GPT-4, and then running it in production is out of reach for most companies. OpenAI's foundational models have become too large to train (and run) on regular infrastructure, which lead to their collaboration with industry giant Microsoft.\\n\\nIf smaller companies want to get access to high performing AI models, they have a few options:\\n\\n- build on top of proprietary foundational models, risking being dependent on their model supplier;\\n- find creative ways to get the same performance from smaller models or domain-specific data;\\n- find cheap sources of compute and storage, in-house or at cloud providers.\\n\\nOpenAI has a virtual monopoly on high-performing general use LLMs at the moment, and they might raise their prices in the future. When that happens, I expect more people will consider using open source models instead, many of which are currently being developed.\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e12906ca-5416-4d1a-9b40-8af249452df8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7e7b9220c4dcc870a2d149533e71ca1c30cd5554118471c0f8dbf7f9abb413b4', text='\\n\\nPredictable biases in forecasting\\n\\nBiases in our thinking can hinder our understanding of the current situation. Rumelt gives examples: many people assume that something that has been growing in the past will continue to grow indefinitely (for example revenue, share prices). Other fallacies are: expecting that new industry developments will always result in battles between industry leaders, and assuming that industry best practices will be determined solely by current incumbents.\\n\\nI see a lot of these biases and blind spots in action at the moment, ranging from rose-colored glasses (\"AI will solve all our problems”) to doomsday thinking (\"AI will lead to human extinction\").\\n\\nPeople seem to expect AI capabilities to increase exponentially in the future, without any interruptions or plateaus. However, AI is susceptible to hype cycles, where we alternate between periods of optimism and increased attention for AI, and periods of skepticism and lack of interest in AI applications. For example, working on neural networks in the 90s was considered career suicide in academia.\\n\\nAt this moment, we might be running into the limitations of Transformers-based large language models for the first time. Issues such as hallucination, limited context length, biases, and inconsistent performance need to be addressed if we want to see large-scale adoption. We don’t have any new innovations that structurally deal with these problems — yet.\\n\\nIn the meantime, the share prices of companies that are supposedly active in the AI domain have gone through the roof, even though most real AI companies are hemorrhaging money and struggling with user retention. I don\\'t doubt that we will soon see the release of GPT-5, a model even more gargantuan than its predecessor. But is larger always better, for language models? (So far, it seems the answer is yes, but if we can’t come up with ways to scale the quality of training data as well, we’re in for large language models that are trained on conspiracy theories, white supremacy and other toxic content.) And besides model size, should we let OpenAI determine the best practices concerning AI ethics, user data handling, LLM safety, and interface design?\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='11f13210-0a11-42de-8eb0-dc97b7b2a8f8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='94e0e535172b52af853b59088795530d2a146d7efeea4e6dac933de14fc77084', text='\\n\\nIncumbent response\\n\\nHow do established industry leaders react to new developments within their industry? Often, they either take no action or focus on protecting their intellectual property, falling victim to first-mover disadvantage.\\n\\nAI is more a technological approach than a coherent industry, so it\\'s hard to pinpoint which companies can be considered AI companies. There are very few \"pure AI\" public companies, as most are regular technology companies using AI as a tool in their toolbelt. As the sector is so new and unconsolidated, there are very few large pure AI private companies like OpenAI. So who are the industry leaders in LLMs, apart from OpenAI?\\n\\nZooming out to the technology sector at large, the renewed battle between Microsoft and Google in the search engine space is very illustrative of Rumelt\\'s notion of \"incumbent response\". It has turned into a veritable arms race, where both companies try to be the first to release a new technology. This resulted in in hurried presentations of early-stage technology, that might not be quite yet ready for large-scale production release. Both OpenAI and Microsoft\\'s infrastructure are groaning under the weight of ChatGPT and Bing inference requests, and Google had to retract the integration of their LLM Bard after it predictably made an error during the launch presentation.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='07843e64-fbb4-4606-acf3-e95edf6ecd65', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a917bd6fa268ab9fb39d530c310788260262a3c884382aad1c942a05d8375472', text='\\n\\nIndustry attractor state\\n\\nWhat would the industry look like if it efficiently met market demands? Which developments and business decisions move the industry closer to this state, and which hinder progress in reaching it?\\n\\nGenerative AI still needs to mature as an industry. We need proper tooling for understanding, building, monitoring, and maintaining generative AI systems. Businesses need to gain better control over their data, because this is a prerequisite to building proprietary unique AI systems. After all, most language models are supplied as API endpoints, and if you use the vanilla version of an LLM, your competitors can use the exact same technology as you. We also need to explore alternative business models for generative AI, beyond mobile apps and SaaS. And businesses need to address AI activism, deal with concerns about privacy and copyright, come up with an AI usage policy for employees, and make plans for responsible stewardship of customer data. All these developments are just starting.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0b9202ae-1adf-4a0f-8e0c-bb11d215e653', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='11a97b4f63cc9c52e7bdcba7544b31b252739ad0635878398b7d268434550bae', text=\"\\n\\nSo... what now?\\n\\nRumelt’s guideposts raise more questions than they answer, but they provide a framework to reflect on the changes happening in the generative AI space. His case studies illustrate that small upstart companies can beat their behemoth competitors, if they manage to strategically use sweeping changes to their advantage. If this applies to inventions like microprocessors and GPU chips, it must hold for generative AI as well.\\n\\nWe're *in* the change as it happens, and the best we can do is try to observe it as objectively as possible, without being tricked by our biases and expectations. The generative AI domain needs our proactive thinking to grow into a mature industry. We need to think and talk about all the thorny issues of applied generative AI: from user data stewardship to model explainability, data quality to user interfaces, and scalable infrastructure to long-term model monitoring. Building and learning in public can help with that.\\n\\nWe are in luck: even though generative AI is high-tech, it is not solely in the hands of a few big companies. Most natural language processing techniques are described in open-access research papers, and many technologies underlying large language models are open source. As a result, open source projects, individual researcher-engineers, startups building in public, and user communities can contribute as much to the direction of generative AI as large technology companies.\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6a6e221a-1fb2-45ed-8124-40cd26aaebaf', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='81b24d3afb5510232b34dcc9905eb46ba3e838df39410b18dca15a0e5fb5b796', text=\"\\n\\nRelated reads\\n\\n- Bert Hubert’s startup library page. Bert recommended Rumelt's book to me.\\n- John Kay’s review of Good Strategy/Bad Strategy\\n- Andreessen Horowitz report Who owns the generative AI platform?\\n\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae082b-d7d0-4b7f-b448-e3a4e5842930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
