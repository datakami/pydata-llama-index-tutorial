{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913d2a2-f87f-4505-abb4-49e6b2291e52",
   "metadata": {},
   "source": [
    "# Part 1: Retrieving and querying\n",
    "\n",
    "This notebook creates a vector index with the PyData Amsterdam 2023. This index is used for the Exercises of part 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd39ab-b6ba-4d69-8e8f-39f382d7712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pprint # dev\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# loguru: logging for lazy people :)\n",
    "from loguru import logger\n",
    "\n",
    "# llama_index: the topic of this tutorial\n",
    "# we're not importing specific methods or classes so it's clear when we actually call llama_index!\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5b922-be03-435f-b581-053dfb47e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret import openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44daa1-a3e6-45bf-bc6e-8c84fad646b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log to stdout and local file\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"{time} - {level} - {message}\", level=\"DEBUG\")\n",
    "logger.add(\"tutorial_part_1.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e8b1-5b94-4dcf-b109-9e8351b087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_PATH = Path(\"data/pydata/schedule.json\")\n",
    "INDEX_PATH = Path(\"indices/pydata_schedule_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb43be-6be1-4cd1-933f-cd3a77052e33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f10b1-759c-459b-bece-1495c099cf62",
   "metadata": {},
   "source": [
    "### Use a local embeddings model\n",
    "\n",
    "(So no calls to OpenAI APIs :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b220f-e86c-48e6-a069-ec4d84a2b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531cae-9ec3-47f3-b4c1-e9df4cea39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all-minilm-l6-v2 has a maximum size of 256 tokens\n",
    "# source: https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=\"local:sentence-transformers/all-minilm-l6-v2\", chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b968eeb-82c6-49c4-88bf-3b776dd1ec49",
   "metadata": {},
   "source": [
    "### Load PyData schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c312d-62ed-492c-9466-4aa7f96f0a89",
   "metadata": {},
   "source": [
    "**Load JSON file with the PyData Amsterdam 2023 schedule**\n",
    "* source: https://amsterdam2023.pydata.org/cfp/schedule/export/schedule.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8ada1-48d2-4ab1-982d-177727d7d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04T11:32:06.998790+0200 - INFO - Loaded the PyData schedule JSON from file data/pydata/schedule.json\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'r') as infile:\n",
    "    schedule = json.load(infile)\n",
    "    logger.info(f\"Loaded the PyData schedule JSON from file {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd308a6-84e7-4964-aa90-205cc6b2556c",
   "metadata": {},
   "source": [
    "**Extract the talks from the schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae6ab9-adc3-4456-9212-6a50a37fdb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04T11:32:07.530362+0200 - INFO - Loaded 68 talks from the PyData schedule JSON!\n"
     ]
    }
   ],
   "source": [
    "talks = {}\n",
    "for day in schedule['schedule']['conference']['days']:\n",
    "    for room in day['rooms'].values():\n",
    "        for talk in room:\n",
    "            talk['filename'] = str(DATA_PATH)\n",
    "            talk['category'] = \"Conference talk at PyData Amsterdam 2023\"\n",
    "            talks[talk['guid']] = talk\n",
    "\n",
    "logger.info(f\"Loaded {len(talks)} talks from the PyData schedule JSON!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fe918-339e-4de3-a614-f73a44f4b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a PyData talk:\n",
      "{'abstract': 'Working on ML serving for couple of years we learned a lot. I '\n",
      "             'would like to share a set of best practices / learnings with the '\n",
      "             'community',\n",
      " 'answers': [],\n",
      " 'attachments': [],\n",
      " 'category': 'Conference talk at PyData Amsterdam 2023',\n",
      " 'date': '2023-09-14T12:00:00+02:00',\n",
      " 'description': 'At Adyen we deploy a lot of models for online inference in '\n",
      "                'the payment flow. Working in the MLOps team to streamline '\n",
      "                'this process, I learned a lot about best practices / things '\n",
      "                'to consider before (after) putting a model online. These are '\n",
      "                'small things but they do contribute to a production and '\n",
      "                'reliable setup for online inference. Some examples:\\r\\n'\n",
      "                '\\r\\n'\n",
      "                '- Adding meta data & creating a self contained archive\\r\\n'\n",
      "                '- Separating serving sources from training sources\\r\\n'\n",
      "                '- Choosing the requirements of model\\r\\n'\n",
      "                '- Adding an example input & output request\\r\\n'\n",
      "                '- Adding schemas for input and output\\r\\n'\n",
      "                '- Common issues when putting models online: memory leaks, '\n",
      "                'concurrency\\r\\n'\n",
      "                '- Which server is best? Process based or thread based\\r\\n'\n",
      "                '- How different python versions affect inference (execution) '\n",
      "                'time',\n",
      " 'do_not_record': False,\n",
      " 'duration': '00:30',\n",
      " 'filename': 'data/pydata/schedule.json',\n",
      " 'guid': '80309313-5f6a-58dd-b81f-730483b2dd6b',\n",
      " 'id': 28,\n",
      " 'language': 'en',\n",
      " 'links': [],\n",
      " 'logo': '',\n",
      " 'persons': [{'answers': [],\n",
      "              'biography': 'Staff Engineer @ Adyen. I am passionate about high '\n",
      "                           'performance distributed systems. Recently I was '\n",
      "                           \"working on scaling Adyen's Data & ML \"\n",
      "                           'infrastructure.',\n",
      "              'code': 'RBKUVL',\n",
      "              'id': 42,\n",
      "              'public_name': 'Ziad Al Moubayed'}],\n",
      " 'recording_license': '',\n",
      " 'room': 'Bar',\n",
      " 'slug': 'cfp-28-reliable-and-scalable-ml-serving-best-practices-for-online-model-deployment',\n",
      " 'start': '12:00',\n",
      " 'subtitle': '',\n",
      " 'title': 'Reliable and Scalable ML Serving: Best Practices for Online Model '\n",
      "          'Deployment',\n",
      " 'track': None,\n",
      " 'type': 'Talk',\n",
      " 'url': 'https://amsterdam2023.pydata.org/cfp/talk/NSZFRB/'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a PyData talk:\")\n",
    "pprint.pprint(list(talks.values())[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaeac5c-91f1-4899-b1dc-714c45cbe17e",
   "metadata": {},
   "source": [
    "**Turn the talks data into llama_index Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b91d79-a54f-4f1a-aaf3-8ce1c01dbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for talk in talks.values():\n",
    "    talk_text = f\"{talk['title']}\\n\\n{talk['abstract']}\\n\\n{talk['description']}\"\n",
    "    speakers = \", \".join([p['public_name'] for p in talk['persons']])\n",
    "    doc = llama_index.Document(text = talk_text, id_ = talk[\"guid\"], extra_info={'title': talk['title'], 'speakers' : speakers})\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68d2b6-23ea-4a9d-bc9c-87743dbec032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a PyData talk Document:\n",
      "{'embedding': None,\n",
      " 'end_char_idx': None,\n",
      " 'excluded_embed_metadata_keys': [],\n",
      " 'excluded_llm_metadata_keys': [],\n",
      " 'hash': 'e1a90272b9b8d29972d9674b7c1ab976aca6fdf7ce73f49b1ca7432c90123c74',\n",
      " 'id_': '80309313-5f6a-58dd-b81f-730483b2dd6b',\n",
      " 'metadata': {'speakers': 'Ziad Al Moubayed',\n",
      "              'title': 'Reliable and Scalable ML Serving: Best Practices for '\n",
      "                       'Online Model Deployment'},\n",
      " 'metadata_seperator': '\\n',\n",
      " 'metadata_template': '{key}: {value}',\n",
      " 'relationships': {},\n",
      " 'start_char_idx': None,\n",
      " 'text': 'Reliable and Scalable ML Serving: Best Practices for Online Model '\n",
      "         'Deployment\\n'\n",
      "         '\\n'\n",
      "         'Working on ML serving for couple of years we learned a lot. I would '\n",
      "         'like to share a set of best practices / learnings with the '\n",
      "         'community\\n'\n",
      "         '\\n'\n",
      "         'At Adyen we deploy a lot of models for online inference in the '\n",
      "         'payment flow. Working in the MLOps team to streamline this process, '\n",
      "         'I learned a lot about best practices / things to consider before '\n",
      "         '(after) putting a model online. These are small things but they do '\n",
      "         'contribute to a production and reliable setup for online inference. '\n",
      "         'Some examples:\\r\\n'\n",
      "         '\\r\\n'\n",
      "         '- Adding meta data & creating a self contained archive\\r\\n'\n",
      "         '- Separating serving sources from training sources\\r\\n'\n",
      "         '- Choosing the requirements of model\\r\\n'\n",
      "         '- Adding an example input & output request\\r\\n'\n",
      "         '- Adding schemas for input and output\\r\\n'\n",
      "         '- Common issues when putting models online: memory leaks, '\n",
      "         'concurrency\\r\\n'\n",
      "         '- Which server is best? Process based or thread based\\r\\n'\n",
      "         '- How different python versions affect inference (execution) time',\n",
      " 'text_template': '{metadata_str}\\n\\n{content}'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a PyData talk Document:\")\n",
    "pprint.pprint(dict(documents[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151cd83-2133-48e6-a7b2-6c152d14a323",
   "metadata": {},
   "source": [
    "### Create vector index from PyData schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe9102-054b-4cc2-b0c0-8d37233b4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04T11:32:19.446396+0200 - INFO - Building a VectorStoreIndex from 68 documents\n"
     ]
    }
   ],
   "source": [
    "# create vector index from PyData schedule\n",
    "logger.info(f\"Building a VectorStoreIndex from {len(documents)} documents\")\n",
    "index = llama_index.VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# store index to disk\n",
    "index.storage_context.persist(INDEX_PATH)\n",
    "logger.info(f\"Saved VectorStoreIndex to {INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b317df6-0c8c-4ec8-beb9-ce932bc34a50",
   "metadata": {},
   "source": [
    "### Load vector index from storage to test if everything worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bc3e5-6bc7-4970-9a1f-c722ba931956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04T11:30:16.493377+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "try:                                                                             \n",
    "    # rebuild storage context from disk                                          \n",
    "    storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "    # load index                                                                 \n",
    "    index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "    logger.info(\"Loaded index from local storage\")                               \n",
    "except Exception as e:                                                           \n",
    "    logger.error(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc7b3a5-91b4-47f8-a8eb-90aaa1b6f1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
