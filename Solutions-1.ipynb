{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913d2a2-f87f-4505-abb4-49e6b2291e52",
   "metadata": {},
   "source": [
    "# Part 1: Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e084dce-bd60-43c1-a13d-cd9636ea2856",
   "metadata": {},
   "source": [
    "If you want to use the OpenAPI API during this tutorial, make a file `secret.py` with `openai_api_key = \"YOURKEYHERE\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb43be-6be1-4cd1-933f-cd3a77052e33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb21f09-827a-412c-8b19-c40fd14825a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd39ab-b6ba-4d69-8e8f-39f382d7712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# logging for lazy people :)\n",
    "from loguru import logger\n",
    "\n",
    "# we're not importing specific methods or classes so it's clear when we actually call llama_index!\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b056809-10bc-4f4d-90bf-a9b8e8acd34e",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44daa1-a3e6-45bf-bc6e-8c84fad646b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"{time} - {level} - {message}\", level=\"DEBUG\")\n",
    "logger.add(\"tutorial_part_1.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee221890-7faa-4b1a-b593-ffe15d5976dc",
   "metadata": {},
   "source": [
    "### Tell the notebook where files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e8b1-5b94-4dcf-b109-9e8351b087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/pydata/schedule.json\")\n",
    "INDEX_PATH = Path(\"indices/pydata_schedule_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f10b1-759c-459b-bece-1495c099cf62",
   "metadata": {},
   "source": [
    "### Tell `llama-index` to use a local embeddings model for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420c723-b6f0-4c1f-8a5f-9da6b875747b",
   "metadata": {},
   "source": [
    "More information about this embeddings model: https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "\n",
    "Note that all-minilm-l6-v2 has a maximum size of 256 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b220f-e86c-48e6-a069-ec4d84a2b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"local:sentence-transformers/all-minilm-l6-v2\"\n",
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531cae-9ec3-47f3-b4c1-e9df4cea39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=embed_model, chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b317df6-0c8c-4ec8-beb9-ce932bc34a50",
   "metadata": {},
   "source": [
    "### Load a vector index with the PyData Amsterdam 2023 schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81545f8f-f540-4b5a-bc39-eac70da0771a",
   "metadata": {},
   "source": [
    "Load the index from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bc3e5-6bc7-4970-9a1f-c722ba931956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04T10:54:01.995195+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede9a40-f6e1-42f3-92bf-4519bfad87ef",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d0cad-f3bb-4467-b3ed-ec87e5fa966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector index `index`\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343d465-8b22-4778-91e7-b33f3c63e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve chunks that mention llama-index\n",
    "results = retriever.retrieve(\"llama_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741e34d-0edb-4aae-97d6-f1760ba8e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node=TextNode(id_='e5163ca0-dbf3-455c-9577-905893dc33a1', embedding=None, metadata={'title': 'Building a personal search engine with llama-index', 'speakers': 'Judith van Stegeren, Yorick van Pelt'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f71f1eae-c12e-5b4f-8902-b8d0b8a38d72', node_type=None, metadata={'title': 'Building a personal search engine with llama-index', 'speakers': 'Judith van Stegeren, Yorick van Pelt'}, hash='ad9a94ef0dbb13afaa9a4a03c12f20c93873fc7fab5cf57bc88176de147749a0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='db5d421e-0e1d-4e6f-98b0-a98866ff9d61', node_type=None, metadata={'title': 'Building a personal search engine with llama-index', 'speakers': 'Judith van Stegeren, Yorick van Pelt'}, hash='2d9407d1e16cbae760a7599b871c7d5e22787d217f522a204e2c310b0d68a0cf')}, hash='e279497d370f982bc12a34c7f0eb19e888975d85b14b5a4c3218d733c2c7d55e', text='For the demo application, we will only use open-source software and models that are light enough to run on an average laptop without a GPU.Using llama-index with OpenAI’s API is optional.If you want to explore postprocessing your results with GPT-3.5, we recommend registering an OpenAI account and making sure you have your OpenAI API key ready.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.5057451278480072\n",
      "\n",
      "node=TextNode(id_='db5d421e-0e1d-4e6f-98b0-a98866ff9d61', embedding=None, metadata={'title': 'Building a personal search engine with llama-index', 'speakers': 'Judith van Stegeren, Yorick van Pelt'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f71f1eae-c12e-5b4f-8902-b8d0b8a38d72', node_type=None, metadata={'title': 'Building a personal search engine with llama-index', 'speakers': 'Judith van Stegeren, Yorick van Pelt'}, hash='ad9a94ef0dbb13afaa9a4a03c12f20c93873fc7fab5cf57bc88176de147749a0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e5163ca0-dbf3-455c-9577-905893dc33a1', node_type=None, metadata={'title': 'Building a personal search engine with llama-index', 'speakers': 'Judith van Stegeren, Yorick van Pelt'}, hash='e279497d370f982bc12a34c7f0eb19e888975d85b14b5a4c3218d733c2c7d55e')}, hash='2d9407d1e16cbae760a7599b871c7d5e22787d217f522a204e2c310b0d68a0cf', text='Building a personal search engine with llama-index\\n\\nWouldn’t it be great to have a Google-like search engine, but then for your own text files and completely private?In this tutorial we’ll build a small personal search engine using open source library llama-index.In this tutorial we will build a small personal search engine using open source library `llama-index`.Llama-index provides utility functions for ingesting various kinds of data, breaking the data up in chunks, building an index of that data using vector embeddings, and retrieving data from the index based on queries.We can even use llama-index to post-process the retrieval results for us using large language models such as GPT.The target audience is people that are already familiar with Python.Participants will experience working with unstructured data, vector embeddings, and explore the possibilities of the recent developments in natural language processing.Workshop materials will be provided via Github before the start of the workshop.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.4092205934719865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01462a67-ebcc-4282-a7c0-5fadb0cc75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node=TextNode(id_='65b097e3-7cab-42d1-85b6-a850b1a4d33c', embedding=None, metadata={'title': 'The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls', 'speakers': 'Jakob Willisch'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8', node_type=None, metadata={'title': 'The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls', 'speakers': 'Jakob Willisch'}, hash='7ea1436f06df89779e327b88d70a9064aeb40f842a522e502f74727cf3287d1a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a3b902b3-1144-44b8-9ae9-0b53e7029138', node_type=None, metadata={'title': 'The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls', 'speakers': 'Jakob Willisch'}, hash='bf37f1115e40ebf303a5d3beb1285a35825f84695ee5832141c289aa6caa52be')}, hash='fd918f91548e8d09c08b03d6358d92706ccd3e44e9f1f3ffd95d5bcc8df43388', text=\"The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls\\n\\nData scientists and analysts are using quasi-experimental methods to make recommendations based on causality instead of randomized control trials.While these methods are easy to use, their assumptions can be complex to explain.This talk will explain these assumptions for data scientists and analysts without in-depth training of causal inference so they can use and explain these methods more confidently to change people's minds using data.Instead of relying solely on randomized control trials (also known as A/B tests), which are considered the gold standard for inferring causality, data scientists and analysts are increasingly turning to quasi-experimental methods to make recommendations based on causality.These methods, including open-source libraries such as CausalImpact (originally an R package but with numerous Python ports), are easy to use, but their assumptions can be complex to explain.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.5365977414214874\n",
      "\n",
      "node=TextNode(id_='d3a09695-68b5-40c8-b3dc-6b6ab31367fb', embedding=None, metadata={'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\", 'speakers': 'Kevin Klein'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0b21795f-fb6f-583d-a77d-bceee45fd768', node_type=None, metadata={'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\", 'speakers': 'Kevin Klein'}, hash='ba37eaf63d2629ea1fbf3345af76e1dc3a797adf175144f8e9fe10a87eecd8f9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f3794d8b-b455-411f-9dc2-745e533e43d9', node_type=None, metadata={'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\", 'speakers': 'Kevin Klein'}, hash='d9021be6a89c321a2507dc5b592fd65d21309941dd5df982d660b2faea511257')}, hash='cadcc8444800ed39cb34f89dafefaa2b06f9a173d45b07d8d05061aadfc240c4', text='They will know which libraries to use for different types of data and which methods are most appropriate for different scenarios.This talk could be particularly relevant for Data Scientists wishing to analyze experiments, such as A/B tests, or trying to derive causal statements from observational, non-experimental data.Participants are not expected to have Causal Inference expertise.Yet, a fundamental understanding of Machine Learning and Probability Theory will be beneficial.0-5’: Why Causal Inference and why CATE estimation?5-10’: What are some conceptual ways of estimating CATEs?10-20’: How can we use EconML and CausalML for CATE estimation on a real dataset?20-30’: What are we missing from EconML and CausalML?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') score=0.47913421453638927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve chunks related to causal machine learning\n",
    "results = retriever.retrieve(\"causal\")\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc675732-2371-4a41-8a11-4c00577bd4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls', 'speakers': 'Jakob Willisch'}\n",
      "\n",
      "{'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\", 'speakers': 'Kevin Klein'}\n",
      "\n",
      "{'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\", 'speakers': 'Kevin Klein'}\n",
      "\n",
      "{'title': 'The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls', 'speakers': 'Jakob Willisch'}\n",
      "\n",
      "{'title': 'Personalization at Uber scale via causal-driven machine learning', 'speakers': 'Okke van der Wal'}\n",
      "\n",
      "{'title': 'Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry', 'speakers': 'Nazli M. Alagoz'}\n",
      "\n",
      "{'title': 'Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry', 'speakers': 'Nazli M. Alagoz'}\n",
      "\n",
      "{'title': 'Personalization at Uber scale via causal-driven machine learning', 'speakers': 'Okke van der Wal'}\n",
      "\n",
      "{'title': \"Causal Inference Libraries: What They Do, What I'd Like Them To Do\", 'speakers': 'Kevin Klein'}\n",
      "\n",
      "{'title': 'Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry', 'speakers': 'Nazli M. Alagoz'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the talk title and speaker for results related to causal machine learning\n",
    "for result in results:\n",
    "    print(result.node.metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b98123-b560-4bc9-b611-3f2db8705262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new retriever that retrieves more than 2 results.\n",
    "retriever = index.as_retriever(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f418a4-19d4-4ec0-b102-714de283c7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(\"Causal Inference Libraries: What They Do, What I'd Like Them To Do\",\n",
      "  'Kevin Klein'),\n",
      " ('Personalization at Uber scale via causal-driven machine learning',\n",
      "  'Okke van der Wal'),\n",
      " ('Staggered Difference-in-Differences in Practice: Causal Insights from the '\n",
      "  'Music Industry',\n",
      "  'Nazli M. Alagoz'),\n",
      " ('The proof of the pudding is in the (way of) eating: quasi-experimental '\n",
      "  'methods of causal inference and their practical pitfalls',\n",
      "  'Jakob Willisch')}\n"
     ]
    }
   ],
   "source": [
    "# Find all talks about causal inference at PyData\n",
    "results = retriever.retrieve('causal')\n",
    "talks = set([(r.node.metadata['title'], r.node.metadata['speakers']) for r in results])\n",
    "pprint.pprint(talks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ee400-d0c5-4ad2-b68e-09c84ce4044c",
   "metadata": {},
   "source": [
    "## Querying the vector index with an external LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ced37-8de0-4d5e-8abb-6fae18bf1803",
   "metadata": {},
   "source": [
    "### Set OpenAI API key (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5b922-be03-435f-b581-053dfb47e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret import openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fc70c-f18f-412c-bae5-3ac06a090a92",
   "metadata": {},
   "source": [
    "### Use OpenAI's gpt-3.5-turbo for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6ce8a-85d1-4d29-a182-a749782d14c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m llama_index\u001b[38;5;241m.\u001b[39mllms\u001b[38;5;241m.\u001b[39mOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39m\u001b[43mopenai_api_key\u001b[49m)\n\u001b[1;32m      2\u001b[0m service_context \u001b[38;5;241m=\u001b[39m llama_index\u001b[38;5;241m.\u001b[39mServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m      3\u001b[0m   embed_model\u001b[38;5;241m=\u001b[39membed_model, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, llm\u001b[38;5;241m=\u001b[39mllm\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'openai_api_key' is not defined"
     ]
    }
   ],
   "source": [
    "llm = llama_index.llms.OpenAI(model=\"gpt-3.5-turbo\", api_key=openai_api_key)\n",
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=embed_model, chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c91e61-7b94-4a1c-a68b-b22553e8b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-30T16:38:04.830107+0200 - INFO - Loaded index from local storage\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d00b1-c133-4af2-b9f7-0e82113797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64136237-b60d-48b5-89d7-e749b71e061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find talks that might be interesting for startup founders\n",
    "response = query_engine.query(\"Which talks are probably interesting for startup founders?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad3806-ad31-4774-a2f4-fbf085d48a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "title: Kickstart AI sponsored drinks [time & location TBD]\n",
      "\n",
      "Kickstart AI sponsored drinks [time & location TBD]\n",
      "\n",
      "Kickstart AI is a foundation powered by a coalition of iconic Dutch brands (Ahold Delhaize, ING, KLM and NS). Their mission is to accelerate AI adoption in the Netherlands, and improve society through the use of AI.\n",
      "\n",
      "Lorem ipsum dolor\n",
      "\n",
      "title: LLM Agents 101: How I Gave ChatGPT Access to My To-Do List\n",
      "\n",
      "We'll showcase these solutions through amusing moments and challenges encountered during the development of my to-do list agent.- A summary of what works well and what still needs improvement\n",
      "\n",
      "This talk is designed as an introduction to LLM agents.Throughout the presentation, I will aim to maintain a high-level perspective to ensure that even less technical audience members can grasp the concepts.To achieve this, I will share entertaining situations where my agent did something unexpected and how I resolved those issues.However, the presentation will still be practical enough for technical people to know all the concepts and techniques to develop their LLM agents.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: Which talks are probably interesting for startup founders?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc130808-7ec1-4ec0-aeaf-92b2a4ec72ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
