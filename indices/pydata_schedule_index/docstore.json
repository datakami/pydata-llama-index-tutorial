{"docstore/metadata": {"0028b7eb-2e98-56ec-9fed-e4cd83f4d118": {"doc_hash": "7799e8a102d19a961063b472dde9ec5962d7d6de948e72338287ad98c89d44ea"}, "0b21795f-fb6f-583d-a77d-bceee45fd768": {"doc_hash": "7a91c5802ae5d6b1f852ab755a02fa2c32823673782773c83ce35e6ded5f3b3f"}, "7b6015d4-f08c-5df1-a904-965815a3180b": {"doc_hash": "65a4633cabe5058f9f3ac884dc91bc7a23436d15d20d5ea1b581da649dc235b9"}, "64283b01-5050-5b2a-bfa7-67044ee6b343": {"doc_hash": "b34578560a71268ca4027b88e934a2ec14fd70477dabb733ce5386d852ebf96a"}, "df2190cb-ce2b-5155-bf89-47ef2a5a8594": {"doc_hash": "2964357a9d27fdaa4d00743275d74b26fc945a01b6193409f623bf7a1d5b5a2f"}, "7097b087-3738-56ca-b572-6026df468b73": {"doc_hash": "04e5b4f5cb2f01a27691083b1656533cf52e9a4a87cd63475a8c29fb6cf69cd2"}, "e5266795-c955-5045-bfbe-9b7206a3da31": {"doc_hash": "98506c65c030d4cf6b22e9270dd3644fc41618e951c780006e106ced8a961f52"}, "32b0fc9d-f825-59d5-87bf-1865861452e6": {"doc_hash": "8a6a59d85917e565c6c5a5d6537a67f8b15e3628a3c0bd9f6fb814f8efd7a518"}, "80309313-5f6a-58dd-b81f-730483b2dd6b": {"doc_hash": "0cc8d7acd7b5266927af3169e6ac867dbb15fb0faf53ed2755080c540f5f0cc4"}, "e5441b76-3ff7-596b-9e16-45abb13d0a77": {"doc_hash": "138a922f6b512934ef0c5c4fa37a332a49b63cdc3362b7cb20c5c3aafc2da47a"}, "c450cf56-9c5f-5fa4-8e9d-d05c0a5af981": {"doc_hash": "7829bd37e635086e646ea955bd07de75e73f9081f13168b087014002bb4c9fa3"}, "e9d34ab2-6fdf-5b41-82c0-2365a11d012e": {"doc_hash": "d23a1ab2d5def466dc7f66243782167d06b1cf79b5113e5d0b8202ffbf9ad6f4"}, "e82f37c8-03f9-5cb5-92f2-8f157924b59d": {"doc_hash": "c5fb067f65ff26094019c1a7a405e64a730756661e970816d8759a2329d6395d"}, "21e0cc6f-bb25-5f30-8afd-896a68790937": {"doc_hash": "1665c4a41efeda648c5fecf5bd471fc62279514f0d39782db82e5cdae3d630dc"}, "82e3db42-5170-5eac-ad21-977d67e729b8": {"doc_hash": "eaef1897efd297f9aca6e50656e906bc21110fade431379aedca284c6b698e9f"}, "9a85e18c-c167-5721-994b-8a15333fe5af": {"doc_hash": "c69c32031df31289fbf0ceeac0cc72f2516849b461ceacf052ae42b971ecf695"}, "f8de93a7-8a01-5b3c-ab6a-bafed8754db1": {"doc_hash": "4f7325cbbd1495c6dd44aca1afc05380a183ac5ec5a322ddda5b3f883176e037"}, "37b038f8-5842-5f59-b441-c26b96cd2690": {"doc_hash": "2d1c8c13f14c0ac42b4829faab3c6159d6c3be89bb57989852e8fd774fdea487"}, "4219e333-9d0f-595f-a7ef-8ef7d4cdea8a": {"doc_hash": "2bb65ad1affda0290c154a630222d7e5a2ecdff3b612cd04cbaa71d8b8b1b987"}, "7896c0f3-e1e9-5103-a9af-2976ed5118d5": {"doc_hash": "f097eb0b470905ea6ddf8baca615bae74529dd40f505e58c60a19a108e4486a6"}, "68f57a2f-125a-57f2-ae71-5ae344efa647": {"doc_hash": "fd06d1481218429f3e8857c2aa29f6fe86ade1deaa3ae33f2968839ec8dd9139"}, "1796a41f-f4b9-5498-b08a-069ce827c06d": {"doc_hash": "bfe5b2b0da09e82b9acd9b64358ab6d3de37bcc11a389b40c7dc86ce54c7e159"}, "73e8f0d5-c586-5bb5-866f-9071f5bc2d40": {"doc_hash": "07e54f339d9cf2a6ce55d0cf8ec0911591571708e3b59b246bec7c77593cf455"}, "e054832c-e8fd-56bb-987b-1ce5eb94d7dd": {"doc_hash": "cf47996cf2785d8be9df184021b732638ed47186836fe9efb739482ddbdf6b33"}, "20780d65-1ec2-51f9-b7e2-ab25c930becc": {"doc_hash": "a060e721265a83a8e7357617f8874aeb1597daa358690b1354361c507dbaac52"}, "38fcc911-f1bf-53f2-a217-fa7979592794": {"doc_hash": "4514070895eeed4c1b0f1da9e40b77ded5d5050a1a7f183e0de5752591585387"}, "b1316bcc-7a02-5d20-83ac-77806aa91efd": {"doc_hash": "aa0088579390d0120c0b947fdc500bc631a236f98b9c24b763b4a575352a90db"}, "8f739f49-0ed3-5db9-99bc-b4d02ae9ad97": {"doc_hash": "28d8023cb02b8d3bc6ec45726a17988739df636b77ab45c9c9e3e896952ed8a0"}, "4edb2500-cc63-510c-9d30-5dd259022b4a": {"doc_hash": "9e11533891134f6a93b664b468f52409ccd3313037db9f78f0f93baff2613213"}, "cd77e690-a63f-564c-8a15-5da291b8040f": {"doc_hash": "de1908fd636e9bb514f54037faa58148e6d362e02badce762a7001a8ef7f1331"}, "f418d8b0-4976-5fb0-ab84-49ddfeff58d2": {"doc_hash": "9d9d378edbc5802b7be138fd9df217c8c9ab928aded91890578984a9d1bc8cfa"}, "da6b1185-bfaf-568b-a2dd-2e5236440ea2": {"doc_hash": "05437e51229f4f047ae675bde2a32785f1d5c4aa9229564b9692875d18affd93"}, "cbb0b87d-a8ba-536e-b126-878f6600ff69": {"doc_hash": "0117c31e2a240a087ab69b45040764b3422c991ce378046cb7a53d3e2e04c8ab"}, "79731e0e-a1d2-597c-88a9-69920acd5d09": {"doc_hash": "65d9ed173b42716a60c0fc3e0f360c27b6a21dd466d516f5714c65cdbf84dddc"}, "c2604d42-c404-581f-a4f5-b1b2cc1ebca5": {"doc_hash": "2c667bc7eab4599b1e58251c89049882e065617226c904f2556213a341a76a28"}, "7c5cd37a-c881-51b5-ad88-bad2af81cbb7": {"doc_hash": "ea14a13c9893a91225f40a8e558d7a6722cfd4256c619fe0862daf522360dce1"}, "7afe76a0-a34b-5e97-95b5-c3192c87e543": {"doc_hash": "5c390712491156260e22672e5af10724f9bd8761444b76e8b5c66f99542cca4f"}, "31d15b15-0304-58da-91d1-947c6fcb9995": {"doc_hash": "966e707ea325d8a441b3c8b32021b6680a12340b49c674beb9026ad979f6ce22"}, "7bf14af7-ada1-5696-86bb-294140a478eb": {"doc_hash": "2b3ae107831c629f4bcfcdaaa9eece1c2f18977b1e867a5f68a6822859657137"}, "e8e06482-a55b-5837-adc3-f7bc91be7eb0": {"doc_hash": "fd2bc27a258976ac222d509d0e6950481e2059a460e78f2a7c8b1efeca80526c"}, "3c7480aa-cb34-5f97-bc1a-af7186c3eadb": {"doc_hash": "bfd591d42ad8b7ca4acd4e740db20085c117a8538d111df1c26d9ed957770249"}, "4a325da5-4ccc-5d5d-ab16-858cd62b8c87": {"doc_hash": "1882776d2b3139510aa96ee84dcd2fb180c02c4c7458d3901b3378ccef3b0a01"}, "e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8": {"doc_hash": "09d3ccd7fff13f4a78184786a5d02a94ebb301f0e20b6ad873d1d070d4066989"}, "ac0cd2f6-d8ea-5e5f-8cec-57631b7fc9c8": {"doc_hash": "dbbf51179a8ba90dcf625fee9932c88d8bad61bf2f510e49c36465cc2828f6f5"}, "9d38c367-284b-546b-ab29-f5fb5089392d": {"doc_hash": "f9bc479c8f36b2483cbc1dc2006ce06b2b1d73b7679e7e2ca1b33c96bdf30642"}, "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea": {"doc_hash": "851e5c4780ccfa93c1772e2b2b7737ce5f0c3c3bf7adee866d9d6174018a4ec9"}, "63aa42ea-2fc6-5bfb-92ac-11e83a7f45f8": {"doc_hash": "1506fb1d0688060897b51bc9c2ec92df5f3a16be837cab27919d1b97feb6e003"}, "d9ad798a-cb3b-5209-9d45-7cef2617937f": {"doc_hash": "4feaed964b69829a3dd81d699f608a2a783d492e7a34fd1966150eacfb1b1322"}, "f71f1eae-c12e-5b4f-8902-b8d0b8a38d72": {"doc_hash": "3048db44e02fa581744056cb7a31a225dfdd5eb0ef6d99ecc4c82dc85e0e2872"}, "66510cf5-c4a9-5cf2-94c1-863495919590": {"doc_hash": "ec05d58f529fb77bb8afd6850243f28b08273477109c49defebfe58d9640619f"}, "3b32bca8-108d-5c64-9dd4-d577b0d284f6": {"doc_hash": "e439270c239d60a2e04fe706d08a5fd5758a10f6ed982ffef726d16b243d0064"}, "89243294-cdfa-57b6-a06f-a150fc1b56b7": {"doc_hash": "e9a99ca852ed4fb373eec195989ce75498d899d38221523e10eb1a7a4ba3a0aa"}, "791c9c2c-69d2-5f10-bf2f-707ed3992b42": {"doc_hash": "f4ccd5adbf55da98b6cac86faa000fd414a322048f1f5fc2226fd8db4c67c1c2"}, "d6fbbf46-8263-5421-80ae-34972cf15788": {"doc_hash": "72c4e3bb0d31780ae4f782e51606c8e65ea87974038cd091e312d58e9875d3e5"}, "0b7e549a-0c06-502d-ad0f-ec3ed74ca80f": {"doc_hash": "e97a4bf15e314855e9a212a05921b3efcdf5cdfadff15131a08295cd85f4cf8c"}, "56b64bc2-19f4-55f7-bed5-fbc4a5c50425": {"doc_hash": "751d8f430469b8122aa659594f018a80a32e9aad2727eae7527c45b9ba24337c"}, "86f6e0f4-9664-5e9c-af9b-addbb9375abf": {"doc_hash": "9488c6f69ad407604dffe93ffea1cd092bb566da6598fd7280c1d2d9b0638950"}, "0c8b6f48-94a7-579a-9920-00350de3f2e0": {"doc_hash": "4133a2d40bcde4f04a14f4d4ffa9e05948f13beb572842caca05165de796f21a"}, "569b4008-5ff9-5ec2-8c88-6dfca3e583cd": {"doc_hash": "7b06c59603a33f33918b9a1f3e1c6351b48feb3c2bf845ef8bde01f284ad4d8b"}, "ffdd7835-3743-5f4b-99ca-ebe6cfbb21ee": {"doc_hash": "d67d8c18c2a0772208be9893f38e451d51522aeaca85f3098d7c133ed3265449"}, "e0485fc1-fcb3-535a-98fd-85efd758a50e": {"doc_hash": "1584232fcfdb1a3a6aaa233d93a8a8dc01b181c4b85fdb4569ad58ecb213f12a"}, "ce9ef2b8-e1bc-59de-8de7-8d5809253967": {"doc_hash": "21f139bcea12a14ae8c1c65839519ba551bd02131c59fe421d156c0d8f8dba00"}, "3f11b211-2573-5bfb-bf12-33dfea2ab2a7": {"doc_hash": "2d9117c247274db8b4b5f1046f64323227d4b35b5da67131fb7bccb1d9de6250"}, "89e8ef97-184b-5aad-9054-447f5410a089": {"doc_hash": "db6e17fca12bf054d9837c98ea22f9b8b8bb642a43d231448d832b946d087599"}, "73f28a4c-f282-5739-9cc5-d517ef4870d8": {"doc_hash": "615cdab0bced221235adda41f63bfbf07f1b05ef0658f6af78925bfa1164daea"}, "a9cfdb94-6920-5b26-b93c-f9511d7aec1f": {"doc_hash": "c22b92a1f1adb4798289c619a54fcf1fbd52dc282781771c822d2cbde68798ba"}, "28ea4e75-e11b-56a7-b889-08724577cd95": {"doc_hash": "13cb367eb18224c50642e6d148d679fa7f92888488c2b65bd049a5ad64943730"}, "605a748e-e813-4a4f-993b-daa7fe1fd9f6": {"doc_hash": "4bf1bf8a45af648c0243e5b76117939a3018f11be19e078c8264c9ba8d60b8f5", "ref_doc_id": "0028b7eb-2e98-56ec-9fed-e4cd83f4d118"}, "d9457ff1-153d-41cd-8c3a-1a8ec98e0e0c": {"doc_hash": "98649b2baf529073ee4848835862da24925deaafec77b939c6c1361ca344036f", "ref_doc_id": "0028b7eb-2e98-56ec-9fed-e4cd83f4d118"}, "07818f93-ad74-4701-8db9-28af07b448a0": {"doc_hash": "9cc3a30f2bb53733afffc38cd2d8c4e3f4daf7951b6a4587f2504028563bbf0e", "ref_doc_id": "0028b7eb-2e98-56ec-9fed-e4cd83f4d118"}, "4def9880-4c0c-45df-8c34-c03845d9231c": {"doc_hash": "a907c0086a178fdb8a1383d17ff7f78604989be126c846b01b423a592964855d", "ref_doc_id": "0028b7eb-2e98-56ec-9fed-e4cd83f4d118"}, "8e793421-25f9-40f0-9919-8d54dbc4a67e": {"doc_hash": "611e25d24370cd9191928add78420996f118aab222ea2a410adc275eb6f75cbf", "ref_doc_id": "0b21795f-fb6f-583d-a77d-bceee45fd768"}, "12285416-baab-407e-b1e8-45626a98d769": {"doc_hash": "0e3fb8eb6fb247e646bb2a884ccd05f76645a5b7a97ddf1d77a9c3f2c246288e", "ref_doc_id": "0b21795f-fb6f-583d-a77d-bceee45fd768"}, "dea587a7-93d4-4061-afa7-12856d76e08b": {"doc_hash": "06bf2a5debb317c148d053852050648893e2e15db9c9a96567ee99e01981ec9a", "ref_doc_id": "0b21795f-fb6f-583d-a77d-bceee45fd768"}, "b8f4b6b8-aa9b-422d-8606-1c0cfc1dd375": {"doc_hash": "d33e4fc8d594a86f8d2aa530855efe49a7d0a977f464e62359df726cbdb80a9a", "ref_doc_id": "7b6015d4-f08c-5df1-a904-965815a3180b"}, "9928fea1-99d4-4b5c-a4f8-dcdde8a96878": {"doc_hash": "d6c7235fd47e34690ae93c64ed44a6b48ecdf722c69e1c1169618c0e7eb407ee", "ref_doc_id": "7b6015d4-f08c-5df1-a904-965815a3180b"}, "4666beb1-d82c-4952-a8c1-94c19c4a8716": {"doc_hash": "6187cd7404d200be1b43f4595e5f920014473fe8026f14a5ed5504d5f4b94acb", "ref_doc_id": "7b6015d4-f08c-5df1-a904-965815a3180b"}, "6d5d25f3-351a-4e7f-aeb5-a5edb2d07f33": {"doc_hash": "2fb5a5a1ec70e5247fd2a9c5e5d54502228187ab0acfdd87ae2759be585cfcec", "ref_doc_id": "64283b01-5050-5b2a-bfa7-67044ee6b343"}, "681576f0-41c0-44c1-b318-d79dfc64e7a5": {"doc_hash": "db0cca2f7629c01d381d5a2b967a788463fcee8d6618880106cda10a44b89ce6", "ref_doc_id": "64283b01-5050-5b2a-bfa7-67044ee6b343"}, "35344444-233c-4e22-a2fc-b121373345be": {"doc_hash": "7d8e7343ae7f35925b8ced09626a6678947f4289d6c683c0c95a36580195f0db", "ref_doc_id": "df2190cb-ce2b-5155-bf89-47ef2a5a8594"}, "4f467eb7-fbb8-4779-9ada-2f64b8703207": {"doc_hash": "21f09477acc193e83342ac399aaf174770d70c353af2179c526d8268a8b5126f", "ref_doc_id": "df2190cb-ce2b-5155-bf89-47ef2a5a8594"}, "bb24db79-e3c5-4770-ba8b-703596e6aea8": {"doc_hash": "4a665a20b6750c0cb2557d584384dab84c6b3ca743d40ca02fc80fb6034e71db", "ref_doc_id": "7097b087-3738-56ca-b572-6026df468b73"}, "8e156afc-147e-4d3e-bb68-d6741225a052": {"doc_hash": "9002dda0ae3d3177426b357030f30e50e46e19de16e9e626a0c970a938e6e110", "ref_doc_id": "7097b087-3738-56ca-b572-6026df468b73"}, "a53add3d-456a-4158-bb5d-59e83f82015e": {"doc_hash": "2f0c47db1601794add456906ee0618bd3e638c2fdf7ce1062304e71664ad713c", "ref_doc_id": "e5266795-c955-5045-bfbe-9b7206a3da31"}, "716a3d0c-dbba-47f7-b9a3-9df4c1bcd9cb": {"doc_hash": "1f8596300ae6a01d82a2404657560606dba99d1bf5e482c23059fbbf4ebdc884", "ref_doc_id": "e5266795-c955-5045-bfbe-9b7206a3da31"}, "f1af43b7-3872-4265-98ef-4c114ebbcde4": {"doc_hash": "748b5e9db8bd1be9dabb5bc3649a0397541a498329eeed9e4305d10d6278f842", "ref_doc_id": "32b0fc9d-f825-59d5-87bf-1865861452e6"}, "997ebb8a-e46f-424d-8a94-e8894ca44dd0": {"doc_hash": "40cacdfe0a1e00293fbcb66509d6e7fcbd0b2eb3b4abf87d9179e31e16ab21cb", "ref_doc_id": "32b0fc9d-f825-59d5-87bf-1865861452e6"}, "ec0155d6-a3fd-4772-852e-501fdabec2e1": {"doc_hash": "0cc8d7acd7b5266927af3169e6ac867dbb15fb0faf53ed2755080c540f5f0cc4", "ref_doc_id": "80309313-5f6a-58dd-b81f-730483b2dd6b"}, "b596bea5-ef57-4ac1-aca6-93cd0d9b837b": {"doc_hash": "938829799f4e8ca55de5fb011ef52921728c5d9a274ab9995c07218ca6038893", "ref_doc_id": "e5441b76-3ff7-596b-9e16-45abb13d0a77"}, "68ad0027-e543-400d-b8a9-c1821ba70eb7": {"doc_hash": "60d53f10a67a2db0e2007098c7e18d3a4fa11f558eb2c7438df5da489bbfae8a", "ref_doc_id": "e5441b76-3ff7-596b-9e16-45abb13d0a77"}, "c64baa1b-48fc-4917-bf79-0529a6354f0f": {"doc_hash": "494d20a041432d61a43357ce28544903cf81893651b0a516f987ab6f366efdbf", "ref_doc_id": "e5441b76-3ff7-596b-9e16-45abb13d0a77"}, "233508d3-608f-426d-9fe4-213b3fbd2256": {"doc_hash": "684e9f7004a85be8f1418a4e1f734a08d2c2727abfa777eb7790abf8e0aebcb0", "ref_doc_id": "e5441b76-3ff7-596b-9e16-45abb13d0a77"}, "14acdc8e-4682-4f6f-9515-14f59d314acd": {"doc_hash": "e171a8ddce343590dc78517f9eef847fcb149097e12686562aaffa65fe33b2f4", "ref_doc_id": "c450cf56-9c5f-5fa4-8e9d-d05c0a5af981"}, "d2124416-9247-428b-b1a6-0878552f97e3": {"doc_hash": "9d3c4bde5b965cc4977c0a65bce99aa842b46e8268a18ba273a12d31ff537c2c", "ref_doc_id": "c450cf56-9c5f-5fa4-8e9d-d05c0a5af981"}, "1ed03643-25db-445b-b684-e1efefe280bf": {"doc_hash": "6ef74f06f2ffd086fd80be0c4fd69950e40dc74ddf4f58ec40da0c61a291a405", "ref_doc_id": "c450cf56-9c5f-5fa4-8e9d-d05c0a5af981"}, "c349aed2-5b64-4672-a89c-1f9a4e2bbbf2": {"doc_hash": "38d537b6d931d8ebb46e0e6840112c2d55248056c6344fa03a1626ef476b75f5", "ref_doc_id": "e9d34ab2-6fdf-5b41-82c0-2365a11d012e"}, "6f8eaa7a-34dc-4855-b8a5-4bcf54471140": {"doc_hash": "1332d331f23b8175d917f1bc58c842ec2a9ebe62fd58c633eada5651f28196b5", "ref_doc_id": "e9d34ab2-6fdf-5b41-82c0-2365a11d012e"}, "0a06b37c-6100-42ad-8866-768322a148c5": {"doc_hash": "c5fb067f65ff26094019c1a7a405e64a730756661e970816d8759a2329d6395d", "ref_doc_id": "e82f37c8-03f9-5cb5-92f2-8f157924b59d"}, "64132b3c-fa33-4ae2-82df-aaf1f029018f": {"doc_hash": "f8279a76c56caafd4ded1fff73e7f0248658b8d08523c8bc53218d9edd2778f6", "ref_doc_id": "21e0cc6f-bb25-5f30-8afd-896a68790937"}, "c6d66437-122d-47d3-8a8e-64ed191b5c11": {"doc_hash": "3469e890920bdb6e0e969f8e286b027793c1de2f5e141afe77fbd08ff2542d91", "ref_doc_id": "21e0cc6f-bb25-5f30-8afd-896a68790937"}, "a287e9e2-37a0-47ad-a4ad-a74ff94ba760": {"doc_hash": "3c8f436e03bd53720963c0ff836ec89153916d211212734fd5e5e638910f4964", "ref_doc_id": "82e3db42-5170-5eac-ad21-977d67e729b8"}, "7a1b2479-89c5-4a64-a63c-8574a1241093": {"doc_hash": "c5f25f42c2f9ffba5beafcb57a9a595b09a53b3c69bc541c179d1386efa0354a", "ref_doc_id": "82e3db42-5170-5eac-ad21-977d67e729b8"}, "6ba97669-ec4a-44ac-a3b3-e87483d02ef2": {"doc_hash": "0f0fbd7437448776b75201939d98a3be93c47419a65d38c17e081ee5b5fe1bd1", "ref_doc_id": "9a85e18c-c167-5721-994b-8a15333fe5af"}, "29840ed9-38e3-4d5c-af5e-494a344b3690": {"doc_hash": "a7e3aafed9bde827ca338aa1d54632853d10c2233d8eb7f6d111e647b08d3fed", "ref_doc_id": "9a85e18c-c167-5721-994b-8a15333fe5af"}, "4ee063a6-5de0-4993-9f65-771d9ad95928": {"doc_hash": "362e13aa726a946734ea67355ff04b06fafd74cbc4bbcf09f6155c367334656a", "ref_doc_id": "9a85e18c-c167-5721-994b-8a15333fe5af"}, "ffd95951-7d70-4bdd-9bd6-4e562e7b6e9f": {"doc_hash": "327ad60e49d7bf0dde87a87e1b5f6b4a31b7e2a9385709a1414d6146383dd283", "ref_doc_id": "9a85e18c-c167-5721-994b-8a15333fe5af"}, "0c68890f-2a56-43d8-8b8f-2cca9543bc77": {"doc_hash": "459930ad110031bd289659f821f35190e913ec40ccc2d0d551ab1e5ab0087299", "ref_doc_id": "f8de93a7-8a01-5b3c-ab6a-bafed8754db1"}, "6bd2fe90-317c-4662-8c2f-1c3f46926175": {"doc_hash": "7a73481a7ae4393cfb5fba72a248cdb6bcef9cb9786e06645c63326dacf240fd", "ref_doc_id": "f8de93a7-8a01-5b3c-ab6a-bafed8754db1"}, "fbea8871-0243-4c4a-923f-4b750eafcb29": {"doc_hash": "2d1c8c13f14c0ac42b4829faab3c6159d6c3be89bb57989852e8fd774fdea487", "ref_doc_id": "37b038f8-5842-5f59-b441-c26b96cd2690"}, "0f94b288-176a-41eb-a2e3-36c3b1e50f0c": {"doc_hash": "f0ec40b1f920b35dc88cd91c53d3634692e0c951ac32d58827fef85b47271a75", "ref_doc_id": "4219e333-9d0f-595f-a7ef-8ef7d4cdea8a"}, "01042a01-bc0b-437e-92e1-7261b83c7ba9": {"doc_hash": "b786ecfcecf912bc267578bfb12df4595a03c51c6e85bb62dcdd7545ae04d435", "ref_doc_id": "4219e333-9d0f-595f-a7ef-8ef7d4cdea8a"}, "edc76a48-3fb9-49c8-955a-c3b407523cb1": {"doc_hash": "04badb6cf7dbfe35a75cd9f1a46ff8da61cc75b8016bfa355ca9afbe3c5f9b39", "ref_doc_id": "4219e333-9d0f-595f-a7ef-8ef7d4cdea8a"}, "fdcc5a66-1c5a-4f53-b785-51910876a809": {"doc_hash": "f097eb0b470905ea6ddf8baca615bae74529dd40f505e58c60a19a108e4486a6", "ref_doc_id": "7896c0f3-e1e9-5103-a9af-2976ed5118d5"}, "0c8f746e-7b94-46bd-93cf-a2fcd56b200b": {"doc_hash": "fd06d1481218429f3e8857c2aa29f6fe86ade1deaa3ae33f2968839ec8dd9139", "ref_doc_id": "68f57a2f-125a-57f2-ae71-5ae344efa647"}, "5095e989-9f90-46c6-be52-3708616d7e91": {"doc_hash": "5992cf7589da94357e621aac29ed18d55b6fb1f18db0a05fa06ec7313dbddf29", "ref_doc_id": "1796a41f-f4b9-5498-b08a-069ce827c06d"}, "c075a1e6-91e5-4d09-8800-dfcf062314e8": {"doc_hash": "feebda29247aa9583589f907dd6a0de2ff002fda3367d1aa1137510b2fe7ff2c", "ref_doc_id": "1796a41f-f4b9-5498-b08a-069ce827c06d"}, "f3a42fba-5390-4b51-860e-569b73ff61c8": {"doc_hash": "07e54f339d9cf2a6ce55d0cf8ec0911591571708e3b59b246bec7c77593cf455", "ref_doc_id": "73e8f0d5-c586-5bb5-866f-9071f5bc2d40"}, "e85aa15b-8cff-4976-8fa3-7c34d3a4ab20": {"doc_hash": "876f969a6fc24ea7a22f4a1768b6c952cccdc25f71b0e7cbe0c90f75f1de8627", "ref_doc_id": "e054832c-e8fd-56bb-987b-1ce5eb94d7dd"}, "80819620-31b8-43a0-829b-89e7e35cbad8": {"doc_hash": "70fbe604b3c9dd1ebdb65c037b3db1e28abd52d25cc6d2a9498b15263495782f", "ref_doc_id": "e054832c-e8fd-56bb-987b-1ce5eb94d7dd"}, "c0c578e3-4bcc-4253-8b63-335f2f6fa4f3": {"doc_hash": "555ebebeda367ab948b0a4015322c810d94ed4ffeafa0cd0408b68f2191d7e9d", "ref_doc_id": "e054832c-e8fd-56bb-987b-1ce5eb94d7dd"}, "872e3b26-8321-4c01-a794-8da24fec212e": {"doc_hash": "844bd8098f8bdf9771338b7dbb3640a2d9d2ff70d2a39f94eae75af618e25045", "ref_doc_id": "e054832c-e8fd-56bb-987b-1ce5eb94d7dd"}, "c89e546e-c997-47aa-8130-30c8c6f4d7a4": {"doc_hash": "229cb5e436f6b0b2071cf30eca4d0ead48e510193ae31757206309930b7f52da", "ref_doc_id": "20780d65-1ec2-51f9-b7e2-ab25c930becc"}, "6867abd6-99eb-480b-be4c-feea03f9a951": {"doc_hash": "0dd6545ea2376e8b94c14b5da27c6f2009f1fe3d380f3d71edee1f03de63b394", "ref_doc_id": "20780d65-1ec2-51f9-b7e2-ab25c930becc"}, "26b6170b-5340-4e52-8e2c-8971d0ff2c33": {"doc_hash": "4514070895eeed4c1b0f1da9e40b77ded5d5050a1a7f183e0de5752591585387", "ref_doc_id": "38fcc911-f1bf-53f2-a217-fa7979592794"}, "a53a0321-3d18-470e-8e31-004ed623593f": {"doc_hash": "d3b2a186f61839d02ac17203803e0108a9ebcca7532cd47a06188084eb94b2e0", "ref_doc_id": "b1316bcc-7a02-5d20-83ac-77806aa91efd"}, "4cd10bd5-f299-42bf-846e-383912adc23e": {"doc_hash": "a8e7d37438a79dad86fe43d5bf16f6edd082e783c7eddc4cfe0e75fc245d5c60", "ref_doc_id": "b1316bcc-7a02-5d20-83ac-77806aa91efd"}, "9ed7246f-859a-4e3c-b853-ffbc7e254760": {"doc_hash": "28d8023cb02b8d3bc6ec45726a17988739df636b77ab45c9c9e3e896952ed8a0", "ref_doc_id": "8f739f49-0ed3-5db9-99bc-b4d02ae9ad97"}, "cccf5f12-66b2-46a0-8e7a-bc5b23a0db36": {"doc_hash": "c1ebea166697af25b168c8c9e71f2fc9a7385c883ec74171f55cc6dd0b216553", "ref_doc_id": "4edb2500-cc63-510c-9d30-5dd259022b4a"}, "29b4ac38-bbf7-4b0b-9baa-9098daf45d9d": {"doc_hash": "dc43b766a3f1568fbf8ef23a652696193d991334e2d1e722eaab17d12da42318", "ref_doc_id": "4edb2500-cc63-510c-9d30-5dd259022b4a"}, "ee7a64f4-fc28-400e-87a9-47b14dc64c81": {"doc_hash": "417e2b30a50f6b478e2cfe6944f47cbfb77924f8c34bda443c429ca13a890b98", "ref_doc_id": "cd77e690-a63f-564c-8a15-5da291b8040f"}, "775d5324-9cac-49f3-b393-d2b5a73bc687": {"doc_hash": "cf35aa3f4a75841d9858b0ad13b198cb825aedf948043646c831ad3e93f4c755", "ref_doc_id": "cd77e690-a63f-564c-8a15-5da291b8040f"}, "5db5678f-2e9a-4956-a4e8-ad0ec4e92d96": {"doc_hash": "b9b1c5789f816b5e4e9a1a4f332c3f2ded07cad6aa3082f132fab3ae982ff89b", "ref_doc_id": "f418d8b0-4976-5fb0-ab84-49ddfeff58d2"}, "2263dc47-4263-42df-8e6d-63aa2810c9df": {"doc_hash": "b7c52b89b5bbb11de1871cd16814e4a43f93bce1cbbf663d1de7a441068c2661", "ref_doc_id": "f418d8b0-4976-5fb0-ab84-49ddfeff58d2"}, "0a815d19-1f1f-404a-a926-81fd70e92597": {"doc_hash": "64ae1f4ea489ded7997427c394e8ee559699499884bf37d27b851bb631ba7da7", "ref_doc_id": "da6b1185-bfaf-568b-a2dd-2e5236440ea2"}, "b2426976-f96a-4fae-bdc5-44e2d52fa83d": {"doc_hash": "8d4b8c4e5b9e9aaf1f86a1a16c7ea871e26309ad015ccf1ca825b3397cb3390e", "ref_doc_id": "da6b1185-bfaf-568b-a2dd-2e5236440ea2"}, "804ef235-d6d9-46b3-9111-9b39f5f468ec": {"doc_hash": "f443517ab2b760090d1f4e427b3e93290d076a181d44c36a3c1906732c93b59e", "ref_doc_id": "da6b1185-bfaf-568b-a2dd-2e5236440ea2"}, "84e65ea3-0818-4784-b4e2-22a08e59ac17": {"doc_hash": "a97487be4d4476f7d8f265ea45ba6fb659766a118fb0525f363638a83be86fa2", "ref_doc_id": "cbb0b87d-a8ba-536e-b126-878f6600ff69"}, "634fd595-6c9a-4f98-b5ba-a6ea0b2a0fad": {"doc_hash": "2d3478e854c7a5c8f3eee33053e360b0f212ab1be86897d31618ac0f57cdeb56", "ref_doc_id": "cbb0b87d-a8ba-536e-b126-878f6600ff69"}, "c6431e49-1b5d-48e9-84d5-1be2ee34500b": {"doc_hash": "04724e06872264d84a8a3b1f551b876d13c545b3ae3ae60f3363c31e8f2815cd", "ref_doc_id": "79731e0e-a1d2-597c-88a9-69920acd5d09"}, "ec6b2bc7-f9e7-4ff2-9be3-5d1bd4e8e84c": {"doc_hash": "34f85f5c534644e799ec738b2af505b54f122cd7375e7d0e9ad5b965a415d8a4", "ref_doc_id": "79731e0e-a1d2-597c-88a9-69920acd5d09"}, "3f29108f-7f5d-47b1-b5a2-6f864b66b4b2": {"doc_hash": "5c49ff6f71caae0e68ad8420291ea2cf80c6b97d1ed39e21c6d87b14717636fb", "ref_doc_id": "c2604d42-c404-581f-a4f5-b1b2cc1ebca5"}, "63d9510b-9880-4415-85fe-56ef411c8c26": {"doc_hash": "207dc945ec1c80010e3f50a9a1abd1e10109364c91c88890b326a7d92d21b8ce", "ref_doc_id": "c2604d42-c404-581f-a4f5-b1b2cc1ebca5"}, "80204bd9-b8c7-4947-8896-96e1b26606df": {"doc_hash": "a595a0c1f3cee7c8b27d5956e986444cdd554169712b327c7d628715bb635d4e", "ref_doc_id": "7c5cd37a-c881-51b5-ad88-bad2af81cbb7"}, "d91faaa8-b1dc-40e5-b632-f329e6644003": {"doc_hash": "00c25d964167ad72dd60da2132cb49283088a5b8bd9f2c4d6ad56e4fe3dc71e0", "ref_doc_id": "7c5cd37a-c881-51b5-ad88-bad2af81cbb7"}, "18a0c84c-a5ea-4045-aab7-e46cbc156777": {"doc_hash": "b081cde7af77a80d58b0fc495c344e958837d1174318c17757c31aafeb6ecf0f", "ref_doc_id": "7c5cd37a-c881-51b5-ad88-bad2af81cbb7"}, "42b9a7a5-57d2-4c75-a708-4970e5a3cdc4": {"doc_hash": "4499a8fb9348b4344fe5256a60cd2d43355236e53e9d41c7645cbfbb9ba2ecda", "ref_doc_id": "7c5cd37a-c881-51b5-ad88-bad2af81cbb7"}, "4b310b5f-430d-4b3a-b664-1dd04b2e798d": {"doc_hash": "e05fd62714765a240edebe0bb4e1d31596fa469e4b8cf375e76d187405b3f0e5", "ref_doc_id": "7afe76a0-a34b-5e97-95b5-c3192c87e543"}, "182123cd-68a9-41f3-9655-fb8d24bbb0e1": {"doc_hash": "cbc12527ed187edaad4764b7a34eb09382b1b106e3ad7b69ae60eaac4ccf314e", "ref_doc_id": "7afe76a0-a34b-5e97-95b5-c3192c87e543"}, "c4b93c82-f872-4e29-a54f-e7e50b98601f": {"doc_hash": "3865be6f606d936e4a5dff44194a254f1d836584f62ceb17384fda88db57dad3", "ref_doc_id": "31d15b15-0304-58da-91d1-947c6fcb9995"}, "5d5ade97-5f47-4b27-8f75-35490c5c2fbe": {"doc_hash": "31edd295eb4831a8455ffc73b4ababafb7cf62179daddae8a73f60566ef53e16", "ref_doc_id": "31d15b15-0304-58da-91d1-947c6fcb9995"}, "6b9886e0-6c60-421d-a4ca-e636648bd176": {"doc_hash": "dbe07efd4a052e01b8faf3f60a3204a2017af2bbca311e057e40c115c76cc4d5", "ref_doc_id": "7bf14af7-ada1-5696-86bb-294140a478eb"}, "0e540f0f-63f1-47ac-ba90-e6cff70d04be": {"doc_hash": "65ce9e12040e63036f3bae120d728a30a7bcbdaeb2b6e1e96cc07ae4dae8b1f6", "ref_doc_id": "7bf14af7-ada1-5696-86bb-294140a478eb"}, "ae913d8e-bd36-4b2f-baa1-57a2d8ff97a1": {"doc_hash": "fd2bc27a258976ac222d509d0e6950481e2059a460e78f2a7c8b1efeca80526c", "ref_doc_id": "e8e06482-a55b-5837-adc3-f7bc91be7eb0"}, "cb845b37-2146-4f82-894b-670f16762cfc": {"doc_hash": "ce330e0d8de395515241cf109595c94f6706ff21da4a28f5d2f74bb437ea2d10", "ref_doc_id": "3c7480aa-cb34-5f97-bc1a-af7186c3eadb"}, "8da78579-8972-49a3-809f-369e4c72e54a": {"doc_hash": "f1b428ede45c1e00d93bcdead819e727d263d8b6984f2469a3d6c26d756e6155", "ref_doc_id": "3c7480aa-cb34-5f97-bc1a-af7186c3eadb"}, "058883b1-2f80-4a63-80f4-e9bc83a98dd1": {"doc_hash": "846629df9680afd610845f2597846d05fadbf3e081a521f174d6273c2e8e0a69", "ref_doc_id": "4a325da5-4ccc-5d5d-ab16-858cd62b8c87"}, "15085444-2c42-4e6f-bdea-7ae31cb08b08": {"doc_hash": "c6344890bf813fef5bdcaad6372cdb6629e1440d4e6568f557b0bcba836e7c43", "ref_doc_id": "4a325da5-4ccc-5d5d-ab16-858cd62b8c87"}, "3aa39d86-22c4-4215-8263-d4fee2f3a041": {"doc_hash": "c742c9696f5116d28be0742413be8c5802df9b578d6152546874cd13b0dd1b80", "ref_doc_id": "e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8"}, "d68f7a36-fc89-44f1-bb4e-41095158b490": {"doc_hash": "c15d2084f0b21e8e4a21b5ded4791bc95fcdc1c399e31eb370cf170423d6d13b", "ref_doc_id": "e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8"}, "91be920b-9b95-4166-8430-01297783f4a2": {"doc_hash": "73f0a70835386487b7aa95742387dfa7ac5e0c72fdf68f58d0e4ecd1d89c005c", "ref_doc_id": "ac0cd2f6-d8ea-5e5f-8cec-57631b7fc9c8"}, "0682a5ba-45e7-4d57-9ad4-615a741dd16e": {"doc_hash": "3c12fd753b918831ca269d989ba8fade19e33768afacaa2325c31630b83647fc", "ref_doc_id": "ac0cd2f6-d8ea-5e5f-8cec-57631b7fc9c8"}, "7af708cb-4979-4dd4-8df9-698f2c47d474": {"doc_hash": "383ce495df1c57b975c2aa9763e17d61252309228058f21d279f480c2049e773", "ref_doc_id": "9d38c367-284b-546b-ab29-f5fb5089392d"}, "3e5eecc9-639a-4822-9f76-199f31e9eeeb": {"doc_hash": "b6fc04801d82f94af7cebf9a2ccc8aaec256a270ffd857e86ff811d43171d1a1", "ref_doc_id": "9d38c367-284b-546b-ab29-f5fb5089392d"}, "258cdc1e-10b0-4ee8-9f64-396740ea3025": {"doc_hash": "d9fb12eaf11842af9574e8808a9c5e42f75547b36a939b2d25b32a69b0388417", "ref_doc_id": "9d38c367-284b-546b-ab29-f5fb5089392d"}, "9a27b557-69ae-4008-9ceb-0c3f9367f62a": {"doc_hash": "78d9617dfbf0e0906cffad16944a3617d41b497806386c6ca4812a2cdd295fcb", "ref_doc_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea"}, "6576c8f8-422f-4a55-a149-b4e1d7b6f9d1": {"doc_hash": "ab548d772e086766fd1cbdc6f216417c648a8eb926f39f50c1d96bf16d9a6024", "ref_doc_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea"}, "9113c3dd-6cb1-4bb9-8068-7ad0bfe785bb": {"doc_hash": "0cae1bb46173748440b918999243cfb09bd59df8eb7cb4c967b8c059feed7b03", "ref_doc_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea"}, "150168c1-0721-4cd9-b7a8-69f188bf9391": {"doc_hash": "fdf7b9e4dfdf7d887b2236145622decfa14bd4debb9a56570a2582e91806b8fa", "ref_doc_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea"}, "083b9719-ba34-408d-af6f-7a9af11d0a87": {"doc_hash": "e6a7719bfd7f1edc262f15043dc5717f945bd876e82cbfd82c8be1abf7c7fb4d", "ref_doc_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea"}, "76f33570-5547-499c-be8c-59038873bb3e": {"doc_hash": "1506fb1d0688060897b51bc9c2ec92df5f3a16be837cab27919d1b97feb6e003", "ref_doc_id": "63aa42ea-2fc6-5bfb-92ac-11e83a7f45f8"}, "26c19831-c98f-4435-b165-c82427c12766": {"doc_hash": "e7b6ea5d96348335bcbaaaed598edcc603afa0ca0f54db729afbb358d8e209c0", "ref_doc_id": "d9ad798a-cb3b-5209-9d45-7cef2617937f"}, "b9440585-20c6-4ebf-a222-c95195f805bd": {"doc_hash": "b23961b4f0d2893635c392f00ab75f95d429cd44eff3cbe6733d953f05b77e65", "ref_doc_id": "d9ad798a-cb3b-5209-9d45-7cef2617937f"}, "daecb07c-9247-4bca-9dc8-89cfc0250d93": {"doc_hash": "18dddb7f30eb0d9e79afef5ea3a3fd74a04a8353cbc332d0f5009af31813fdab", "ref_doc_id": "f71f1eae-c12e-5b4f-8902-b8d0b8a38d72"}, "e6dd1229-39dd-40db-b8cf-56d1c0bd49b1": {"doc_hash": "e6b09c77ade2a5bb7b6c5afc3c9851bc0689fa2625cf85be8831fcc85a0e85ec", "ref_doc_id": "f71f1eae-c12e-5b4f-8902-b8d0b8a38d72"}, "f1ed581d-81a0-4248-86c1-ce82e7d01cd7": {"doc_hash": "37e3937f38f3fcb44d5ab8cbdefdcc6d2784a9b1f96d8e4c4e31094c7cd17705", "ref_doc_id": "66510cf5-c4a9-5cf2-94c1-863495919590"}, "9b3c6264-77b5-41d4-838b-c1d110270fd2": {"doc_hash": "e25254af0bca1f4b89cd5c0f72542ffe1b4a117c11cc9821a572cff41834b7d0", "ref_doc_id": "66510cf5-c4a9-5cf2-94c1-863495919590"}, "9c6d6801-bd7c-4447-9877-493db9fe92da": {"doc_hash": "6a53f1987313a750b4f7abb7c4c92f41d3d284f6fe9e40da520fde08cf6b00a7", "ref_doc_id": "66510cf5-c4a9-5cf2-94c1-863495919590"}, "f7736f86-c42f-42d4-89b1-22e85500bac8": {"doc_hash": "e439270c239d60a2e04fe706d08a5fd5758a10f6ed982ffef726d16b243d0064", "ref_doc_id": "3b32bca8-108d-5c64-9dd4-d577b0d284f6"}, "ffec7b14-342c-4e0e-9fa4-ec7a01a69c1b": {"doc_hash": "e9a99ca852ed4fb373eec195989ce75498d899d38221523e10eb1a7a4ba3a0aa", "ref_doc_id": "89243294-cdfa-57b6-a06f-a150fc1b56b7"}, "c7d37103-da06-446a-b7f8-1edf72487406": {"doc_hash": "286e965dfbed0635f1b3ecb21bd79d098c1b5cebcb392ad9f70f9033bc75f8c0", "ref_doc_id": "791c9c2c-69d2-5f10-bf2f-707ed3992b42"}, "91b88d61-146b-414e-9d3d-b6fffac3a756": {"doc_hash": "acc9894eb3764fe107fd3db0c91147d7848da88dc51f962cdb833a36d3fefba5", "ref_doc_id": "791c9c2c-69d2-5f10-bf2f-707ed3992b42"}, "1f14fe1f-3196-4465-92ce-ba6252be10a4": {"doc_hash": "549c4f2d7dee353a3cd983fb576729e6297a465630fcaddb5b29a141f8236948", "ref_doc_id": "791c9c2c-69d2-5f10-bf2f-707ed3992b42"}, "3562a775-839c-40c1-b4e4-44a0b945b921": {"doc_hash": "72c4e3bb0d31780ae4f782e51606c8e65ea87974038cd091e312d58e9875d3e5", "ref_doc_id": "d6fbbf46-8263-5421-80ae-34972cf15788"}, "c0e2ce9f-fb13-49b4-a0cf-3cc7f0f90d71": {"doc_hash": "28812916347a8b72c1d045d82b92c055180ebe7fee6dafe67e066db4e050d102", "ref_doc_id": "0b7e549a-0c06-502d-ad0f-ec3ed74ca80f"}, "e4a70cd9-08aa-4028-8809-d84bef981628": {"doc_hash": "f423b4a7024b8f284a5a508f4cd24838af1746a14f7b863eff1daec36370a146", "ref_doc_id": "0b7e549a-0c06-502d-ad0f-ec3ed74ca80f"}, "c73a06fc-6db4-483d-b491-826f7b88bd91": {"doc_hash": "39d905b3341d95224b186448b2d73982df7c4e1ca43c84330ae1bb51eeddf8d0", "ref_doc_id": "56b64bc2-19f4-55f7-bed5-fbc4a5c50425"}, "41910b9a-478c-44eb-879c-b12a75e9c0ae": {"doc_hash": "1eec878a3af793985aab255ab5ed42b404ff50545bcd9cc2f032838a8d703708", "ref_doc_id": "56b64bc2-19f4-55f7-bed5-fbc4a5c50425"}, "cc33d7ce-ed84-4b88-b3ca-fe565545de34": {"doc_hash": "b4e8a641314a55e6de901225deda91c6387a1abaf0edda12ffc3491496058419", "ref_doc_id": "56b64bc2-19f4-55f7-bed5-fbc4a5c50425"}, "dd4673c0-a2dc-4e1e-9cf8-01ada0da8b8a": {"doc_hash": "9488c6f69ad407604dffe93ffea1cd092bb566da6598fd7280c1d2d9b0638950", "ref_doc_id": "86f6e0f4-9664-5e9c-af9b-addbb9375abf"}, "88100b2e-1b46-45e4-958e-a01eda029839": {"doc_hash": "75ccead6d4fad93739fa72a67eeebc2e21ab074100dcb236cebdc883cc207b5e", "ref_doc_id": "0c8b6f48-94a7-579a-9920-00350de3f2e0"}, "9a35f25a-4ff1-46e9-94b8-c33568e70981": {"doc_hash": "dc64223ef5d38a80bbaeb09f7c77cc65a1e1caa154563efb109ed179daed3946", "ref_doc_id": "0c8b6f48-94a7-579a-9920-00350de3f2e0"}, "ae01cd15-adc8-456d-99a9-3d68cf7566ef": {"doc_hash": "d53134e246352b8d865626ff462065c472efcfef46dd8b351572676183a71a6d", "ref_doc_id": "0c8b6f48-94a7-579a-9920-00350de3f2e0"}, "e4c9d236-16eb-4fab-9abb-fe422d1cd8be": {"doc_hash": "a835b7584569c4d48f9dc61e16ba95595f8efd2a0c9e1dedbc23e8f3b770bceb", "ref_doc_id": "569b4008-5ff9-5ec2-8c88-6dfca3e583cd"}, "b760ef53-aab3-4d87-98d0-c76ba277e96f": {"doc_hash": "154f42af2dcd0b850891340e7257de2692cd0ab4e9bcb134ead2398c0a1dca83", "ref_doc_id": "569b4008-5ff9-5ec2-8c88-6dfca3e583cd"}, "2d099a71-5487-444c-83d8-7d8dc6569ad4": {"doc_hash": "6236c38f4426d9072e6b75046d51645341c1cb97df6612ee6a54f1b774a85923", "ref_doc_id": "ffdd7835-3743-5f4b-99ca-ebe6cfbb21ee"}, "604919f9-c6c8-4222-bc6f-3dfcd5299be6": {"doc_hash": "cac2335232851d7dc766e69ad0a53fc64f67c636518d80376d06fac3c608ad94", "ref_doc_id": "ffdd7835-3743-5f4b-99ca-ebe6cfbb21ee"}, "7c68894e-4f93-44a9-b983-6a36984222a0": {"doc_hash": "ad2d6ba727797e7562d745d2473169174d9927efb57fd406596c6c52783d24cd", "ref_doc_id": "ffdd7835-3743-5f4b-99ca-ebe6cfbb21ee"}, "197cca09-5692-4951-80c3-e5ee9742ac36": {"doc_hash": "45d80980d544e707006c6906aee81ef9a00c439f9a2d67f3ec9559921a7ce05c", "ref_doc_id": "e0485fc1-fcb3-535a-98fd-85efd758a50e"}, "b59e8c16-5a9a-449c-bbb6-ec3ab66b6bc9": {"doc_hash": "07abf7e9fe9c55f80440b49ab9da9548aee1d3459970f6067618ecb5cf8c9b52", "ref_doc_id": "e0485fc1-fcb3-535a-98fd-85efd758a50e"}, "6d590827-52d5-45df-9791-451711fc524e": {"doc_hash": "a08d789a33018ff7b914fb141f681714e93d931e9e30412962a8d7b03fa90e3d", "ref_doc_id": "ce9ef2b8-e1bc-59de-8de7-8d5809253967"}, "209ef90a-650a-4767-bed4-3ce70b9a86d4": {"doc_hash": "8cb3e1353ed68248a1930eb111a8c80b738af50e902509fd3186af78cced3e6b", "ref_doc_id": "ce9ef2b8-e1bc-59de-8de7-8d5809253967"}, "b3f98ffe-a897-4d5a-8fca-6721fd7035f6": {"doc_hash": "bfab3eb6db4efe2ccc7adddf7c69294cce8112cafb234290c603247f1401119f", "ref_doc_id": "3f11b211-2573-5bfb-bf12-33dfea2ab2a7"}, "73bb4c05-757c-4eb4-919e-bd7147de6206": {"doc_hash": "a8318e59cf451657a0ecd44d0ceac062b6a98483e2018dd5f7596a900117e201", "ref_doc_id": "3f11b211-2573-5bfb-bf12-33dfea2ab2a7"}, "7657eefb-fbf1-4fdd-9518-2392754c936e": {"doc_hash": "ef50e90ba2734cdfc931b01620c5821532f063d5d32d3c08dfd6da7007725bbf", "ref_doc_id": "3f11b211-2573-5bfb-bf12-33dfea2ab2a7"}, "f0833169-e7c7-433b-8e28-7da0c3924cc4": {"doc_hash": "db6e17fca12bf054d9837c98ea22f9b8b8bb642a43d231448d832b946d087599", "ref_doc_id": "89e8ef97-184b-5aad-9054-447f5410a089"}, "8aebd03a-c249-4c02-9e06-730da20a5a0e": {"doc_hash": "f888830e4c4bd952916d799fee6794a13570446fcee976e766bee232a737c306", "ref_doc_id": "73f28a4c-f282-5739-9cc5-d517ef4870d8"}, "f14d2c0b-5e9d-48e3-a1bf-e9fb3bfd54a4": {"doc_hash": "1569462aba5ceaf4726a4c7272f740de47e3b5b0d8736f3f293612c6df5b3342", "ref_doc_id": "73f28a4c-f282-5739-9cc5-d517ef4870d8"}, "ab2dade4-0372-4ef3-9d0f-fc3e1f92fba8": {"doc_hash": "d87dbef339dd91a1633a88e571c716b164430978e2d6a822750b346a1e73df09", "ref_doc_id": "a9cfdb94-6920-5b26-b93c-f9511d7aec1f"}, "aa636989-22d8-4fb8-8619-61fe6e8ffa7b": {"doc_hash": "ec0ac3ed3351796e43f346b39b6085559d32798e323e9971c78e2b6f7ae3e599", "ref_doc_id": "a9cfdb94-6920-5b26-b93c-f9511d7aec1f"}, "e63cad79-5195-44db-b730-34c9b94785cf": {"doc_hash": "78266db3a4c108a0cd348d59418d53715e579f7ddb5707209849ca11d9cca863", "ref_doc_id": "28ea4e75-e11b-56a7-b889-08724577cd95"}, "e0cab5ab-e70e-47e2-80aa-e3879037e8d5": {"doc_hash": "aac82dd7d95bd154a0cda33a7d16148ef6885cd9fc5f6c7de50e940ece6743cd", "ref_doc_id": "28ea4e75-e11b-56a7-b889-08724577cd95"}}, "docstore/data": {"605a748e-e813-4a4f-993b-daa7fe1fd9f6": {"__data__": {"id_": "605a748e-e813-4a4f-993b-daa7fe1fd9f6", "embedding": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0028b7eb-2e98-56ec-9fed-e4cd83f4d118", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "7799e8a102d19a961063b472dde9ec5962d7d6de948e72338287ad98c89d44ea"}, "3": {"node_id": "d9457ff1-153d-41cd-8c3a-1a8ec98e0e0c", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "98649b2baf529073ee4848835862da24925deaafec77b939c6c1361ca344036f"}}, "hash": "4bf1bf8a45af648c0243e5b76117939a3018f11be19e078c8264c9ba8d60b8f5", "text": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN\n\nIn this talk, we discuss how we can use the python package PySTAN to estimate the Lifetime Value (LTV) of the users that can be acquired from a marketing campaign, and use this estimate to find the optimal bidding strategy when the LTV estimate itself has uncertainty.Throughout the presentation, we highlight the benefits from using Bayesian modeling to estimate LTV, and the potential pitfalls when forecasting LTV.By the end of the presentation, attendees will have a solid understanding of how to use PySTAN to estimate LTV,  optimize their marketing campaign bidding strategies, and implement the best Bayesian modelling solution.All of the contents and numbers in this presentation can be found in the shared GIT\n\nWe describe how to use the PySTAN to forecast the LTV of the marketing campaigns.PySTAN is a Python interface to STAN, which is a package for Bayesian inference capable of high-performance statistical computation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d9457ff1-153d-41cd-8c3a-1a8ec98e0e0c": {"__data__": {"id_": "d9457ff1-153d-41cd-8c3a-1a8ec98e0e0c", "embedding": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0028b7eb-2e98-56ec-9fed-e4cd83f4d118", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "7799e8a102d19a961063b472dde9ec5962d7d6de948e72338287ad98c89d44ea"}, "2": {"node_id": "605a748e-e813-4a4f-993b-daa7fe1fd9f6", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "4bf1bf8a45af648c0243e5b76117939a3018f11be19e078c8264c9ba8d60b8f5"}, "3": {"node_id": "07818f93-ad74-4701-8db9-28af07b448a0", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "9cc3a30f2bb53733afffc38cd2d8c4e3f4daf7951b6a4587f2504028563bbf0e"}}, "hash": "98649b2baf529073ee4848835862da24925deaafec77b939c6c1361ca344036f", "text": "PySTAN\u2019s computation speed is essential in a marketing context, where we need to predict the LTV of multiple marketing campaigns over a long period, while still estimating the LTV distribution.We demonstrate how to implement a PySTAN model to predict a time-series using the Lifetime Value data from Kaggle [2], which contains approximately 200 days, in less than 2 minutes.While this is long compared to point-estimate algorithms, it is still much faster than a similar code using PyMC, another probabilistic programming library.With the LTV accurately predicted for the Lifetime Value data, we explain the steps to optimize the bid of marketing campaigns under uncertainty about the accuracy of our predictions.We show how different levels of uncertainty of our LTV predictions can change the optimal bidding strategy and answer questions such as \u201cHow much should we underbid when we are unsure of our LTV?\u201d.By the end of the presentation, attendees will be able to implement PySTAN to estimate LTV and apply this knowledge to find the best bidding strategy for their marketing campaigns.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "07818f93-ad74-4701-8db9-28af07b448a0": {"__data__": {"id_": "07818f93-ad74-4701-8db9-28af07b448a0", "embedding": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0028b7eb-2e98-56ec-9fed-e4cd83f4d118", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "7799e8a102d19a961063b472dde9ec5962d7d6de948e72338287ad98c89d44ea"}, "2": {"node_id": "d9457ff1-153d-41cd-8c3a-1a8ec98e0e0c", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "98649b2baf529073ee4848835862da24925deaafec77b939c6c1361ca344036f"}, "3": {"node_id": "4def9880-4c0c-45df-8c34-c03845d9231c", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "a907c0086a178fdb8a1383d17ff7f78604989be126c846b01b423a592964855d"}}, "hash": "9cc3a30f2bb53733afffc38cd2d8c4e3f4daf7951b6a4587f2504028563bbf0e", "text": "In this presentation, we will thus cover the following topics:\r\n- [3 min] What is PyMC\r\n- [5 min] Modeling advertisement for digital products\r\n- [10 min] How to use PySTAN to estimate the LTV of your marketing campaigns\r\n- [10 min] How to achieve the same model through PyMC\r\n- [2 min] How PyMC compares to those alternatives in terms of implementation ease, execution speed, and support for further improvements\r\n[5 min] How to find the optimal bid for your marketing campaign\r\n[5 min] How the optimal bid for a marketing campaign varies under different levels of uncertainty of its LTV and elasticity (i.e.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4def9880-4c0c-45df-8c34-c03845d9231c": {"__data__": {"id_": "4def9880-4c0c-45df-8c34-c03845d9231c", "embedding": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0028b7eb-2e98-56ec-9fed-e4cd83f4d118", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "7799e8a102d19a961063b472dde9ec5962d7d6de948e72338287ad98c89d44ea"}, "2": {"node_id": "07818f93-ad74-4701-8db9-28af07b448a0", "node_type": null, "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}, "hash": "9cc3a30f2bb53733afffc38cd2d8c4e3f4daf7951b6a4587f2504028563bbf0e"}}, "hash": "a907c0086a178fdb8a1383d17ff7f78604989be126c846b01b423a592964855d", "text": "how many more users we get by increasing the spend in the marketing campaign)\r\n\r\n\r\n**References**\r\n- The Duopoly is over because everything is an ad network[ [https://mobiledevmemo.com/the-duopoly-is-over-because-everything-is-an-ad-network/]\r\n- Lifetime Value data from Kaggle: https://www.kaggle.com/datasets/baetulo/lifetime-value?select=train.csv", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e793421-25f9-40f0-9919-8d54dbc4a67e": {"__data__": {"id_": "8e793421-25f9-40f0-9919-8d54dbc4a67e", "embedding": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b21795f-fb6f-583d-a77d-bceee45fd768", "node_type": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "hash": "7a91c5802ae5d6b1f852ab755a02fa2c32823673782773c83ce35e6ded5f3b3f"}, "3": {"node_id": "12285416-baab-407e-b1e8-45626a98d769", "node_type": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "hash": "0e3fb8eb6fb247e646bb2a884ccd05f76645a5b7a97ddf1d77a9c3f2c246288e"}}, "hash": "611e25d24370cd9191928add78420996f118aab222ea2a410adc275eb6f75cbf", "text": "Causal Inference Libraries: What They Do, What I'd Like Them To Do\n\nThis talk will explore the Python tooling and ecosystem for estimating conditional average treatment effects (CATEs) in a Causal Inference setting.Using real world-examples, it will compare and contrast the pros and cons of various existing libraries as well as outline desirable functionalities not currently offered by any public library.Conditional average treatment effects (CATEs) are a fundamental concept in Causal Inference, allowing for the estimation of the effect of a particular treatment or intervention.For CATEs, the effect estimation is not only with respect to an entire population, e.g.all experiment participants, but rather with respect to units, e.g.a single experiment participant, with individual characteristics.This can be very important to meaningfully personalize services and products.In this talk, we will explore the Python tooling and ecosystem for estimating CATEs, including libraries such as EconML and CausalML.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "12285416-baab-407e-b1e8-45626a98d769": {"__data__": {"id_": "12285416-baab-407e-b1e8-45626a98d769", "embedding": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b21795f-fb6f-583d-a77d-bceee45fd768", "node_type": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "hash": "7a91c5802ae5d6b1f852ab755a02fa2c32823673782773c83ce35e6ded5f3b3f"}, "2": {"node_id": "8e793421-25f9-40f0-9919-8d54dbc4a67e", "node_type": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "hash": "611e25d24370cd9191928add78420996f118aab222ea2a410adc275eb6f75cbf"}, "3": {"node_id": "dea587a7-93d4-4061-afa7-12856d76e08b", "node_type": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "hash": "06bf2a5debb317c148d053852050648893e2e15db9c9a96567ee99e01981ec9a"}}, "hash": "0e3fb8eb6fb247e646bb2a884ccd05f76645a5b7a97ddf1d77a9c3f2c246288e", "text": "We will begin by providing an overview of the theory behind CATE estimation, how it fits into the broader field of causal inference and how Machine Learning has recently broken into CATE estimation.We will then dive into the various libraries available for Python, discussing their strengths and weaknesses and providing real-world examples of their usage.Specifically, we will cover:\r\n- EconML: An open-source library for general Causal Inference purposes, by Microsoft Research\r\n- CausalML: An open-source library for uplift modeling in particular, by Uber\r\n\r\nWe will compare and contrast these libraries with respect to CATE estimation, discussing which methods they use, which assumptions they make, and which types of data they are best suited for.We will also provide code examples to illustrate how to use each library in practice.Moreover, we will discuss what we think is missing from both of them.By the end of the talk, attendees will have a solid understanding of the Python tooling and ecosystem for estimating CATEs in a causal inference setting.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dea587a7-93d4-4061-afa7-12856d76e08b": {"__data__": {"id_": "dea587a7-93d4-4061-afa7-12856d76e08b", "embedding": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b21795f-fb6f-583d-a77d-bceee45fd768", "node_type": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "hash": "7a91c5802ae5d6b1f852ab755a02fa2c32823673782773c83ce35e6ded5f3b3f"}, "2": {"node_id": "12285416-baab-407e-b1e8-45626a98d769", "node_type": null, "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}, "hash": "0e3fb8eb6fb247e646bb2a884ccd05f76645a5b7a97ddf1d77a9c3f2c246288e"}}, "hash": "06bf2a5debb317c148d053852050648893e2e15db9c9a96567ee99e01981ec9a", "text": "They will know which libraries to use for different types of data and which methods are most appropriate for different scenarios.This talk could be particularly relevant for Data Scientists wishing to analyze experiments, such as A/B tests, or trying to derive causal statements from observational, non-experimental data.Participants are not expected to have Causal Inference expertise.Yet, a fundamental understanding of Machine Learning and Probability Theory will be beneficial.0-5\u2019: Why Causal Inference and why CATE estimation?5-10\u2019: What are some conceptual ways of estimating CATEs?10-20\u2019: How can we use EconML and CausalML for CATE estimation on a real dataset?20-30\u2019: What are we missing from EconML and CausalML?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8f4b6b8-aa9b-422d-8606-1c0cfc1dd375": {"__data__": {"id_": "b8f4b6b8-aa9b-422d-8606-1c0cfc1dd375", "embedding": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b6015d4-f08c-5df1-a904-965815a3180b", "node_type": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "hash": "65a4633cabe5058f9f3ac884dc91bc7a23436d15d20d5ea1b581da649dc235b9"}, "3": {"node_id": "9928fea1-99d4-4b5c-a4f8-dcdde8a96878", "node_type": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "hash": "d6c7235fd47e34690ae93c64ed44a6b48ecdf722c69e1c1169618c0e7eb407ee"}}, "hash": "d33e4fc8d594a86f8d2aa530855efe49a7d0a977f464e62359df726cbdb80a9a", "text": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry\n\nThe Difference-in-Differences (DiD) methodology is a popular causal inference method utilized by leading tech firms such as Microsoft Research, LinkedIn, Meta, and Uber.Yet recent studies suggest that traditional DiD methods may have significant limitations when treatment timings differ.An effective alternative is the implementation of the staggered DiD design.We exemplify this by investigating an interesting question in the music industry: Does featuring a song in TV shows influence its popularity, and are there specific factors that could moderate this impact?Difference-in-differences (DiD) is a causal inference method frequently used in empirical research in industry and academia.However, standard DiD has limitations when interventions occur at different times or affect varying groups.This talk will highlight the application of the Staggered DiD method, a more nuanced approach that addresses these limitations, in the context of the music industry.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9928fea1-99d4-4b5c-a4f8-dcdde8a96878": {"__data__": {"id_": "9928fea1-99d4-4b5c-a4f8-dcdde8a96878", "embedding": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b6015d4-f08c-5df1-a904-965815a3180b", "node_type": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "hash": "65a4633cabe5058f9f3ac884dc91bc7a23436d15d20d5ea1b581da649dc235b9"}, "2": {"node_id": "b8f4b6b8-aa9b-422d-8606-1c0cfc1dd375", "node_type": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "hash": "d33e4fc8d594a86f8d2aa530855efe49a7d0a977f464e62359df726cbdb80a9a"}, "3": {"node_id": "4666beb1-d82c-4952-a8c1-94c19c4a8716", "node_type": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "hash": "6187cd7404d200be1b43f4595e5f920014473fe8026f14a5ed5504d5f4b94acb"}}, "hash": "d6c7235fd47e34690ae93c64ed44a6b48ecdf722c69e1c1169618c0e7eb407ee", "text": "We will try to answer the question of how music features in TV shows affect music popularity and how this effect might change for different types of music using the staggered DiD method.Attendees will gain an understanding of causal inference through observational studies and specifically how the new DiD methods are used through an interesting and original case study.The talk will be structured as follows:\r\n\r\n1.Intro to the case (e.g., background on music features on TV, dataset) (5 mins)\r\n2.Explanation of the DiD approach and its limitations.(5 mins)\r\n3.Introduction to the Staggered DiD method.(5 mins)\r\n4.Application of staggered DiD for the case study from the music industry (10 mins)\r\n5.Conclusions (5 mins)\r\n\r\nTarget Audience: The talk would be beneficial for data scientists, researchers, and practitioners interested in causal inference, marketing analytics, and quasi-experimental design.Attendees should have a basic understanding of statistical methods used in data science.Key Takeaways:\r\n\r\n1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4666beb1-d82c-4952-a8c1-94c19c4a8716": {"__data__": {"id_": "4666beb1-d82c-4952-a8c1-94c19c4a8716", "embedding": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b6015d4-f08c-5df1-a904-965815a3180b", "node_type": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "hash": "65a4633cabe5058f9f3ac884dc91bc7a23436d15d20d5ea1b581da649dc235b9"}, "2": {"node_id": "9928fea1-99d4-4b5c-a4f8-dcdde8a96878", "node_type": null, "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}, "hash": "d6c7235fd47e34690ae93c64ed44a6b48ecdf722c69e1c1169618c0e7eb407ee"}}, "hash": "6187cd7404d200be1b43f4595e5f920014473fe8026f14a5ed5504d5f4b94acb", "text": "Understanding of the DiD approach and its limitations in the context of analyses with observational data.2.Insights into the Staggered DiD method and its application.3.Practical knowledge about executing and evaluating DiD studies effectively.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6d5d25f3-351a-4e7f-aeb5-a5edb2d07f33": {"__data__": {"id_": "6d5d25f3-351a-4e7f-aeb5-a5edb2d07f33", "embedding": null, "metadata": {"title": "Don\u2019t judge a book by its cover: Using LLM created datasets to train models that detect literary features"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64283b01-5050-5b2a-bfa7-67044ee6b343", "node_type": null, "metadata": {"title": "Don\u2019t judge a book by its cover: Using LLM created datasets to train models that detect literary features"}, "hash": "b34578560a71268ca4027b88e934a2ec14fd70477dabb733ce5386d852ebf96a"}, "3": {"node_id": "681576f0-41c0-44c1-b318-d79dfc64e7a5", "node_type": null, "metadata": {"title": "Don\u2019t judge a book by its cover: Using LLM created datasets to train models that detect literary features"}, "hash": "db0cca2f7629c01d381d5a2b967a788463fcee8d6618880106cda10a44b89ce6"}}, "hash": "2fb5a5a1ec70e5247fd2a9c5e5d54502228187ab0acfdd87ae2759be585cfcec", "text": "Don\u2019t judge a book by its cover: Using LLM created datasets to train models that detect literary features\n\nExisting book recommendation systems like Goodreads are based on correlating the reading habits of people.But what if you want a humorous book?Or a book that is set in 19th century Paris?Or a thriller, but without violence?We build book recommendation systems for Dutch libraries based on more than a dozen features from historical setting, to writing style, to main character characteristics.This allows us to tailor each recommendation to individual readers.The recent developments in LLMs are an interesting area for us to explore to improve our recommendations.However, running LLMs in production is unfortunately not always feasible.The associated costs may be too high, and running code from third parties in your daily pipeline may be undesirable.And then there\u2019s data privacy -  or, in our case, intellectual copyright - to be considered as well.So how can you reap the benefits of an LLM, without exposing yourself or your company to some of these major downsides?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "681576f0-41c0-44c1-b318-d79dfc64e7a5": {"__data__": {"id_": "681576f0-41c0-44c1-b318-d79dfc64e7a5", "embedding": null, "metadata": {"title": "Don\u2019t judge a book by its cover: Using LLM created datasets to train models that detect literary features"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64283b01-5050-5b2a-bfa7-67044ee6b343", "node_type": null, "metadata": {"title": "Don\u2019t judge a book by its cover: Using LLM created datasets to train models that detect literary features"}, "hash": "b34578560a71268ca4027b88e934a2ec14fd70477dabb733ce5386d852ebf96a"}, "2": {"node_id": "6d5d25f3-351a-4e7f-aeb5-a5edb2d07f33", "node_type": null, "metadata": {"title": "Don\u2019t judge a book by its cover: Using LLM created datasets to train models that detect literary features"}, "hash": "2fb5a5a1ec70e5247fd2a9c5e5d54502228187ab0acfdd87ae2759be585cfcec"}}, "hash": "db0cca2f7629c01d381d5a2b967a788463fcee8d6618880106cda10a44b89ce6", "text": "We utilized LLMs to generate custom, tailor-made datasets for our literary feature detection models to train on.This allowed us to benefit from the high performance of large language models, without continued reliance on external parties such as OpenAI or Google.While you may think LLMs are not as effective for languages other than English, we\u2019ve seen major improvements in several of our models.In this talk, we\u2019ll highlight:\r\n- A note on recommenders: Why does Goodreads recommender not work for me, while Spotify\u2019s Discover Weekly is so good?- Different methods of getting data from books\r\n- Iterative process of creating a dataset using an LLM and retraining our models\r\n- Some notes on intellectual property and evaluation of models.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35344444-233c-4e22-a2fc-b121373345be": {"__data__": {"id_": "35344444-233c-4e22-a2fc-b121373345be", "embedding": null, "metadata": {"title": "Mind the language: how to monitor NLP and LLM in production"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df2190cb-ce2b-5155-bf89-47ef2a5a8594", "node_type": null, "metadata": {"title": "Mind the language: how to monitor NLP and LLM in production"}, "hash": "2964357a9d27fdaa4d00743275d74b26fc945a01b6193409f623bf7a1d5b5a2f"}, "3": {"node_id": "4f467eb7-fbb8-4779-9ada-2f64b8703207", "node_type": null, "metadata": {"title": "Mind the language: how to monitor NLP and LLM in production"}, "hash": "21f09477acc193e83342ac399aaf174770d70c353af2179c526d8268a8b5126f"}}, "hash": "7d8e7343ae7f35925b8ced09626a6678947f4289d6c683c0c95a36580195f0db", "text": "Mind the language: how to monitor NLP and LLM in production\n\nHow can you evaluate your production models when the data is not structured and you have no labels?To start, by tracking patterns and changes in the input data and model outputs.In this talk, I will give an overview of the possible approaches to monitor NLP and LLM models: from embedding drift detection to using regular expressions.Once LLMs or NLP models are in production, you want to ensure they work as intended.But how can you observe their behavior in the wild and detect when something goes wrong?First, you often lack true labels.To add to this, the data is unstructured - how exactly can you track a pile of texts?Monitoring the patterns in the input data and model outputs is often the first line of defense.In the talk, I will review possible approaches to monitoring drift and data quality issues in text data and explain their pros and cons.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4f467eb7-fbb8-4779-9ada-2f64b8703207": {"__data__": {"id_": "4f467eb7-fbb8-4779-9ada-2f64b8703207", "embedding": null, "metadata": {"title": "Mind the language: how to monitor NLP and LLM in production"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df2190cb-ce2b-5155-bf89-47ef2a5a8594", "node_type": null, "metadata": {"title": "Mind the language: how to monitor NLP and LLM in production"}, "hash": "2964357a9d27fdaa4d00743275d74b26fc945a01b6193409f623bf7a1d5b5a2f"}, "2": {"node_id": "35344444-233c-4e22-a2fc-b121373345be", "node_type": null, "metadata": {"title": "Mind the language: how to monitor NLP and LLM in production"}, "hash": "7d8e7343ae7f35925b8ced09626a6678947f4289d6c683c0c95a36580195f0db"}}, "hash": "21f09477acc193e83342ac399aaf174770d70c353af2179c526d8268a8b5126f", "text": "I will cover:\r\n- Statistical embedding drift detection  \r\n- Tracking interpretable text descriptors like text length and sentiment\r\n- Using regular expressions to validate outputs\r\n- Explaining drift through model-based drift detection\r\n- Detecting changes in multi-modal data\r\n\r\nI will also introduce open-source tools, models, and visualization techniques one can use to monitor LLM and NLP models.This talk will benefit data scientists and machine learning engineers who work with NLP and LLM in production.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bb24db79-e3c5-4770-ba8b-703596e6aea8": {"__data__": {"id_": "bb24db79-e3c5-4770-ba8b-703596e6aea8", "embedding": null, "metadata": {"title": "Revealing the True Motives of News Readers"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7097b087-3738-56ca-b572-6026df468b73", "node_type": null, "metadata": {"title": "Revealing the True Motives of News Readers"}, "hash": "04e5b4f5cb2f01a27691083b1656533cf52e9a4a87cd63475a8c29fb6cf69cd2"}, "3": {"node_id": "8e156afc-147e-4d3e-bb68-d6741225a052", "node_type": null, "metadata": {"title": "Revealing the True Motives of News Readers"}, "hash": "9002dda0ae3d3177426b357030f30e50e46e19de16e9e626a0c970a938e6e110"}}, "hash": "4a665a20b6750c0cb2557d584384dab84c6b3ca743d40ca02fc80fb6034e71db", "text": "Revealing the True Motives of News Readers\n\nEvery news consumer has needs and in order to build a true bond with your customer it is vital to meet these, sometimes, diverse needs.To achieve this, first of all, it is important to identify the overarching needs of users; the reason why they read news.The BBC conducted research to determine these needs and identified six distinct categories: Update me, Keep me on trend, Give me perspective, Educate me, Divert me, and Inspire me.Their research showed that an equal distribution of content across these user needs will lead to higher customer engagement and loyalty.To apply this concept within DPG Media, we started building our own user needs model.Through various iterations of text labelling, text preparation, model building, fine-tuning and evaluation, we have arrived at a BERT model that is capable of determining the associated user needs based solely on the article text.We would like to take the audience through all the steps that we have taken to get to the point where we are right now.During this process we had to find solutions to many obstacles and we are happy to share these lessons with the audience.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e156afc-147e-4d3e-bb68-d6741225a052": {"__data__": {"id_": "8e156afc-147e-4d3e-bb68-d6741225a052", "embedding": null, "metadata": {"title": "Revealing the True Motives of News Readers"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7097b087-3738-56ca-b572-6026df468b73", "node_type": null, "metadata": {"title": "Revealing the True Motives of News Readers"}, "hash": "04e5b4f5cb2f01a27691083b1656533cf52e9a4a87cd63475a8c29fb6cf69cd2"}, "2": {"node_id": "bb24db79-e3c5-4770-ba8b-703596e6aea8", "node_type": null, "metadata": {"title": "Revealing the True Motives of News Readers"}, "hash": "4a665a20b6750c0cb2557d584384dab84c6b3ca743d40ca02fc80fb6034e71db"}}, "hash": "9002dda0ae3d3177426b357030f30e50e46e19de16e9e626a0c970a938e6e110", "text": "Furthermore, we want to discuss all the tools and techniques that we used in order to arrive at the current phase.The focus of the talk is on preparing the datasets and building the models, so a background in data science, engineering and/or machine learning is usefull.The time breakdown will be the following:\r\nMinutes 0-5: introducing the topic and explaining why it is important\r\nMinutes 5-10: discussing the tools that we used and prior decisions we made\r\nMinutes 10-20: going through the labelling process and different models we build\r\nMinutes 20-25: sharing results and lessons learnt\r\nMinutes 25-30: giving insights into next steps and future applications", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a53add3d-456a-4158-bb5d-59e83f82015e": {"__data__": {"id_": "a53add3d-456a-4158-bb5d-59e83f82015e", "embedding": null, "metadata": {"title": "Extend your scikit-learn workflow with Hugging Face and skorch"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5266795-c955-5045-bfbe-9b7206a3da31", "node_type": null, "metadata": {"title": "Extend your scikit-learn workflow with Hugging Face and skorch"}, "hash": "98506c65c030d4cf6b22e9270dd3644fc41618e951c780006e106ced8a961f52"}, "3": {"node_id": "716a3d0c-dbba-47f7-b9a3-9df4c1bcd9cb", "node_type": null, "metadata": {"title": "Extend your scikit-learn workflow with Hugging Face and skorch"}, "hash": "1f8596300ae6a01d82a2404657560606dba99d1bf5e482c23059fbbf4ebdc884"}}, "hash": "2f0c47db1601794add456906ee0618bd3e638c2fdf7ce1062304e71664ad713c", "text": "Extend your scikit-learn workflow with Hugging Face and skorch\n\nDiscover how to bridge the gap between traditional machine learning and the rapidly evolving world of AI with skorch.This package integrates the Hugging Face ecosystem while adhering to the familiar scikit-learn API.We will explore fine-turing of pre-trained models, creating our own tokenizers, accelerating model training, and leveraging Large Language Models.The machine learning world is evolving quickly, AI is talked about everywhere, with the Hugging Face ecosystem being in the midst of it.For traditional machine learning users, especially coming from scikit-learn, keeping up can be quite overwhelming.With the help of the skorch package, it is possible to marry the best of both worlds.It allows you to integrate with many of the Hugging Face features while conforming to the sklearn API.In this talk, I'll give a brief introduction to skorch.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "716a3d0c-dbba-47f7-b9a3-9df4c1bcd9cb": {"__data__": {"id_": "716a3d0c-dbba-47f7-b9a3-9df4c1bcd9cb", "embedding": null, "metadata": {"title": "Extend your scikit-learn workflow with Hugging Face and skorch"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5266795-c955-5045-bfbe-9b7206a3da31", "node_type": null, "metadata": {"title": "Extend your scikit-learn workflow with Hugging Face and skorch"}, "hash": "98506c65c030d4cf6b22e9270dd3644fc41618e951c780006e106ced8a961f52"}, "2": {"node_id": "a53add3d-456a-4158-bb5d-59e83f82015e", "node_type": null, "metadata": {"title": "Extend your scikit-learn workflow with Hugging Face and skorch"}, "hash": "2f0c47db1601794add456906ee0618bd3e638c2fdf7ce1062304e71664ad713c"}}, "hash": "1f8596300ae6a01d82a2404657560606dba99d1bf5e482c23059fbbf4ebdc884", "text": "Then we will learn how to use it to tap into the Hugging Face ecosystem, benefiting from: using pre-trained models and fine-tuning them, working with tokenizers as if they were sklearn transformers, accelerating model training, and even using Large Language Models as zero-shot classifiers.I'll discuss some benefits and drawbacks of this approach.This talk should be of interest to you if you're coming from the scikit-learn world and are interested in the latest deep learning developments.Familiarity with scikit-learn and a little bit of PyTorch knowledge is recommended.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f1af43b7-3872-4265-98ef-4c114ebbcde4": {"__data__": {"id_": "f1af43b7-3872-4265-98ef-4c114ebbcde4", "embedding": null, "metadata": {"title": "From Vision to Action: Designing and Deploying Effective Computer Vision Pipelines"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32b0fc9d-f825-59d5-87bf-1865861452e6", "node_type": null, "metadata": {"title": "From Vision to Action: Designing and Deploying Effective Computer Vision Pipelines"}, "hash": "8a6a59d85917e565c6c5a5d6537a67f8b15e3628a3c0bd9f6fb814f8efd7a518"}, "3": {"node_id": "997ebb8a-e46f-424d-8a94-e8894ca44dd0", "node_type": null, "metadata": {"title": "From Vision to Action: Designing and Deploying Effective Computer Vision Pipelines"}, "hash": "40cacdfe0a1e00293fbcb66509d6e7fcbd0b2eb3b4abf87d9179e31e16ab21cb"}}, "hash": "748b5e9db8bd1be9dabb5bc3649a0397541a498329eeed9e4305d10d6278f842", "text": "From Vision to Action: Designing and Deploying Effective Computer Vision Pipelines\n\nIn the world of computer vision, the focus is often on cutting-edge neural network architectures.However, the true impact usually lies in designing a robust system around the model to solve real-world business challenges.In this talk, we guide you through the process of building practical computer vision pipelines that leverage techniques such as segmentation, classification, and object tracking, demonstrated by our predictive maintenance application at Port of Rotterdam.Whether you're an experienced expert seeking production-worthy pipelines or a novice with a background in data science or engineering eager to dive into image and video processing, we will explore the use of open-source tools to develop and deploy computer vision applications.This talk provides a comprehensive demonstration of creating a powerful computer vision pipeline using widely-used libraries such as PyTorch, torchvision, and OpenCV.We break the pipeline down into manageable components, discussing the importance of proper separation of concerns.Onboarding new use cases becomes a breeze when following best practices in the project structure, combined with user-friendly command-line interfaces.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "997ebb8a-e46f-424d-8a94-e8894ca44dd0": {"__data__": {"id_": "997ebb8a-e46f-424d-8a94-e8894ca44dd0", "embedding": null, "metadata": {"title": "From Vision to Action: Designing and Deploying Effective Computer Vision Pipelines"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32b0fc9d-f825-59d5-87bf-1865861452e6", "node_type": null, "metadata": {"title": "From Vision to Action: Designing and Deploying Effective Computer Vision Pipelines"}, "hash": "8a6a59d85917e565c6c5a5d6537a67f8b15e3628a3c0bd9f6fb814f8efd7a518"}, "2": {"node_id": "f1af43b7-3872-4265-98ef-4c114ebbcde4", "node_type": null, "metadata": {"title": "From Vision to Action: Designing and Deploying Effective Computer Vision Pipelines"}, "hash": "748b5e9db8bd1be9dabb5bc3649a0397541a498329eeed9e4305d10d6278f842"}}, "hash": "40cacdfe0a1e00293fbcb66509d6e7fcbd0b2eb3b4abf87d9179e31e16ab21cb", "text": "Efficient development and validation processes are ensured by designing a sane data model and writing useful tests.Additionally, we explore the critical topic of maintainability, applying MLOps principles for long-term success.To bring these concepts to life, we present a real-world application: the Machine Learning Inspector.This predictive maintenance tool, deployed at the Port of Rotterdam, automatically detects and inspects objects in video streams from trucks and ships, delivering actionable insights.We discuss how we work together with asset inspectors to capture their knowledge of the real world in our artificially intelligent computer vision tool.Join us in this talk to gain practical knowledge and valuable insights for designing, deploying, and maintaining computer vision pipelines that drive tangible impact.We aim to empower the audience to build their own computer vision pipelines; with the right design philosophy, every data professional should be able to build computer vision pipelines that might be complex, but not complicated.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec0155d6-a3fd-4772-852e-501fdabec2e1": {"__data__": {"id_": "ec0155d6-a3fd-4772-852e-501fdabec2e1", "embedding": null, "metadata": {"title": "Online ML Serving best practices"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80309313-5f6a-58dd-b81f-730483b2dd6b", "node_type": null, "metadata": {"title": "Online ML Serving best practices"}, "hash": "0cc8d7acd7b5266927af3169e6ac867dbb15fb0faf53ed2755080c540f5f0cc4"}}, "hash": "0cc8d7acd7b5266927af3169e6ac867dbb15fb0faf53ed2755080c540f5f0cc4", "text": "Online ML Serving best practices\n\nWorking on ML serving for couple of years we learned a lot. I would like to share a set of best practices / learnings with the community\n\nAt Adyen we deploy a lot of models for online inference in the payment flow. Working in the MLOps team to streamline this process, I learned a lot about best practices / things to consider before (after) putting a model online. These are small things but they do contribute to a production and reliable setup for online inference. Some examples:\r\n\r\n- Adding meta data & creating a self contained archive\r\n- Separating serving sources from training sources\r\n- Choosing the requirements of model\r\n- Adding an example input & output request\r\n- Adding schemas for input and output\r\n- Common issues when putting models online: memory leaks, concurrency\r\n- Which server is best? Process based or thread based\r\n- How different python versions affect inference (execution) time", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b596bea5-ef57-4ac1-aca6-93cd0d9b837b": {"__data__": {"id_": "b596bea5-ef57-4ac1-aca6-93cd0d9b837b", "embedding": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5441b76-3ff7-596b-9e16-45abb13d0a77", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "138a922f6b512934ef0c5c4fa37a332a49b63cdc3362b7cb20c5c3aafc2da47a"}, "3": {"node_id": "68ad0027-e543-400d-b8a9-c1821ba70eb7", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "60d53f10a67a2db0e2007098c7e18d3a4fa11f558eb2c7438df5da489bbfae8a"}}, "hash": "938829799f4e8ca55de5fb011ef52921728c5d9a274ab9995c07218ca6038893", "text": "In-Process Analytical Data Management with DuckDB\n\nDuckDB is a novel analytical data management system.DuckDB supports complex queries, has no external dependencies, and is deeply integrated into the Python ecosystem.Because DuckDB runs in the same process, no serialization or socket communication has to occur, making data transfer virtually instantaneous.For example, DuckDB can directly query Pandas data frames faster than Pandas itself.In our talk, we will describe the user values of DuckDB, and how it can be used to improve their day-to-day lives through automatic parallelization, efficient operators and out-of-core operations.Data management systems and data analysts have a troubled relationship: Common systems such as Postgres or Spark are unwieldy, hard to set up and maintain, hard to transfer data in and out, and hard to integrate into complex end-to-end workflows.As a response, analysts have developed their own ecosystem of data wrangling tools such as Pandas or Polars.These tools are much more natural for analysts to use, but are limited in the amount of data they can process or the amount of automatic optimization that is supported.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "68ad0027-e543-400d-b8a9-c1821ba70eb7": {"__data__": {"id_": "68ad0027-e543-400d-b8a9-c1821ba70eb7", "embedding": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5441b76-3ff7-596b-9e16-45abb13d0a77", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "138a922f6b512934ef0c5c4fa37a332a49b63cdc3362b7cb20c5c3aafc2da47a"}, "2": {"node_id": "b596bea5-ef57-4ac1-aca6-93cd0d9b837b", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "938829799f4e8ca55de5fb011ef52921728c5d9a274ab9995c07218ca6038893"}, "3": {"node_id": "c64baa1b-48fc-4917-bf79-0529a6354f0f", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "494d20a041432d61a43357ce28544903cf81893651b0a516f987ab6f366efdbf"}}, "hash": "60d53f10a67a2db0e2007098c7e18d3a4fa11f558eb2c7438df5da489bbfae8a", "text": "DuckDB is a new analytical data management system that is built for an in-process use case.DuckDB speaks SQL, has no external dependencies, and is deeply integrated into the Python ecosystem.DuckDB is Free and Open Source software under the MIT license.DuckDB uses state-of-the art query processing techniques with vectorized execution, lightweight compression, and morsel-driven automatic parallelism.DuckDB is out-of-core capable, meaning that it is capable of not only reading datasets that are bigger than main memory.This allows for analysis of far greater datasets and in many cases removes the need to run separate infrastructure.The \u201cduckdb\u201d Python package is not a client to the DuckDB system, it provides the entire database engine.DuckDB runs without any external server directly inside the Python process.Once there, DuckDB can run complex SQL queries on data frames in Pandas, Polars or PyArrow formats out-of-the box.DuckDB can also directly ingest files in Parquet, CSV or JSON formats.Because DuckDB runs in the same process, data transfer are virtually instantaneous.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c64baa1b-48fc-4917-bf79-0529a6354f0f": {"__data__": {"id_": "c64baa1b-48fc-4917-bf79-0529a6354f0f", "embedding": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5441b76-3ff7-596b-9e16-45abb13d0a77", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "138a922f6b512934ef0c5c4fa37a332a49b63cdc3362b7cb20c5c3aafc2da47a"}, "2": {"node_id": "68ad0027-e543-400d-b8a9-c1821ba70eb7", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "60d53f10a67a2db0e2007098c7e18d3a4fa11f558eb2c7438df5da489bbfae8a"}, "3": {"node_id": "233508d3-608f-426d-9fe4-213b3fbd2256", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "684e9f7004a85be8f1418a4e1f734a08d2c2727abfa777eb7790abf8e0aebcb0"}}, "hash": "494d20a041432d61a43357ce28544903cf81893651b0a516f987ab6f366efdbf", "text": "Conversely, DuckDB\u2019s query results can be transferred back into data frames very cheaply, allowing direct integration with complex downstream libraries such as PyTorch or TensorFlow.DuckDB enjoys fast-growing popularity, the Python package alone is currently downloaded around one million times a month.DuckDB has recently become the default backend of the Ibis project that offers a consistent interface in Python over a variety of data backends.This talk is aimed at two main groups, data analysts and data engineers.For the analysts, we will explain the user values of DuckDB, and how it can be used to improve their day-to-day lives.For data engineers, we will describe DuckDB\u2019s capabilities to become part of large automated data pipelines.The presenters for the proposed talk, Hannes M\u00fchleisen and Mark Raasveldt are the original creators of DuckDB, they are still leading the project and are deeply familiar with its Python integration.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "233508d3-608f-426d-9fe4-213b3fbd2256": {"__data__": {"id_": "233508d3-608f-426d-9fe4-213b3fbd2256", "embedding": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5441b76-3ff7-596b-9e16-45abb13d0a77", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "138a922f6b512934ef0c5c4fa37a332a49b63cdc3362b7cb20c5c3aafc2da47a"}, "2": {"node_id": "c64baa1b-48fc-4917-bf79-0529a6354f0f", "node_type": null, "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}, "hash": "494d20a041432d61a43357ce28544903cf81893651b0a516f987ab6f366efdbf"}}, "hash": "684e9f7004a85be8f1418a4e1f734a08d2c2727abfa777eb7790abf8e0aebcb0", "text": "- DuckDB Python API Overview: https://duckdb.org/docs/api/python/overview\r\n- DuckDB PyPI Download Statistics: https://pypistats.org/packages/duckdb\r\n- DuckDB Ibis Backend: https://ibis-project.org/backends/DuckDB/\r\n- Peer-reviewed paper about the concept behind DuckDB by the presenters\r\nhttps://www.cidrdb.org/cidr2020/papers/p23-raasveldt-cidr20.pdf\r\n- Talk about DuckDB at FOSDEM 2020 by Hannes: https://archive.fosdem.org/2020/schedule/event/duckdb/\r\n- Talk about DuckDB at CMU by Mark:\r\nhttps://www.youtube.com/watch?v=PFUZlNQIndo", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "14acdc8e-4682-4f6f-9515-14f59d314acd": {"__data__": {"id_": "14acdc8e-4682-4f6f-9515-14f59d314acd", "embedding": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c450cf56-9c5f-5fa4-8e9d-d05c0a5af981", "node_type": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "hash": "7829bd37e635086e646ea955bd07de75e73f9081f13168b087014002bb4c9fa3"}, "3": {"node_id": "d2124416-9247-428b-b1a6-0878552f97e3", "node_type": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "hash": "9d3c4bde5b965cc4977c0a65bce99aa842b46e8268a18ba273a12d31ff537c2c"}}, "hash": "e171a8ddce343590dc78517f9eef847fcb149097e12686562aaffa65fe33b2f4", "text": "Declarative data manipulation pipeline with Dagster\n\nBored of old pipeline orchestrator?Difficult to understand if data is up-to-date?Trouble with development workflow of data pipeline?Dagster, an open-source tool, offers a unique paradigm that simplifies the orchestration and management of data pipelines.By adopting declarative principles, data engineers and data scientists can build scalable, maintainable, and reliable pipelines effortlessly.We will commence with an introduction to Dagster, covering its fundamental concepts to ensure a comprehensive understanding of the material.Subsequently, we will explore practical scenarios and use cases, with also DBT for empower the power of SQL language.Minutes 0-5: Explain the design pattern problem of actual data pipeline framework.Minutes 5-15: Introduction to Dagster and its core concepts.Minutes 10-25: Practical examples of building declarative data pipelines with Dagster, with also DBT, the power of gRPC server.Minutes 25-30: Q&A and conclusion.Are you tired of struggling with outdated pipeline orchestrators?Do you find it challenging to ensure your data is always up-to-date?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d2124416-9247-428b-b1a6-0878552f97e3": {"__data__": {"id_": "d2124416-9247-428b-b1a6-0878552f97e3", "embedding": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c450cf56-9c5f-5fa4-8e9d-d05c0a5af981", "node_type": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "hash": "7829bd37e635086e646ea955bd07de75e73f9081f13168b087014002bb4c9fa3"}, "2": {"node_id": "14acdc8e-4682-4f6f-9515-14f59d314acd", "node_type": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "hash": "e171a8ddce343590dc78517f9eef847fcb149097e12686562aaffa65fe33b2f4"}, "3": {"node_id": "1ed03643-25db-445b-b684-e1efefe280bf", "node_type": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "hash": "6ef74f06f2ffd086fd80be0c4fd69950e40dc74ddf4f58ec40da0c61a291a405"}}, "hash": "9d3c4bde5b965cc4977c0a65bce99aa842b46e8268a18ba273a12d31ff537c2c", "text": "Are you facing difficulties with the development workflow of your data pipeline?In this session, we will introduce Dagster, an open-source tool that revolutionizes the orchestration and management of data pipelines.By embracing declarative principles, data engineers and data scientists can effortlessly build scalable, maintainable, and reliable pipelines.We will begin by providing an overview of the design pattern problem that many existing data pipeline frameworks face.Understanding the limitations of these frameworks will set the stage for exploring the transformative capabilities of Dagster\r\n\r\nNext, we will delve into the core concepts of Dagster, ensuring a comprehensive understanding of the material.You will learn how Dagster simplifies pipeline development and execution by providing a declarative and intuitive approach.Through practical examples and hands-on demonstrations, we will showcase how you can leverage Dagster to build powerful data pipelines.But that's not all!We will also explore the integration of DBT, empowering you to harness the full potential of the SQL language within your data pipelines.You will witness the synergy between Dagster and DBT, unlocking new possibilities for data manipulation and transformation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1ed03643-25db-445b-b684-e1efefe280bf": {"__data__": {"id_": "1ed03643-25db-445b-b684-e1efefe280bf", "embedding": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c450cf56-9c5f-5fa4-8e9d-d05c0a5af981", "node_type": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "hash": "7829bd37e635086e646ea955bd07de75e73f9081f13168b087014002bb4c9fa3"}, "2": {"node_id": "d2124416-9247-428b-b1a6-0878552f97e3", "node_type": null, "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}, "hash": "9d3c4bde5b965cc4977c0a65bce99aa842b46e8268a18ba273a12d31ff537c2c"}}, "hash": "6ef74f06f2ffd086fd80be0c4fd69950e40dc74ddf4f58ec40da0c61a291a405", "text": "By the end, you'll be equipped with the knowledge and inspiration to elevate your data pipeline workflows to new heights.Outline:\r\n\r\nMinutes 0-5: Understanding the design pattern problem of existing data pipeline frameworks\r\nMinutes 5-15: Introduction to Dagster and its core concepts\r\nMinutes 10-25: Practical examples of building declarative data pipelines with Dagster, including the integration with DBT and the power of gRPC server\r\nMinutes 25-30: Q&A and conclusion", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c349aed2-5b64-4672-a89c-1f9a4e2bbbf2": {"__data__": {"id_": "c349aed2-5b64-4672-a89c-1f9a4e2bbbf2", "embedding": null, "metadata": {"title": "PyIceberg: Tipping your toes into the petabyte data-lake"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9d34ab2-6fdf-5b41-82c0-2365a11d012e", "node_type": null, "metadata": {"title": "PyIceberg: Tipping your toes into the petabyte data-lake"}, "hash": "d23a1ab2d5def466dc7f66243782167d06b1cf79b5113e5d0b8202ffbf9ad6f4"}, "3": {"node_id": "6f8eaa7a-34dc-4855-b8a5-4bcf54471140", "node_type": null, "metadata": {"title": "PyIceberg: Tipping your toes into the petabyte data-lake"}, "hash": "1332d331f23b8175d917f1bc58c842ec2a9ebe62fd58c633eada5651f28196b5"}}, "hash": "38d537b6d931d8ebb46e0e6840112c2d55248056c6344fa03a1626ef476b75f5", "text": "PyIceberg: Tipping your toes into the petabyte data-lake\n\nWith Apache Iceberg, you store your big data in the cloud as files (e.g., Parquet), but then query it as if it\u2019s a plain SQL table.You enjoy the endless scalability of the cloud, without having to worry about how to store, partition, or query your data efficiently.PyIceberg is the Python implementation of Apache Iceberg that loads your Iceberg tables into PyArrow (pandas), DuckDB, or any of your preferred engines for doing data science.This means that with PyIceberg, you can tap into big data easily by only using Python.It\u2019s time to say goodbye to the ancient Hadoop-based frameworks of the past!In this talk, you'll learn why you need Iceberg, how to use it, and why it is so fast.Description: Working with high volumes of data has always been complex and challenging.Querying data with Spark requires you to know how the data is partitioned, otherwise, your query performance suffers tremendously.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6f8eaa7a-34dc-4855-b8a5-4bcf54471140": {"__data__": {"id_": "6f8eaa7a-34dc-4855-b8a5-4bcf54471140", "embedding": null, "metadata": {"title": "PyIceberg: Tipping your toes into the petabyte data-lake"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9d34ab2-6fdf-5b41-82c0-2365a11d012e", "node_type": null, "metadata": {"title": "PyIceberg: Tipping your toes into the petabyte data-lake"}, "hash": "d23a1ab2d5def466dc7f66243782167d06b1cf79b5113e5d0b8202ffbf9ad6f4"}, "2": {"node_id": "c349aed2-5b64-4672-a89c-1f9a4e2bbbf2", "node_type": null, "metadata": {"title": "PyIceberg: Tipping your toes into the petabyte data-lake"}, "hash": "38d537b6d931d8ebb46e0e6840112c2d55248056c6344fa03a1626ef476b75f5"}}, "hash": "1332d331f23b8175d917f1bc58c842ec2a9ebe62fd58c633eada5651f28196b5", "text": "The Apache Iceberg open table format fixes this by fixing the underlying storage, instead of by educating the end users.Iceberg originated at Netflix and provides a cloud-native layer on top of your data files.It solves traditional issues regarding correctness by supporting concurrent reading and writing to the table.Iceberg improves performance dramatically by collecting metrics on the data, having the ability to easily repartition your data, and being able to compact the underlying data.Finally, it supports time travel, so the model that you're training doesn't change because new data has been added.After this talk, you'll be comfortable using Apache Iceberg.Minutes 0-5: History and why we need a table format\r\nMinutes 5-15: Overview of Iceberg, and how it works under the hood\r\nMinutes 15-30: Introduction to PyIceberg with code and real examples (notebook!!)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a06b37c-6100-42ad-8866-768322a148c5": {"__data__": {"id_": "0a06b37c-6100-42ad-8866-768322a148c5", "embedding": null, "metadata": {"title": "Keynote Vicki Boykis"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e82f37c8-03f9-5cb5-92f2-8f157924b59d", "node_type": null, "metadata": {"title": "Keynote Vicki Boykis"}, "hash": "c5fb067f65ff26094019c1a7a405e64a730756661e970816d8759a2329d6395d"}}, "hash": "c5fb067f65ff26094019c1a7a405e64a730756661e970816d8759a2329d6395d", "text": "Keynote Vicki Boykis\n\nLorem ipsum dolor\n\nLorem ipsum dolor", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "64132b3c-fa33-4ae2-82df-aaf1f029018f": {"__data__": {"id_": "64132b3c-fa33-4ae2-82df-aaf1f029018f", "embedding": null, "metadata": {"title": "What the PDEP? An overview of some upcoming pandas changes"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "21e0cc6f-bb25-5f30-8afd-896a68790937", "node_type": null, "metadata": {"title": "What the PDEP? An overview of some upcoming pandas changes"}, "hash": "1665c4a41efeda648c5fecf5bd471fc62279514f0d39782db82e5cdae3d630dc"}, "3": {"node_id": "c6d66437-122d-47d3-8a8e-64ed191b5c11", "node_type": null, "metadata": {"title": "What the PDEP? An overview of some upcoming pandas changes"}, "hash": "3469e890920bdb6e0e969f8e286b027793c1de2f5e141afe77fbd08ff2542d91"}}, "hash": "f8279a76c56caafd4ded1fff73e7f0248658b8d08523c8bc53218d9edd2778f6", "text": "What the PDEP?An overview of some upcoming pandas changes\n\nLast year, the pandas community adopted a new process for making significant changes to the library: the Pandas Enhancement Proposals, aka PDEPs.In the meantime, several of those proposals have been proposed and discussed, and some already accepted.This talk will give an overview of some of the behavioural changes you can expect as a pandas user.Last year, the pandas community adopted a new process for making significant changes to the library: the Pandas Enhancement Proposals, aka PDEPs (similar to Python's PEPs and numpy's NEPs, ..).In the meantime, several of those proposals have been proposed and discussed, and some already accepted, shaping up the pandas roadmap (https://pandas.pydata.org/about/roadmap.html).The goal of this talk is to introduce you to this new process, and give an overview of a few of the proposed PDEPs.This way, you will learn about some of the behavioural changes you can expect as a pandas user in the near future.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c6d66437-122d-47d3-8a8e-64ed191b5c11": {"__data__": {"id_": "c6d66437-122d-47d3-8a8e-64ed191b5c11", "embedding": null, "metadata": {"title": "What the PDEP? An overview of some upcoming pandas changes"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "21e0cc6f-bb25-5f30-8afd-896a68790937", "node_type": null, "metadata": {"title": "What the PDEP? An overview of some upcoming pandas changes"}, "hash": "1665c4a41efeda648c5fecf5bd471fc62279514f0d39782db82e5cdae3d630dc"}, "2": {"node_id": "64132b3c-fa33-4ae2-82df-aaf1f029018f", "node_type": null, "metadata": {"title": "What the PDEP? An overview of some upcoming pandas changes"}, "hash": "f8279a76c56caafd4ded1fff73e7f0248658b8d08523c8bc53218d9edd2778f6"}}, "hash": "3469e890920bdb6e0e969f8e286b027793c1de2f5e141afe77fbd08ff2542d91", "text": "Over the many years of development, pandas has grown (or kept since the early days) quite some corner cases and inconsistencies.Some of the proposed PDEPs are an attempt to tackle those?For example, one accepted proposal is to ban any (up)casting in \"setitem-like\" operations, avoiding surprising data type changes.There is also a proposal to stop providing the inplace option for many methods, because even though the name might imply otherwise, those operations were not actually done in-place.Another major change that is under way is a change to the copy and view semantics of operations in pandas (related to the well-known (or hated) SettingWithCopyWarning).This is already available as an experimental opt-in to test and use the new behaviour, and will probably be a highlight of pandas 3.0.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a287e9e2-37a0-47ad-a4ad-a74ff94ba760": {"__data__": {"id_": "a287e9e2-37a0-47ad-a4ad-a74ff94ba760", "embedding": null, "metadata": {"title": "Turning your Data/AI algorithms into full web applications in no time with Taipy"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82e3db42-5170-5eac-ad21-977d67e729b8", "node_type": null, "metadata": {"title": "Turning your Data/AI algorithms into full web applications in no time with Taipy"}, "hash": "eaef1897efd297f9aca6e50656e906bc21110fade431379aedca284c6b698e9f"}, "3": {"node_id": "7a1b2479-89c5-4a64-a63c-8574a1241093", "node_type": null, "metadata": {"title": "Turning your Data/AI algorithms into full web applications in no time with Taipy"}, "hash": "c5f25f42c2f9ffba5beafcb57a9a595b09a53b3c69bc541c179d1386efa0354a"}}, "hash": "3c8f436e03bd53720963c0ff836ec89153916d211212734fd5e5e638910f4964", "text": "Turning your Data/AI algorithms into full web applications in no time with Taipy\n\nNumerous packages exist within the Python open-source ecosystem for algorithm building and data visualization.However, a significant challenge persists, with over 85% of Data Science Pilots failing to transition to the production stage.This talk introduces Taipy, an open-source Python library for front-end and back-end development.It enables Data Scientists and Python Developers to create pilots and production-ready applications for end-users.Its syntax facilitates the creation of interactive, customizable, and multi-page dashboards with augmented Markdown.Without the need for web development expertise (no CSS or HTML), users can generate highly interactive interfaces.Additionally, Taipy is engineered to construct robust and tailored data-driven back-end applications.Intuitive components like pipelines and data flow orchestration empower users to organize and manage data effectively.Taipy also introduces a unique Scenario Management functionality, facilitating \"what-if\" analysis for data scientists and end-users.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7a1b2479-89c5-4a64-a63c-8574a1241093": {"__data__": {"id_": "7a1b2479-89c5-4a64-a63c-8574a1241093", "embedding": null, "metadata": {"title": "Turning your Data/AI algorithms into full web applications in no time with Taipy"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82e3db42-5170-5eac-ad21-977d67e729b8", "node_type": null, "metadata": {"title": "Turning your Data/AI algorithms into full web applications in no time with Taipy"}, "hash": "eaef1897efd297f9aca6e50656e906bc21110fade431379aedca284c6b698e9f"}, "2": {"node_id": "a287e9e2-37a0-47ad-a4ad-a74ff94ba760", "node_type": null, "metadata": {"title": "Turning your Data/AI algorithms into full web applications in no time with Taipy"}, "hash": "3c8f436e03bd53720963c0ff836ec89153916d211212734fd5e5e638910f4964"}}, "hash": "c5f25f42c2f9ffba5beafcb57a9a595b09a53b3c69bc541c179d1386efa0354a", "text": "During this talk, we will showcase the capabilities of Taipy:\r\n- to create highly-interactive applications easily without any knowledge in web development.- to fill a void within the standard Python back-end stack, offering a powerful solution for data-driven applications.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6ba97669-ec4a-44ac-a3b3-e87483d02ef2": {"__data__": {"id_": "6ba97669-ec4a-44ac-a3b3-e87483d02ef2", "embedding": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a85e18c-c167-5721-994b-8a15333fe5af", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "c69c32031df31289fbf0ceeac0cc72f2516849b461ceacf052ae42b971ecf695"}, "3": {"node_id": "29840ed9-38e3-4d5c-af5e-494a344b3690", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "a7e3aafed9bde827ca338aa1d54632853d10c2233d8eb7f6d111e647b08d3fed"}}, "hash": "0f0fbd7437448776b75201939d98a3be93c47419a65d38c17e081ee5b5fe1bd1", "text": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com\n\nUntil a few years ago, data science & engineering at Booking.com had grown largely in an ad-hoc manner.This growth has led to a labyrinth of unrelated scripts representing Extract-Transform-Load (ETL) processes.Without options for quickly testing cross-application interfaces, maintenance and contribution grew unwieldy, and debugging in production was a common practice.Over the past several years, we\u2019ve spearheaded a transition from isolated workflows to a well-structured community-maintained monorepo - a task that required not just technical adaptation, but also a cultural shift.Central to this transformation is the adoption of the concept of \"tables as code\", an approach that has changed the way we write ETL.Our lightweight PySpark extension represents table metadata as a Python class, exposing data to code, and enabling efficient unit test setup and validation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "29840ed9-38e3-4d5c-af5e-494a344b3690": {"__data__": {"id_": "29840ed9-38e3-4d5c-af5e-494a344b3690", "embedding": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a85e18c-c167-5721-994b-8a15333fe5af", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "c69c32031df31289fbf0ceeac0cc72f2516849b461ceacf052ae42b971ecf695"}, "2": {"node_id": "6ba97669-ec4a-44ac-a3b3-e87483d02ef2", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "0f0fbd7437448776b75201939d98a3be93c47419a65d38c17e081ee5b5fe1bd1"}, "3": {"node_id": "4ee063a6-5de0-4993-9f65-771d9ad95928", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "362e13aa726a946734ea67355ff04b06fafd74cbc4bbcf09f6155c367334656a"}}, "hash": "a7e3aafed9bde827ca338aa1d54632853d10c2233d8eb7f6d111e647b08d3fed", "text": "In this talk, we walk you through \u201ctables as code\u201d design and complementary tools such as efficient unit testing, robust telemetry, and automated builds using Bazel.Moreover, we will cover the transformation process, including enabling people with non-engineering backgrounds to create fully tested and maintainable ETL.This includes internal training, maintainers, and support strategies aimed at fostering a community knowledgeable in best practices.Until a few years ago, data science & engineering at Booking.com had grown largely in an ad-hoc manner.This growth has led to a labyrinth of unrelated scripts representing Extract-Transform-Load (ETL) processes.Without options for quickly testing cross-application interfaces, maintenance and contribution grew unwieldy, and debugging in production was a common practice.Over the past several years, we\u2019ve spearheaded a transition from isolated workflows to a well-structured community-maintained monorepo - a task that required not just technical adaptation, but also a cultural shift.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4ee063a6-5de0-4993-9f65-771d9ad95928": {"__data__": {"id_": "4ee063a6-5de0-4993-9f65-771d9ad95928", "embedding": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a85e18c-c167-5721-994b-8a15333fe5af", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "c69c32031df31289fbf0ceeac0cc72f2516849b461ceacf052ae42b971ecf695"}, "2": {"node_id": "29840ed9-38e3-4d5c-af5e-494a344b3690", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "a7e3aafed9bde827ca338aa1d54632853d10c2233d8eb7f6d111e647b08d3fed"}, "3": {"node_id": "ffd95951-7d70-4bdd-9bd6-4e562e7b6e9f", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "327ad60e49d7bf0dde87a87e1b5f6b4a31b7e2a9385709a1414d6146383dd283"}}, "hash": "362e13aa726a946734ea67355ff04b06fafd74cbc4bbcf09f6155c367334656a", "text": "Central to this transformation is the adoption of the concept of \"tables as code\", an approach that has changed the way we write ETL.Our lightweight PySpark extension represents table metadata as a Python class, exposing data to code, and enabling efficient unit test setup and validation.In this talk, we walk you through \u201ctables as code\u201d design and complementary tools such as efficient unit testing, robust telemetry, and automated builds using Bazel.Moreover, we will cover the transformation process, including enabling people with non-engineering backgrounds to create fully tested and maintainable ETL.This includes internal training, maintainers, and support strategies aimed at fostering a community knowledgeable in best practices.This talk is aimed at ETL-adjacent data science practitioners, ideally who have been wondering how to push code quality forward at a data-centric organization.Introduction (0-5 minutes): We begin by shedding light on the infrastructure that hosted the old scripts, and discuss our motivation for change.It\u2019s worth mentioning that this transformative decision emerged from individual product teams, not from an executive mandate.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ffd95951-7d70-4bdd-9bd6-4e562e7b6e9f": {"__data__": {"id_": "ffd95951-7d70-4bdd-9bd6-4e562e7b6e9f", "embedding": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a85e18c-c167-5721-994b-8a15333fe5af", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "c69c32031df31289fbf0ceeac0cc72f2516849b461ceacf052ae42b971ecf695"}, "2": {"node_id": "4ee063a6-5de0-4993-9f65-771d9ad95928", "node_type": null, "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}, "hash": "362e13aa726a946734ea67355ff04b06fafd74cbc4bbcf09f6155c367334656a"}}, "hash": "327ad60e49d7bf0dde87a87e1b5f6b4a31b7e2a9385709a1414d6146383dd283", "text": "Tables as Code (10 minutes): We'll then introduce the concept of 'tables as code', detailing how this approach enables efficient testing.Monorepo Transformation (10 minutes): Building on this foundation, we'll explore how 'tables as code' grew into a vast monorepo with thousands of tests.We'll discuss how we scaled our processes and nurtured this project as a community effort.Community Growth and Future Plans (5 minutes): In our closing segment, we'll share insights gained from growing this project as a community, highlight strategies for orchestrating training, community support, and finally, share our future plans both within and outside our organization.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c68890f-2a56-43d8-8b8f-2cca9543bc77": {"__data__": {"id_": "0c68890f-2a56-43d8-8b8f-2cca9543bc77", "embedding": null, "metadata": {"title": "Graph Neural Networks for Real World Fraud Detection"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8de93a7-8a01-5b3c-ab6a-bafed8754db1", "node_type": null, "metadata": {"title": "Graph Neural Networks for Real World Fraud Detection"}, "hash": "4f7325cbbd1495c6dd44aca1afc05380a183ac5ec5a322ddda5b3f883176e037"}, "3": {"node_id": "6bd2fe90-317c-4662-8c2f-1c3f46926175", "node_type": null, "metadata": {"title": "Graph Neural Networks for Real World Fraud Detection"}, "hash": "7a73481a7ae4393cfb5fba72a248cdb6bcef9cb9786e06645c63326dacf240fd"}}, "hash": "459930ad110031bd289659f821f35190e913ec40ccc2d0d551ab1e5ab0087299", "text": "Graph Neural Networks for Real World Fraud Detection\n\nFraud is a major problem for financial services companies.As fraudsters change tactics, our detection methods need to get smarter.Graph neural networks (GNNs) are a promising model to improve detection performance.Unlike traditional machine learning models or rule-based engines, GNNs can effectively learn from subtle relationships by aggregating neighborhood information in the financial transaction networks.However, it remains a challenge to adopt this new approach in production.The goal of this talk is to share best practices for building a production ready GNN solution and hopefully spark your interest to apply GNNs to your own use cases.In this talk, we focus on suspicious account detection for online marketplaces.These platforms allow users to set up shops and sell products with little friction.Unfortunately, this attracts fraudsters who abuse these platforms.We use GNNs to do supervised learning based on accounts previously flagged as fraudulent, so that we can learn from both account properties and the relationship between accounts.However, productionizing GNNs is a big challenge.Addressing this challenge purely using open source packages is the main focus of this talk.We first give an overview of GNN-based fraud detection.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6bd2fe90-317c-4662-8c2f-1c3f46926175": {"__data__": {"id_": "6bd2fe90-317c-4662-8c2f-1c3f46926175", "embedding": null, "metadata": {"title": "Graph Neural Networks for Real World Fraud Detection"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8de93a7-8a01-5b3c-ab6a-bafed8754db1", "node_type": null, "metadata": {"title": "Graph Neural Networks for Real World Fraud Detection"}, "hash": "4f7325cbbd1495c6dd44aca1afc05380a183ac5ec5a322ddda5b3f883176e037"}, "2": {"node_id": "0c68890f-2a56-43d8-8b8f-2cca9543bc77", "node_type": null, "metadata": {"title": "Graph Neural Networks for Real World Fraud Detection"}, "hash": "459930ad110031bd289659f821f35190e913ec40ccc2d0d551ab1e5ab0087299"}}, "hash": "7a73481a7ae4393cfb5fba72a248cdb6bcef9cb9786e06645c63326dacf240fd", "text": "Then we deep dive into utilizing PySpark and GraphFrames to build a transaction graph in a scalable way and convert it to DGL (Deep Graph Library) format.Next we share our experiences of setting up training and inference graphs in different time intervals, and deploying the end-to-end model pipeline in Airflow.Attendees are required to have a basic understanding of machine learning.In this informative talk, they will gain insights into fraud detection's challenges and learn best practices to productionize GNNs.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fbea8871-0243-4c4a-923f-4b750eafcb29": {"__data__": {"id_": "fbea8871-0243-4c4a-923f-4b750eafcb29", "embedding": null, "metadata": {"title": "Promtly Evaluating Prompts with Bayesian Tournaments"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37b038f8-5842-5f59-b441-c26b96cd2690", "node_type": null, "metadata": {"title": "Promtly Evaluating Prompts with Bayesian Tournaments"}, "hash": "2d1c8c13f14c0ac42b4829faab3c6159d6c3be89bb57989852e8fd774fdea487"}}, "hash": "2d1c8c13f14c0ac42b4829faab3c6159d6c3be89bb57989852e8fd774fdea487", "text": "Promtly Evaluating Prompts with Bayesian Tournaments\n\nPick your next hot LLM prompt using a Bayesian tournament! Get a quick LLM dopamine hit with a side of decision theory vegetables. It's Bayesian Thunderdome: many prompts enter, one prompt leaves.\n\nHow do you chose the best LLM prompt systematically beyond guessing and vibes? Use the winner of a Bayesian tournament! Get a quick dopamine hit from fun LLM prompt magic with a side of Bayesian decision theory vegetables. If you are doing stuff with LLMs \u2014 you'll get a serious tool to improve your prompting game. If you're not using LLMs \u2014 you'll learn about Bayesian tournaments. They are not well known but have wide applicability: they help you optimally choose a winner using a minimal number of matches.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0f94b288-176a-41eb-a2e3-36c3b1e50f0c": {"__data__": {"id_": "0f94b288-176a-41eb-a2e3-36c3b1e50f0c", "embedding": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4219e333-9d0f-595f-a7ef-8ef7d4cdea8a", "node_type": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "hash": "2bb65ad1affda0290c154a630222d7e5a2ecdff3b612cd04cbaa71d8b8b1b987"}, "3": {"node_id": "01042a01-bc0b-437e-92e1-7261b83c7ba9", "node_type": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "hash": "b786ecfcecf912bc267578bfb12df4595a03c51c6e85bb62dcdd7545ae04d435"}}, "hash": "f0ec40b1f920b35dc88cd91c53d3634692e0c951ac32d58827fef85b47271a75", "text": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy\n\nData scientists in industry often have to wear many hats.They must navigate statistical validity, business acumen and strategic thinking, while also representing the end user.In this talk, we will talk about the pillars that make a metric the right one for a job, and how to choose appropriate Key Performance Indicators (KPIs) to drive product success and strategic gains.Our presentation will traverse the relationship of data science skills in product strategy - embracing the multifaceted role of the data scientist and navigating the journey from user segmentation to making data-driven decisions.1.The Data Scientist's Hat Trick: We initiate by emphasising the assorted roles that a data scientist plays in today's business landscape - from being a statistician ensuring the accuracy and validity of data to a strategist driving business decisions.[5 mins]\r\n\r\n2.Choosing Significant Metrics: Next, we'll delve into the nuances of selecting the right metric for the job.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "01042a01-bc0b-437e-92e1-7261b83c7ba9": {"__data__": {"id_": "01042a01-bc0b-437e-92e1-7261b83c7ba9", "embedding": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4219e333-9d0f-595f-a7ef-8ef7d4cdea8a", "node_type": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "hash": "2bb65ad1affda0290c154a630222d7e5a2ecdff3b612cd04cbaa71d8b8b1b987"}, "2": {"node_id": "0f94b288-176a-41eb-a2e3-36c3b1e50f0c", "node_type": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "hash": "f0ec40b1f920b35dc88cd91c53d3634692e0c951ac32d58827fef85b47271a75"}, "3": {"node_id": "edc76a48-3fb9-49c8-955a-c3b407523cb1", "node_type": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "hash": "04badb6cf7dbfe35a75cd9f1a46ff8da61cc75b8016bfa355ca9afbe3c5f9b39"}}, "hash": "b786ecfcecf912bc267578bfb12df4595a03c51c6e85bb62dcdd7545ae04d435", "text": "Specifically, we\u2019ll talk about the different pillars of metrics setting, for common data science responsibilities such as randomised controlled trials, offline evaluation, opportunity analysis etc.[7 mins]\r\n\r\n3.Setting The Right KPIs: Once metrics are defined, we'll venture into setting the correct KPIs - the small set of top line numbers that say if our venture is doing the job.[7 mins]\r\n\r\n4.Data-Driven Decision Making: Lastly, we'll elucidate how to leverage the data you've gathered to make informed, strategic decisions.This necessitates interpreting your metrics and KPIs, spotting trends, and making necessary adjustments to stay on course.[7 mins]\r\n\r\nIncorporating real-world case studies, we'll demonstrate how these concepts intertwine to contribute to product success.Learning Objectives:\r\n* Appreciate the multifaceted role of a data scientist in driving product strategies.* Learn to set realistic and challenging KPIs that align with your company's overarching objectives.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "edc76a48-3fb9-49c8-955a-c3b407523cb1": {"__data__": {"id_": "edc76a48-3fb9-49c8-955a-c3b407523cb1", "embedding": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4219e333-9d0f-595f-a7ef-8ef7d4cdea8a", "node_type": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "hash": "2bb65ad1affda0290c154a630222d7e5a2ecdff3b612cd04cbaa71d8b8b1b987"}, "2": {"node_id": "01042a01-bc0b-437e-92e1-7261b83c7ba9", "node_type": null, "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}, "hash": "b786ecfcecf912bc267578bfb12df4595a03c51c6e85bb62dcdd7545ae04d435"}}, "hash": "04badb6cf7dbfe35a75cd9f1a46ff8da61cc75b8016bfa355ca9afbe3c5f9b39", "text": "* Gain insights into leveraging data for informed decision-making and product strategy adjustments.Who Should Attend:\r\nThis talk is aimed for data professionals, however anyone involved in shaping product strategy and making data-driven decisions could find this useful.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fdcc5a66-1c5a-4f53-b785-51910876a809": {"__data__": {"id_": "fdcc5a66-1c5a-4f53-b785-51910876a809", "embedding": null, "metadata": {"title": "Lightning talks"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7896c0f3-e1e9-5103-a9af-2976ed5118d5", "node_type": null, "metadata": {"title": "Lightning talks"}, "hash": "f097eb0b470905ea6ddf8baca615bae74529dd40f505e58c60a19a108e4486a6"}}, "hash": "f097eb0b470905ea6ddf8baca615bae74529dd40f505e58c60a19a108e4486a6", "text": "Lightning talks\n\nLorem ipsum dolor\n\nLorem ipsum dolor", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c8f746e-7b94-46bd-93cf-a2fcd56b200b": {"__data__": {"id_": "0c8f746e-7b94-46bd-93cf-a2fcd56b200b", "embedding": null, "metadata": {"title": "Kickstart AI sponsored drinks [time & location TBD]"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68f57a2f-125a-57f2-ae71-5ae344efa647", "node_type": null, "metadata": {"title": "Kickstart AI sponsored drinks [time & location TBD]"}, "hash": "fd06d1481218429f3e8857c2aa29f6fe86ade1deaa3ae33f2968839ec8dd9139"}}, "hash": "fd06d1481218429f3e8857c2aa29f6fe86ade1deaa3ae33f2968839ec8dd9139", "text": "Kickstart AI sponsored drinks [time & location TBD]\n\nKickstart AI is a foundation powered by a coalition of iconic Dutch brands (Ahold Delhaize, ING, KLM and NS). Their mission is to accelerate AI adoption in the Netherlands, and improve society through the use of AI.\n\nLorem ipsum dolor", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5095e989-9f90-46c6-be52-3708616d7e91": {"__data__": {"id_": "5095e989-9f90-46c6-be52-3708616d7e91", "embedding": null, "metadata": {"title": "Designing a Machine Learning System"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1796a41f-f4b9-5498-b08a-069ce827c06d", "node_type": null, "metadata": {"title": "Designing a Machine Learning System"}, "hash": "bfe5b2b0da09e82b9acd9b64358ab6d3de37bcc11a389b40c7dc86ce54c7e159"}, "3": {"node_id": "c075a1e6-91e5-4d09-8800-dfcf062314e8", "node_type": null, "metadata": {"title": "Designing a Machine Learning System"}, "hash": "feebda29247aa9583589f907dd6a0de2ff002fda3367d1aa1137510b2fe7ff2c"}}, "hash": "5992cf7589da94357e621aac29ed18d55b6fb1f18db0a05fa06ec7313dbddf29", "text": "Designing a Machine Learning System\n\nAre you a machine learning practitioner struggling with designing, reasoning, and communicating about ML systems?Then this session is for you!With the industry moving towards end-to-end ML teams to enable them to implement MLOps practices, it is paramount for you to understand ML from a systems perspective.In this hands-on session, you will gain a thorough understanding of the technical intricacies of designing valuable, reliable and scalable ML systems.During this session you will improve your understanding of the technical intricacies of designing valuable, reliable and scalable ML systems.With others you will design a machine learning application from scratch, starting with clearly defining business requirements, framing the ML problem, to putting your design to paper.The session enables you to identify trade-offs and bottlenecks in a system.Plus, you\u2019ll learn how to effectively communicate and collaborate with other people on a system design.There will be a mix of theory and practice.We will explain the importance of ML system design, share a common methodology for designing an ML system, and apply it to a real business case.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c075a1e6-91e5-4d09-8800-dfcf062314e8": {"__data__": {"id_": "c075a1e6-91e5-4d09-8800-dfcf062314e8", "embedding": null, "metadata": {"title": "Designing a Machine Learning System"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1796a41f-f4b9-5498-b08a-069ce827c06d", "node_type": null, "metadata": {"title": "Designing a Machine Learning System"}, "hash": "bfe5b2b0da09e82b9acd9b64358ab6d3de37bcc11a389b40c7dc86ce54c7e159"}, "2": {"node_id": "5095e989-9f90-46c6-be52-3708616d7e91", "node_type": null, "metadata": {"title": "Designing a Machine Learning System"}, "hash": "5992cf7589da94357e621aac29ed18d55b6fb1f18db0a05fa06ec7313dbddf29"}}, "hash": "feebda29247aa9583589f907dd6a0de2ff002fda3367d1aa1137510b2fe7ff2c", "text": "A canvas will be provided physically for following the design methodology, and sheets/whiteboards/post-its to iterate on your design.This session is for all that have worked, or studied, in the field of ML/AI, or have affinity with the subject.The session is technical, but also interesting if you are a (product) manager in the field, as it will allow you to experience how technical designs are executed and ensure the design meets the business requirements.Very little ML algorithm knowledge is required, knowing the difference between classification and regression is enough.This Tutorial is for you if\u2026\r\n-\tYou want to improve your understanding of what makes a scalable machine learning application\r\n-\tYou want to make more conscious decisions when working on machine learning applications\r\n-\tYou want to be better at identifying trade-offs when making design decisions for your machine learning application\r\n-\tYou want to improve communicating your design decisions", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f3a42fba-5390-4b51-860e-569b73ff61c8": {"__data__": {"id_": "f3a42fba-5390-4b51-860e-569b73ff61c8", "embedding": null, "metadata": {"title": "Uncertainty visualization with ArviZ"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73e8f0d5-c586-5bb5-866f-9071f5bc2d40", "node_type": null, "metadata": {"title": "Uncertainty visualization with ArviZ"}, "hash": "07e54f339d9cf2a6ce55d0cf8ec0911591571708e3b59b246bec7c77593cf455"}}, "hash": "07e54f339d9cf2a6ce55d0cf8ec0911591571708e3b59b246bec7c77593cf455", "text": "Uncertainty visualization with ArviZ\n\nLearn how to visualize uncertainty in parameters or predictions using mutiple visualizations adapted to your data and task\n\nThis tutorial will cover 4 different plots designed to visualize uncertainty: quantile dotplots, empirical cumulative density functions, histograms and kernel density estimates. Finding optimal solutions in data visualization requires adapting the plots to both the data and task at hand, so we will start explaining each of them, their main pros and cons, how to interpret and how to generate each of those plot with ArviZ. We will then cover multiple data visualization tasks on several datasets to put those skills in practice and extend and deepen our \"visualization space\", both what you know and what you are comfortable enough to use.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e85aa15b-8cff-4976-8fa3-7c34d3a4ab20": {"__data__": {"id_": "e85aa15b-8cff-4976-8fa3-7c34d3a4ab20", "embedding": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e054832c-e8fd-56bb-987b-1ce5eb94d7dd", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "cf47996cf2785d8be9df184021b732638ed47186836fe9efb739482ddbdf6b33"}, "3": {"node_id": "80819620-31b8-43a0-829b-89e7e35cbad8", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "70fbe604b3c9dd1ebdb65c037b3db1e28abd52d25cc6d2a9498b15263495782f"}}, "hash": "876f969a6fc24ea7a22f4a1768b6c952cccdc25f71b0e7cbe0c90f75f1de8627", "text": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro\n\nProbabilistic predictions are predictions that include some statements about uncertainty of the prediction, e.g., prediction intervals that make statements about a likely range of values that a prediction can take.This workshop gives an introduction on making probabilistic predictions with the sktime and skpro python packages, for forecasting and supervised regression.Both packages are sklearn-compatible, built using skbase, with composable and modular interfaces.The presentation includes a practical primer of different types of probabilistic predictions, algorithms and estimators, and evaluation workflows, with python code examples.Probabilistic predictions make statements about the uncertainty or likely variation of the forecast, e.g., intervals at nominal coverage or conditional distributions.They appear in probabilistic forecasting as well as in probabilistic supervised (tabular) regression.This tutorial presents probabilistic forecasting capability in the skpro and sktime packages, combined with a methodological overview.sktime is a widely used package for time series, skpro covers probabilistic (tabular) regression.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "80819620-31b8-43a0-829b-89e7e35cbad8": {"__data__": {"id_": "80819620-31b8-43a0-829b-89e7e35cbad8", "embedding": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e054832c-e8fd-56bb-987b-1ce5eb94d7dd", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "cf47996cf2785d8be9df184021b732638ed47186836fe9efb739482ddbdf6b33"}, "2": {"node_id": "e85aa15b-8cff-4976-8fa3-7c34d3a4ab20", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "876f969a6fc24ea7a22f4a1768b6c952cccdc25f71b0e7cbe0c90f75f1de8627"}, "3": {"node_id": "c0c578e3-4bcc-4253-8b63-335f2f6fa4f3", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "555ebebeda367ab948b0a4015322c810d94ed4ffeafa0cd0408b68f2191d7e9d"}}, "hash": "70fbe604b3c9dd1ebdb65c037b3db1e28abd52d25cc6d2a9498b15263495782f", "text": "Both are based on skbase, and designed for interoperability with each other and sklearn.This tutorial presents the joint designs for probabilistic predictions and modular estimator interfaces.It also gives an overview of pipelines, tuning using probabilistic metrics, and compositors that can be used to turning any point forecaster into probabilistic forecasters, such as conformal or empirical interval estimators.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c0c578e3-4bcc-4253-8b63-335f2f6fa4f3": {"__data__": {"id_": "c0c578e3-4bcc-4253-8b63-335f2f6fa4f3", "embedding": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e054832c-e8fd-56bb-987b-1ce5eb94d7dd", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "cf47996cf2785d8be9df184021b732638ed47186836fe9efb739482ddbdf6b33"}, "2": {"node_id": "80819620-31b8-43a0-829b-89e7e35cbad8", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "70fbe604b3c9dd1ebdb65c037b3db1e28abd52d25cc6d2a9498b15263495782f"}, "3": {"node_id": "872e3b26-8321-4c01-a794-8da24fec212e", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "844bd8098f8bdf9771338b7dbb3640a2d9d2ff70d2a39f94eae75af618e25045"}}, "hash": "555ebebeda367ab948b0a4015322c810d94ed4ffeafa0cd0408b68f2191d7e9d", "text": "The presentation will showcase skpro and sktime, for tabular and time series tasks:\r\n- probabilistic prediction interfaces\r\n- metrics, e.g., quantile loss, or CRPS, log-loss for distribution forecasts\r\n- tuning using probabilistic metrics\r\n- conformal probabilistic intervals for any pipeline\r\n- compositors to make any point prediction estimator probabilistic\r\n- for time series: hierarchical and global probabilistic forecasts, reduction to regression\r\n\r\nFrom a methodological perspective, we will cover:\r\n\u2022\tinterval forecasts: producing intervals with a nominal probability of the observation to be contained in the interval\r\n\u2022\tquantile forecasts: specifying one or multiple quantiles of a predictive forecast distribution\r\n\u2022\tfully probabilistic forecasts: producing a symbolic representation of a predictive forecast distribution\r\n\u2022\tsimulators or samplers from probabilistic forecasting models\r\nAs research on software interfaces and mathematical conceptualization in this area is still an ongoing endeavour, challenges will also be discussed, with invitations to contribute.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "872e3b26-8321-4c01-a794-8da24fec212e": {"__data__": {"id_": "872e3b26-8321-4c01-a794-8da24fec212e", "embedding": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e054832c-e8fd-56bb-987b-1ce5eb94d7dd", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "cf47996cf2785d8be9df184021b732638ed47186836fe9efb739482ddbdf6b33"}, "2": {"node_id": "c0c578e3-4bcc-4253-8b63-335f2f6fa4f3", "node_type": null, "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}, "hash": "555ebebeda367ab948b0a4015322c810d94ed4ffeafa0cd0408b68f2191d7e9d"}}, "hash": "844bd8098f8bdf9771338b7dbb3640a2d9d2ff70d2a39f94eae75af618e25045", "text": "sktime and skpro are developed by an open community, with aims of ecosystem integration in a neutral, charitable space.We welcome contributions and seek to provides opportunity for anyone worldwide.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c89e546e-c997-47aa-8130-30c8c6f4d7a4": {"__data__": {"id_": "c89e546e-c997-47aa-8130-30c8c6f4d7a4", "embedding": null, "metadata": {"title": "Unconference: Interviews: Tips and Stories from Both Sides"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20780d65-1ec2-51f9-b7e2-ab25c930becc", "node_type": null, "metadata": {"title": "Unconference: Interviews: Tips and Stories from Both Sides"}, "hash": "a060e721265a83a8e7357617f8874aeb1597daa358690b1354361c507dbaac52"}, "3": {"node_id": "6867abd6-99eb-480b-be4c-feea03f9a951", "node_type": null, "metadata": {"title": "Unconference: Interviews: Tips and Stories from Both Sides"}, "hash": "0dd6545ea2376e8b94c14b5da27c6f2009f1fe3d380f3d71edee1f03de63b394"}}, "hash": "229cb5e436f6b0b2071cf30eca4d0ead48e510193ae31757206309930b7f52da", "text": "Unconference: Interviews: Tips and Stories from Both Sides\n\nYou are a data science or a machine learning engineering, and you applied for a position.You thought your interview went well, but still got a negative response... What might went wrong?In this talk, we will explore how things may go wrong from both applicant and interviewer side, and what can you do about it.You studied for years, and maybe have years of industry experience in your pocket.You found a nice opportunity and you applied for the job position.You did your best, but still got rejected.Two things for sure are that you are not alone, and it will not be your first time.But how did the interviewers reach to that decision?Could it be about your technical level, the colour of your shirt, or the interviewer did not have enough coffee that morning?In this talk I you will hear about stories, tips and how technical interviewers may reach to a decision:\r\n- What are the red flags from both applicant and interviewer sides?- How interviews might go wrong from both the applicant and the interviewer sides?- What should you expect from your technical interviews?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6867abd6-99eb-480b-be4c-feea03f9a951": {"__data__": {"id_": "6867abd6-99eb-480b-be4c-feea03f9a951", "embedding": null, "metadata": {"title": "Unconference: Interviews: Tips and Stories from Both Sides"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20780d65-1ec2-51f9-b7e2-ab25c930becc", "node_type": null, "metadata": {"title": "Unconference: Interviews: Tips and Stories from Both Sides"}, "hash": "a060e721265a83a8e7357617f8874aeb1597daa358690b1354361c507dbaac52"}, "2": {"node_id": "c89e546e-c997-47aa-8130-30c8c6f4d7a4", "node_type": null, "metadata": {"title": "Unconference: Interviews: Tips and Stories from Both Sides"}, "hash": "229cb5e436f6b0b2071cf30eca4d0ead48e510193ae31757206309930b7f52da"}}, "hash": "0dd6545ea2376e8b94c14b5da27c6f2009f1fe3d380f3d71edee1f03de63b394", "text": "- How can we structure a better interview process?Whether you are hiring or applying, you will have a better understand what is happening at the other side of the table!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26b6170b-5340-4e52-8e2c-8971d0ff2c33": {"__data__": {"id_": "26b6170b-5340-4e52-8e2c-8971d0ff2c33", "embedding": null, "metadata": {"title": "Unconference #2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38fcc911-f1bf-53f2-a217-fa7979592794", "node_type": null, "metadata": {"title": "Unconference #2"}, "hash": "4514070895eeed4c1b0f1da9e40b77ded5d5050a1a7f183e0de5752591585387"}}, "hash": "4514070895eeed4c1b0f1da9e40b77ded5d5050a1a7f183e0de5752591585387", "text": "Unconference #2\n\nLorem ipsum dolor\n\nLorem ipsum dolor", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a53a0321-3d18-470e-8e31-004ed623593f": {"__data__": {"id_": "a53a0321-3d18-470e-8e31-004ed623593f", "embedding": null, "metadata": {"title": "Innovation in the Age of Regulation: Federated Learning with Flower"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1316bcc-7a02-5d20-83ac-77806aa91efd", "node_type": null, "metadata": {"title": "Innovation in the Age of Regulation: Federated Learning with Flower"}, "hash": "aa0088579390d0120c0b947fdc500bc631a236f98b9c24b763b4a575352a90db"}, "3": {"node_id": "4cd10bd5-f299-42bf-846e-383912adc23e", "node_type": null, "metadata": {"title": "Innovation in the Age of Regulation: Federated Learning with Flower"}, "hash": "a8e7d37438a79dad86fe43d5bf16f6edd082e783c7eddc4cfe0e75fc245d5c60"}}, "hash": "d3b2a186f61839d02ac17203803e0108a9ebcca7532cd47a06188084eb94b2e0", "text": "Innovation in the Age of Regulation: Federated Learning with Flower\n\nWith the rise of data privacy concerns around AI in the EU, how can we innovate using AI capabilities despite regulations around consumer data?What tools and features are available to help us build AI in regulated industries?This talk will discuss how we can leverage diverse datasets to build better AI models without ever having to touch the datasets by using a Python library called Flower.In this talk, we\u2019ll review the importance of data privacy concerns, particularly in the EU, and address how we can build AI using sensitive data.We'll discuss a few machine learning techniques (classical, distributed and federated learning), and show how federated learning can help us train AI models without ever touching the sensitive data.Then, we'll evaluate a few main open source Python packages that help engineers get started with federated learning and why Flower is a valuable option to consider for your next project.We'll review the core features of Flower; most notably, it's ease of use.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4cd10bd5-f299-42bf-846e-383912adc23e": {"__data__": {"id_": "4cd10bd5-f299-42bf-846e-383912adc23e", "embedding": null, "metadata": {"title": "Innovation in the Age of Regulation: Federated Learning with Flower"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1316bcc-7a02-5d20-83ac-77806aa91efd", "node_type": null, "metadata": {"title": "Innovation in the Age of Regulation: Federated Learning with Flower"}, "hash": "aa0088579390d0120c0b947fdc500bc631a236f98b9c24b763b4a575352a90db"}, "2": {"node_id": "a53a0321-3d18-470e-8e31-004ed623593f", "node_type": null, "metadata": {"title": "Innovation in the Age of Regulation: Federated Learning with Flower"}, "hash": "d3b2a186f61839d02ac17203803e0108a9ebcca7532cd47a06188084eb94b2e0"}}, "hash": "a8e7d37438a79dad86fe43d5bf16f6edd082e783c7eddc4cfe0e75fc245d5c60", "text": "After that, we\u2019ll jump into a demo and show how, with minimal code, a Python engineer can orchestrate a training job with multiple data sources using federated learning.We\u2019ll walk through different parameters that give engineers the power to control and fine tune the server without the hassle of knowing infrastructure or cloud architecture.By the end of this talk, you\u2019ll be able to:\r\n\r\n* Understand the role of federated learning in a landscape with increasing regulation around AI, particularly in the EU with the proposed Artificial Intelligence Act\r\n* Differentiate between federated learning and classical machine learning\r\n* Design your project so that it is in compliance with current and future legislation passed on how to use personal data\r\n* Build and fine tune a server that hosts the model weights for a model trained without seeing personal data\r\n* Understand options available to increase the privacy around the data that is used to train the model\r\n\r\nThere will be a link to a Github repo at the end of the talk that contains all the code used in the demo in order to help you get started with your first federated learning project.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9ed7246f-859a-4e3c-b853-ffbc7e254760": {"__data__": {"id_": "9ed7246f-859a-4e3c-b853-ffbc7e254760", "embedding": null, "metadata": {"title": "Using Kubernetes as your multi-tenant orchestration layer"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f739f49-0ed3-5db9-99bc-b4d02ae9ad97", "node_type": null, "metadata": {"title": "Using Kubernetes as your multi-tenant orchestration layer"}, "hash": "28d8023cb02b8d3bc6ec45726a17988739df636b77ab45c9c9e3e896952ed8a0"}}, "hash": "28d8023cb02b8d3bc6ec45726a17988739df636b77ab45c9c9e3e896952ed8a0", "text": "Using Kubernetes as your multi-tenant orchestration layer\n\nIntroducing Kubernetes as the orchestration layer for your on-premise data platform poses multiple challenges. From GitOps to introducing Spark and finally setting up stateful application like postgres.\n\nHadoop and big data used to be synonyms for each other. However, as time progressed, the tech stack community grew smaller and cloud-native technologies have taken the main stage. When your data platform consists out of multiple servers, the orchestration layer becomes a crucial part of the infrastructure. This is where Kubernetes comes in as the obvious successor of Yarn. Still, there are many hurdles to be able to set this up as a multi-tenant orchestrator on an on-premise data cluster. This presentation will cover why you need an orchestrator and the common challenges. From introducing Kubernetes itself and setting up a secure GitOps approach. To introducing data tools like Trino, Spark and Jupyterhub. And finally introducing stateful applications like Postgres databases and Kafka clusters.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cccf5f12-66b2-46a0-8e7a-bc5b23a0db36": {"__data__": {"id_": "cccf5f12-66b2-46a0-8e7a-bc5b23a0db36", "embedding": null, "metadata": {"title": "Let\u2019s exploit pickle, and `skops` to the rescue!"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4edb2500-cc63-510c-9d30-5dd259022b4a", "node_type": null, "metadata": {"title": "Let\u2019s exploit pickle, and `skops` to the rescue!"}, "hash": "9e11533891134f6a93b664b468f52409ccd3313037db9f78f0f93baff2613213"}, "3": {"node_id": "29b4ac38-bbf7-4b0b-9baa-9098daf45d9d", "node_type": null, "metadata": {"title": "Let\u2019s exploit pickle, and `skops` to the rescue!"}, "hash": "dc43b766a3f1568fbf8ef23a652696193d991334e2d1e722eaab17d12da42318"}}, "hash": "c1ebea166697af25b168c8c9e71f2fc9a7385c883ec74171f55cc6dd0b216553", "text": "Let\u2019s exploit pickle, and `skops` to the rescue!Pickle files can be evil and simply loading them can run arbitrary code on your system.This talk presents why that is, how it can be exploited, and how `skops` is tackling the issue for scikit-learn/statistical ML models.We go through some lower level pickle related machinery, and go in detail how the new format works.The pickle format has many vulnerabilities and loading them alone can run arbitrary code on the user\u2019s system [1].In this session we go through the process used by the pickle module to persist python objects, while demonstrating how they can be exploited.We go through how `__getstate__` and `__setstate__` are used, and how the output of a `__reduce__` method is used to reconstruct an object, and how one can have a malicious implementation of these methods to create a malicious pickle file without knowing how to manually create a pickle file by manipulating a file on a lower level.We also briefly touch on other known exploits and issues related to the format [2].", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "29b4ac38-bbf7-4b0b-9baa-9098daf45d9d": {"__data__": {"id_": "29b4ac38-bbf7-4b0b-9baa-9098daf45d9d", "embedding": null, "metadata": {"title": "Let\u2019s exploit pickle, and `skops` to the rescue!"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4edb2500-cc63-510c-9d30-5dd259022b4a", "node_type": null, "metadata": {"title": "Let\u2019s exploit pickle, and `skops` to the rescue!"}, "hash": "9e11533891134f6a93b664b468f52409ccd3313037db9f78f0f93baff2613213"}, "2": {"node_id": "cccf5f12-66b2-46a0-8e7a-bc5b23a0db36", "node_type": null, "metadata": {"title": "Let\u2019s exploit pickle, and `skops` to the rescue!"}, "hash": "c1ebea166697af25b168c8c9e71f2fc9a7385c883ec74171f55cc6dd0b216553"}}, "hash": "dc43b766a3f1568fbf8ef23a652696193d991334e2d1e722eaab17d12da42318", "text": "We also show how one can look inside a pickle file and the operations run by it while loading it, and how one could get an equivalent python script which would result in the output of the pickle file [3]\r\nThen I present an alternative format from the `skops` library [4] which can be used to store scikit-learn based models.We talk about what the format is, and how persistence and loading is done, and what we do to prevent loading malicious objects or to avoid running arbitrary code.This format can be used to store almost any scikit-learn estimator, as well as xgboost, lightgbm, and catboost models.- [1] https://peps.python.org/pep-0307/#security-issues\r\n- [2] https://github.com/moreati/pickle-fuzz\r\n- [3] https://github.com/trailofbits/fickling\r\n- [4] https://skops.readthedocs.io/en/stable/persistence.html", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ee7a64f4-fc28-400e-87a9-47b14dc64c81": {"__data__": {"id_": "ee7a64f4-fc28-400e-87a9-47b14dc64c81", "embedding": null, "metadata": {"title": "Mastering Recommendation Systems Evaluation: An A/B Testing Approach with Insights from the Industry"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd77e690-a63f-564c-8a15-5da291b8040f", "node_type": null, "metadata": {"title": "Mastering Recommendation Systems Evaluation: An A/B Testing Approach with Insights from the Industry"}, "hash": "de1908fd636e9bb514f54037faa58148e6d362e02badce762a7001a8ef7f1331"}, "3": {"node_id": "775d5324-9cac-49f3-b393-d2b5a73bc687", "node_type": null, "metadata": {"title": "Mastering Recommendation Systems Evaluation: An A/B Testing Approach with Insights from the Industry"}, "hash": "cf35aa3f4a75841d9858b0ad13b198cb825aedf948043646c831ad3e93f4c755"}}, "hash": "417e2b30a50f6b478e2cfe6944f47cbfb77924f8c34bda443c429ca13a890b98", "text": "Mastering Recommendation Systems Evaluation: An A/B Testing Approach with Insights from the Industry\n\nRecommendation systems shape personalized experiences across various sectors, but evaluating their effectiveness remains a significant challenge.Drawing on experiences from industry leaders such as Booking.com, this talk introduces a robust, practical approach to A/B testing for assessing the quality of recommendation systems.The talk is designed for data scientists, statisticians, and business professionals, offering real-world insights and industry tricks on setting up A/B tests, interpreting results, and circumventing common pitfalls.While basic familiarity with recommendation systems and A/B testing is beneficial, it's not a prerequisite.This talk aims to provide attendees with a practical understanding of A/B testing in the evaluation of recommendation systems, including unique insights from industry practices and specific tricks that enhance effectiveness.My report includes next steps:\r\n- Introduction to recommendation systems, their ubiquity, and the imperative for evaluation, including industry examples.- An overview of A/B testing and its vital role in assessing recommendation systems, supported by insights from Booking.com and other industry leaders.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "775d5324-9cac-49f3-b393-d2b5a73bc687": {"__data__": {"id_": "775d5324-9cac-49f3-b393-d2b5a73bc687", "embedding": null, "metadata": {"title": "Mastering Recommendation Systems Evaluation: An A/B Testing Approach with Insights from the Industry"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd77e690-a63f-564c-8a15-5da291b8040f", "node_type": null, "metadata": {"title": "Mastering Recommendation Systems Evaluation: An A/B Testing Approach with Insights from the Industry"}, "hash": "de1908fd636e9bb514f54037faa58148e6d362e02badce762a7001a8ef7f1331"}, "2": {"node_id": "ee7a64f4-fc28-400e-87a9-47b14dc64c81", "node_type": null, "metadata": {"title": "Mastering Recommendation Systems Evaluation: An A/B Testing Approach with Insights from the Industry"}, "hash": "417e2b30a50f6b478e2cfe6944f47cbfb77924f8c34bda443c429ca13a890b98"}}, "hash": "cf35aa3f4a75841d9858b0ad13b198cb825aedf948043646c831ad3e93f4c755", "text": "- Techniques for designing effective hypotheses for A/B tests, focusing on recommendation systems.- Choosing pertinent metrics for robust evaluation of recommendation systems with industry examples.- Conducting A/B tests: industry best practices, common pitfalls, and strategies for mitigation, reinforced by real-world cases.- Accurate interpretation of A/B testing results and management of statistical biases, with insights from the field.By the end of the talk, attendees will have a comprehensive understanding of how to apply A/B testing effectively to recommendation systems, select relevant metrics, interpret results accurately, and navigate common challenges, backed by industry best practices and practical examples.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5db5678f-2e9a-4956-a4e8-ad0ec4e92d96": {"__data__": {"id_": "5db5678f-2e9a-4956-a4e8-ad0ec4e92d96", "embedding": null, "metadata": {"title": "To One-Hot or Not: A guide to feature encoding and when to use what"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f418d8b0-4976-5fb0-ab84-49ddfeff58d2", "node_type": null, "metadata": {"title": "To One-Hot or Not: A guide to feature encoding and when to use what"}, "hash": "9d9d378edbc5802b7be138fd9df217c8c9ab928aded91890578984a9d1bc8cfa"}, "3": {"node_id": "2263dc47-4263-42df-8e6d-63aa2810c9df", "node_type": null, "metadata": {"title": "To One-Hot or Not: A guide to feature encoding and when to use what"}, "hash": "b7c52b89b5bbb11de1871cd16814e4a43f93bce1cbbf663d1de7a441068c2661"}}, "hash": "b9b1c5789f816b5e4e9a1a4f332c3f2ded07cad6aa3082f132fab3ae982ff89b", "text": "To One-Hot or Not: A guide to feature encoding and when to use what\n\nHave you ever struggled with a multitude of columns created by One Hot Encoder?Or decided to look beyond it, but found it hard to decide which feature encoder would be a good replacement?Good news, there are many encoding techniques that have been developed to address different types of categorical data.This talk will provide an overview on various encoding methods available in data science, and a guidance on decision making about which one is appropriate for the data at hand.Join this talk if you would like to hear about the importance of feature encoding and why it is important to not default to One Hot Encoding in every scenario.It will start with commonly used approaches and will progress into more advanced and powerful techniques which can help extract meaningful information from the data.For each presented encoder, after this talk you will know: \r\n- When to use it\r\n- When NOT to use it\r\n- Important considerations specific to the encoder\r\n- Python library that offers a built-in method with the encoder, facilitating easy integration into feature engineering pipelines.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2263dc47-4263-42df-8e6d-63aa2810c9df": {"__data__": {"id_": "2263dc47-4263-42df-8e6d-63aa2810c9df", "embedding": null, "metadata": {"title": "To One-Hot or Not: A guide to feature encoding and when to use what"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f418d8b0-4976-5fb0-ab84-49ddfeff58d2", "node_type": null, "metadata": {"title": "To One-Hot or Not: A guide to feature encoding and when to use what"}, "hash": "9d9d378edbc5802b7be138fd9df217c8c9ab928aded91890578984a9d1bc8cfa"}, "2": {"node_id": "5db5678f-2e9a-4956-a4e8-ad0ec4e92d96", "node_type": null, "metadata": {"title": "To One-Hot or Not: A guide to feature encoding and when to use what"}, "hash": "b9b1c5789f816b5e4e9a1a4f332c3f2ded07cad6aa3082f132fab3ae982ff89b"}}, "hash": "b7c52b89b5bbb11de1871cd16814e4a43f93bce1cbbf663d1de7a441068c2661", "text": "I will explore different feature encoding approaches and provide guidance for decision-making.I will cover simpler methods like Label, One Hot, and Frequency encoding, progressing to powerful techniques like Target and Rare Label encoding.Finally, I will explain more complex approaches like Weight of Evidence, Ratio of Probabilities, Decision Tree, and Catboost encoding.I will close the talk with summarizing the key takeaways.Target Audience: \r\nData scientists and anyone interested in feature encoding\r\n\r\nPrevious experience with feature encoders can be useful but is not mandatory to follow the talk.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a815d19-1f1f-404a-a926-81fd70e92597": {"__data__": {"id_": "0a815d19-1f1f-404a-a926-81fd70e92597", "embedding": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da6b1185-bfaf-568b-a2dd-2e5236440ea2", "node_type": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "hash": "05437e51229f4f047ae675bde2a32785f1d5c4aa9229564b9692875d18affd93"}, "3": {"node_id": "b2426976-f96a-4fae-bdc5-44e2d52fa83d", "node_type": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "hash": "8d4b8c4e5b9e9aaf1f86a1a16c7ea871e26309ad015ccf1ca825b3397cb3390e"}}, "hash": "64ae1f4ea489ded7997427c394e8ee559699499884bf37d27b851bb631ba7da7", "text": "Survival Analysis: a deep dive\n\nSurvival analysis was initially introduced to handle the data analysis required in use cases revolving death and treatment in health care.Due to its merit, this method has spread to many other domains for analyzing and modeling the data where the outcome is the time until an event of interest occurs.Domains such as finance, economy, sociology and engineering.This talk aims at unraveling the potential of survival analysis with examples from different domains.A taxonomy of the existing descriptive and predictive analytics algorithms in survival analysis are demonstrated.The concept of some candidate algorithms from each group are explained in detail, along with an example and implementation guideline using the right open source framework.This talk aims at introducing the tools and techniques within the survival analysis domain for analyzing the time until an event of interest occurs.Examples of such event are rehospitalization after being discharged from hospital (healthcare), device needing maintenance after (re)commissioning (manufacturing), finding a job after unemployment (economy), an asset being sold after listing for sale (real-estate/finance), getting rearrested after being released from prison (criminology/sociology), and many other examples.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b2426976-f96a-4fae-bdc5-44e2d52fa83d": {"__data__": {"id_": "b2426976-f96a-4fae-bdc5-44e2d52fa83d", "embedding": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da6b1185-bfaf-568b-a2dd-2e5236440ea2", "node_type": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "hash": "05437e51229f4f047ae675bde2a32785f1d5c4aa9229564b9692875d18affd93"}, "2": {"node_id": "0a815d19-1f1f-404a-a926-81fd70e92597", "node_type": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "hash": "64ae1f4ea489ded7997427c394e8ee559699499884bf37d27b851bb631ba7da7"}, "3": {"node_id": "804ef235-d6d9-46b3-9111-9b39f5f468ec", "node_type": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "hash": "f443517ab2b760090d1f4e427b3e93290d076a181d44c36a3c1906732c93b59e"}}, "hash": "8d4b8c4e5b9e9aaf1f86a1a16c7ea871e26309ad015ccf1ca825b3397cb3390e", "text": "The potential of survival analysis tools, in both descriptive and predictive analytics, are hidden to the data science community.As a result of this, such problems are often formulated as classification or regression, where this also comes with its own caveats and pitfalls.The aim of the talk is to simplify methods and algorithms in survival analysis with some shallow mathematical focus and starts by raising awareness about survival analysis and its potential and applications for the general audience.The descriptive and predictive algorithms within survival analysis address the data scientists with basic statistics and machine learning background, as the main audience of the talk.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "804ef235-d6d9-46b3-9111-9b39f5f468ec": {"__data__": {"id_": "804ef235-d6d9-46b3-9111-9b39f5f468ec", "embedding": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da6b1185-bfaf-568b-a2dd-2e5236440ea2", "node_type": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "hash": "05437e51229f4f047ae675bde2a32785f1d5c4aa9229564b9692875d18affd93"}, "2": {"node_id": "b2426976-f96a-4fae-bdc5-44e2d52fa83d", "node_type": null, "metadata": {"title": "Survival Analysis: a deep dive"}, "hash": "8d4b8c4e5b9e9aaf1f86a1a16c7ea871e26309ad015ccf1ca825b3397cb3390e"}}, "hash": "f443517ab2b760090d1f4e427b3e93290d076a181d44c36a3c1906732c93b59e", "text": "- Introduction to Survival Analysis\r\n- Applications in different domains\r\n- Formulating Survival Analysis Problem\r\n- Taxonomy of Descriptive & Predictive Methods with python packages\r\n- Overview of Descriptive Methods\r\n          - Kaplan-Meier [3 slide] \r\n          - Nelson-Aalen & Weibull [half slide]\r\n- Overview of Predictive Methods\r\n           - Cox Proportional Hazard [3 slide]\r\n           - Survival Tree & Forrest [1 slide]\r\n           - Deep Survival Analysis [1 slide]\r\n- Conclusion\r\n\r\nAt the end of the talk, the audience becomes aware of what survival analysis can do and which algorithms, with their corresponding python package, are the low hanging fruit in a data scientist toolbox.In addition, the audience will gain a structured overview on the topic so that any need for further knowledge acquisition could be independently followed in the future.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "84e65ea3-0818-4784-b4e2-22a08e59ac17": {"__data__": {"id_": "84e65ea3-0818-4784-b4e2-22a08e59ac17", "embedding": null, "metadata": {"title": "Deep look into Deepfakes: Mastering Creation, Impact, and Detection"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbb0b87d-a8ba-536e-b126-878f6600ff69", "node_type": null, "metadata": {"title": "Deep look into Deepfakes: Mastering Creation, Impact, and Detection"}, "hash": "0117c31e2a240a087ab69b45040764b3422c991ce378046cb7a53d3e2e04c8ab"}, "3": {"node_id": "634fd595-6c9a-4f98-b5ba-a6ea0b2a0fad", "node_type": null, "metadata": {"title": "Deep look into Deepfakes: Mastering Creation, Impact, and Detection"}, "hash": "2d3478e854c7a5c8f3eee33053e360b0f212ab1be86897d31618ac0f57cdeb56"}}, "hash": "a97487be4d4476f7d8f265ea45ba6fb659766a118fb0525f363638a83be86fa2", "text": "Deep look into Deepfakes: Mastering Creation, Impact, and Detection\n\nDeepfakes, a form of synthetic media where a person's image or video is seamlessly replaced using Generative AI like GANs, have recieved significant attention.This talk aims to provide a comprehensive exploration of deepfakes, covering their creation process, positive and negative effects, development pace, and tools for detection.By the end of the presentation, attendees will be equipped with how to create and detect deepfakes, a deep understanding of the technology and its impact.Talk Outline:\r\n\r\nI.How Deepfakes Work (Approx.8 minutes)\r\n\r\n- Step-by-step explanation of deepfake creation using an opensource tool\r\n- Clarifying the technical aspects behind manipulating existing media with AI algorithms\r\n\r\nII.Deepfakes with GANs (Approx.8 minutes)\r\n\r\n- Introduction to Generative Adversarial Networks (GANs) and their role in deepfake generation\r\n- Different types of GANs and how to craft realistic deepfakes\r\n\r\nIII.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "634fd595-6c9a-4f98-b5ba-a6ea0b2a0fad": {"__data__": {"id_": "634fd595-6c9a-4f98-b5ba-a6ea0b2a0fad", "embedding": null, "metadata": {"title": "Deep look into Deepfakes: Mastering Creation, Impact, and Detection"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbb0b87d-a8ba-536e-b126-878f6600ff69", "node_type": null, "metadata": {"title": "Deep look into Deepfakes: Mastering Creation, Impact, and Detection"}, "hash": "0117c31e2a240a087ab69b45040764b3422c991ce378046cb7a53d3e2e04c8ab"}, "2": {"node_id": "84e65ea3-0818-4784-b4e2-22a08e59ac17", "node_type": null, "metadata": {"title": "Deep look into Deepfakes: Mastering Creation, Impact, and Detection"}, "hash": "a97487be4d4476f7d8f265ea45ba6fb659766a118fb0525f363638a83be86fa2"}}, "hash": "2d3478e854c7a5c8f3eee33053e360b0f212ab1be86897d31618ac0f57cdeb56", "text": "The Good and the Bad (Approx.8 minutes)\r\n\r\n- Exploring the positive effects of deepfakes\r\n- Unveiling the negative implications of deepfakes\r\n- Real-world examples highlighting the ethical concerns\r\n- Speculating on the future developments of deepfake technology\r\n\r\nIV.How to Recognize Deepfakes (Approx.6 minutes)\r\n\r\n- Insight into the ongoing efforts to combat the misuse of deepfakes\r\n- Various approaches and AI-driven tools for detecting deepfake media \r\n- Understanding the limitations in detecting increasingly sophisticated deepfakes\r\n\r\nKey Takeaways:\r\n\r\n- In-depth understanding of deepfake creation and the role of GANs\r\n- Awareness of the positive and negative impacts of deepfakes in different domains\r\n- Real-world examples illustrating the ethical concerns surrounding deepfakes\r\n- Insights into the future trends and advancements in deepfake technology\r\n- Familiarity with a range of AI-based approaches and tools for detecting deepfakes", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c6431e49-1b5d-48e9-84d5-1be2ee34500b": {"__data__": {"id_": "c6431e49-1b5d-48e9-84d5-1be2ee34500b", "embedding": null, "metadata": {"title": "A data-driven approach for distributing scarce goods within the REWE retail supply chain"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79731e0e-a1d2-597c-88a9-69920acd5d09", "node_type": null, "metadata": {"title": "A data-driven approach for distributing scarce goods within the REWE retail supply chain"}, "hash": "65d9ed173b42716a60c0fc3e0f360c27b6a21dd466d516f5714c65cdbf84dddc"}, "3": {"node_id": "ec6b2bc7-f9e7-4ff2-9be3-5d1bd4e8e84c", "node_type": null, "metadata": {"title": "A data-driven approach for distributing scarce goods within the REWE retail supply chain"}, "hash": "34f85f5c534644e799ec738b2af505b54f122cd7375e7d0e9ad5b965a415d8a4"}}, "hash": "04724e06872264d84a8a3b1f551b876d13c545b3ae3ae60f3363c31e8f2815cd", "text": "A data-driven approach for distributing scarce goods within the REWE retail supply chain\n\nGlobal political circumstances and unpredictable crises such as the Covid-19 pandemic can cause a scarcity of grocery goods within the retail supply chain.We present a data-driven approach to ensure a fair and replicable distribution from the supplier to the retail warehouses at REWE, one of the largest grocery chains in Germany.Optimizing the distribution of scarce perishable goods presents a complex challenge in the retail industry.It requires balancing logistical constraints such as pallet weight and truck capacities with the need to ensure a fair distribution of goods according to available stock and customer demand.In this talk, we offer insights into the solutions we have developed to tackle this problem.We will begin by discussing our initial approach, which involves employing a greedy algorithm for distributing one product at a time.We will then delve into an enhanced simultaneous multi-product distribution approach, which constitutes a discrete optimization problem with nonlinear constraints and unfeasible combinatorial search spaces.We present our approach to reduce the complexity of the optimization problem and use algorithms from the scipy library to ensure stable converging solutions.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec6b2bc7-f9e7-4ff2-9be3-5d1bd4e8e84c": {"__data__": {"id_": "ec6b2bc7-f9e7-4ff2-9be3-5d1bd4e8e84c", "embedding": null, "metadata": {"title": "A data-driven approach for distributing scarce goods within the REWE retail supply chain"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79731e0e-a1d2-597c-88a9-69920acd5d09", "node_type": null, "metadata": {"title": "A data-driven approach for distributing scarce goods within the REWE retail supply chain"}, "hash": "65d9ed173b42716a60c0fc3e0f360c27b6a21dd466d516f5714c65cdbf84dddc"}, "2": {"node_id": "c6431e49-1b5d-48e9-84d5-1be2ee34500b", "node_type": null, "metadata": {"title": "A data-driven approach for distributing scarce goods within the REWE retail supply chain"}, "hash": "04724e06872264d84a8a3b1f551b876d13c545b3ae3ae60f3363c31e8f2815cd"}}, "hash": "34f85f5c534644e799ec738b2af505b54f122cd7375e7d0e9ad5b965a415d8a4", "text": "Throughout, we will discuss advantages and disadvantages of the approaches used.Furthermore, we will provide an overview of our tech stack, leveraging the Google Cloud architecture.Additionally, we will discuss our utilization of the Streamlit framework, enabling us to create an interactive dashboard that empowers users to leverage our optimization approaches in addressing upcoming challenges effectively.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3f29108f-7f5d-47b1-b5a2-6f864b66b4b2": {"__data__": {"id_": "3f29108f-7f5d-47b1-b5a2-6f864b66b4b2", "embedding": null, "metadata": {"title": "Balancing the electricity grid with multi-level forecasting models"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2604d42-c404-581f-a4f5-b1b2cc1ebca5", "node_type": null, "metadata": {"title": "Balancing the electricity grid with multi-level forecasting models"}, "hash": "2c667bc7eab4599b1e58251c89049882e065617226c904f2556213a341a76a28"}, "3": {"node_id": "63d9510b-9880-4415-85fe-56ef411c8c26", "node_type": null, "metadata": {"title": "Balancing the electricity grid with multi-level forecasting models"}, "hash": "207dc945ec1c80010e3f50a9a1abd1e10109364c91c88890b326a7d92d21b8ce"}}, "hash": "5c49ff6f71caae0e68ad8420291ea2cf80c6b97d1ed39e21c6d87b14717636fb", "text": "Balancing the electricity grid with multi-level forecasting models\n\nJoin us as we explore the complexities of balancing the electricity grid amidst the rise of renewable energy sources.We\u2019ll discover the challenges in forecasting electricity consumption from diverse industrial resources and the modelling techniques employed by Sympower to achieve accurate forecasts.Gain insights into the trade-offs involved in aggregating data at different hierarchical levels in time series forecasting.The shift to renewable energy sources presents a major challenge for the electricity grid: solar and wind facilities are constantly varying in power output, making it harder to keep the supply and demand in balance.This creates a need for demand response: strategic activation or deactivation of large industrial resources to balance the electricity grid.Reliable demand response requires an accurate forecast of industrial electricity consumption, to get a clear understanding of which resources can be controlled at what time.In this talk we will discuss the challenges faced when forecasting electricity consumption from industrial resources from different kinds of industries such as furnaces, greenhouses or paper mills.We\u2019ll discuss the different modelling approaches for predicting time series including regression, forecasting and deep learning, and we will discuss the suitability of each in different scenarios.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "63d9510b-9880-4415-85fe-56ef411c8c26": {"__data__": {"id_": "63d9510b-9880-4415-85fe-56ef411c8c26", "embedding": null, "metadata": {"title": "Balancing the electricity grid with multi-level forecasting models"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2604d42-c404-581f-a4f5-b1b2cc1ebca5", "node_type": null, "metadata": {"title": "Balancing the electricity grid with multi-level forecasting models"}, "hash": "2c667bc7eab4599b1e58251c89049882e065617226c904f2556213a341a76a28"}, "2": {"node_id": "3f29108f-7f5d-47b1-b5a2-6f864b66b4b2", "node_type": null, "metadata": {"title": "Balancing the electricity grid with multi-level forecasting models"}, "hash": "5c49ff6f71caae0e68ad8420291ea2cf80c6b97d1ed39e21c6d87b14717636fb"}}, "hash": "207dc945ec1c80010e3f50a9a1abd1e10109364c91c88890b326a7d92d21b8ce", "text": "Using the forecasting of electricity consumption of industrial resources as an example, we show how we make our forecasts at Sympower to help balance the electricity grid.Finally we will discuss a trade-off in forecasting: Trends and seasonality often only emerge at aggregate levels, making forecasting at the aggregate level easier.On the other hand, business often requires precision-level insights.Aggregate data is inherently less noisy since the errors tend to cancel out, but also might fail to capture lower-level details.We will discuss the considerations to make when forecasting at different aggregated levels in time or across groups, and what you could do to forecast consistently across different aggregate levels.. \r\n\r\nKEY TAKEAWAYS\r\n  - Gain insights into selecting the most suitable modelling technique for your forecasting need\r\n  - Understand the challenges posed by the evolving electricity grid and the significance of demand response\r\n  - Explore the trade-offs involved in aggregating data at different hierarchical or temporal levels in time series forecasting", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "80204bd9-b8c7-4947-8896-96e1b26606df": {"__data__": {"id_": "80204bd9-b8c7-4947-8896-96e1b26606df", "embedding": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c5cd37a-c881-51b5-ad88-bad2af81cbb7", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "ea14a13c9893a91225f40a8e558d7a6722cfd4256c619fe0862daf522360dce1"}, "3": {"node_id": "d91faaa8-b1dc-40e5-b632-f329e6644003", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "00c25d964167ad72dd60da2132cb49283088a5b8bd9f2c4d6ad56e4fe3dc71e0"}}, "hash": "a595a0c1f3cee7c8b27d5956e986444cdd554169712b327c7d628715bb635d4e", "text": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate\n\nThe customers of Picnic use images and texts of products to decide if they like our products, so why not include those data streams in our Temporal Fusion Transformers that we use for Product Demand Forecasting?Join us for a thrilling journey through convolutional, graph-based, and transformer-based architectures.Learn about methods to turn images, texts, and geographical information into features for other applications as we did for product demand forecasting.Discover how Picnic Technologies uses state-of-the-art multimodal approaches for demand forecasting to prevent food waste and keep our customers happy!Ever wondered how we keep your favorite brand of potato chips in stock, while that exotic sauce is forever \"currently unavailable\"?We'll reveal the secrets behind these mysteries in our talk on how we are using recent advancements in visual, textual, and contextual information processing techniques to optimize our Product Demand Forecasting.Because everybody loves looking at pictures of groceries but prefers having them available and on their doorstep (delivered for free).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d91faaa8-b1dc-40e5-b632-f329e6644003": {"__data__": {"id_": "d91faaa8-b1dc-40e5-b632-f329e6644003", "embedding": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c5cd37a-c881-51b5-ad88-bad2af81cbb7", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "ea14a13c9893a91225f40a8e558d7a6722cfd4256c619fe0862daf522360dce1"}, "2": {"node_id": "80204bd9-b8c7-4947-8896-96e1b26606df", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "a595a0c1f3cee7c8b27d5956e986444cdd554169712b327c7d628715bb635d4e"}, "3": {"node_id": "18a0c84c-a5ea-4045-aab7-e46cbc156777", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "b081cde7af77a80d58b0fc495c344e958837d1174318c17757c31aafeb6ecf0f"}}, "hash": "00c25d964167ad72dd60da2132cb49283088a5b8bd9f2c4d6ad56e4fe3dc71e0", "text": "We begin by shedding light on traditional product demand forecasting - the 'old potatoes' of the industry - and its limitations, like the notorious cold start problem and category dynamics.Our talk is a must-watch for data scientists, product managers, supply chain wizards, and anyone who has ever been curious about the new innovations in number-crunching that gets your favorite snack from the factory to your front door.If you're in the e-commerce or retail industries, this talk will be as essential as oatmilk and bread in a shopping list.Don\u2019t worry if words like multimodal, temporal, and fusion sound intimidating; They will be explained in a way that is informative and entertaining if you have seen them before but also if you have not.We promise it\u2019s not all graphs and matrices \u2013 expect an unexpected rollercoaster ride through the aisle of our digital store.With each turn, you'll discover how our multimodal method uses product images, textual descriptions, and additional contextual information to predict if potatoes will overtake pasta in popularity next month.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "18a0c84c-a5ea-4045-aab7-e46cbc156777": {"__data__": {"id_": "18a0c84c-a5ea-4045-aab7-e46cbc156777", "embedding": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c5cd37a-c881-51b5-ad88-bad2af81cbb7", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "ea14a13c9893a91225f40a8e558d7a6722cfd4256c619fe0862daf522360dce1"}, "2": {"node_id": "d91faaa8-b1dc-40e5-b632-f329e6644003", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "00c25d964167ad72dd60da2132cb49283088a5b8bd9f2c4d6ad56e4fe3dc71e0"}, "3": {"node_id": "42b9a7a5-57d2-4c75-a708-4970e5a3cdc4", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "4499a8fb9348b4344fe5256a60cd2d43355236e53e9d41c7645cbfbb9ba2ecda"}}, "hash": "b081cde7af77a80d58b0fc495c344e958837d1174318c17757c31aafeb6ecf0f", "text": "We'll show you the \u2018cart\u2019 loads of data behind these predictions, putting a fun spin on the world of groceries.In the grand finale, we\u2019ll take you behind the scenes of our model's showdown with traditional methods.Spoiler alert: our method doesn\u2019t just predict demand; it leaves the traditional methods looking like overripe bananas in the back of the fridge (which is a bad state for bananas to be in).The main takeaway from our talk - besides a craving for potatoes - will be an understanding of multimodal demand forecasting and how all these different types of data are becoming easier and easier to use for real-world business value.By the end of our talk, you'll be filled with ideas (and the sudden need to do groceries with Picnic, you are our target audience: Loving reliability, good products and you have busy jobs), inspired by the potential of multimodal machine learning in forecasting.So, whether you're a data scientist, product manager, or a curious shopper, come along for an enjoyable trip through the world of groceries and demand forecasting!Prepare your shopping list and join us.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "42b9a7a5-57d2-4c75-a708-4970e5a3cdc4": {"__data__": {"id_": "42b9a7a5-57d2-4c75-a708-4970e5a3cdc4", "embedding": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c5cd37a-c881-51b5-ad88-bad2af81cbb7", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "ea14a13c9893a91225f40a8e558d7a6722cfd4256c619fe0862daf522360dce1"}, "2": {"node_id": "18a0c84c-a5ea-4045-aab7-e46cbc156777", "node_type": null, "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}, "hash": "b081cde7af77a80d58b0fc495c344e958837d1174318c17757c31aafeb6ecf0f"}}, "hash": "4499a8fb9348b4344fe5256a60cd2d43355236e53e9d41c7645cbfbb9ba2ecda", "text": "Just remember, our model may predict the demand for potatoes, but it's still up to you to remember the dip!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4b310b5f-430d-4b3a-b664-1dd04b2e798d": {"__data__": {"id_": "4b310b5f-430d-4b3a-b664-1dd04b2e798d", "embedding": null, "metadata": {"title": "Minimizing the Data Mesh Mess"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7afe76a0-a34b-5e97-95b5-c3192c87e543", "node_type": null, "metadata": {"title": "Minimizing the Data Mesh Mess"}, "hash": "5c390712491156260e22672e5af10724f9bd8761444b76e8b5c66f99542cca4f"}, "3": {"node_id": "182123cd-68a9-41f3-9655-fb8d24bbb0e1", "node_type": null, "metadata": {"title": "Minimizing the Data Mesh Mess"}, "hash": "cbc12527ed187edaad4764b7a34eb09382b1b106e3ad7b69ae60eaac4ccf314e"}}, "hash": "e05fd62714765a240edebe0bb4e1d31596fa469e4b8cf375e76d187405b3f0e5", "text": "Minimizing the Data Mesh Mess\n\nThis talk delves into the topic of minimizing the Data Mesh mess.We will explore practical strategies and a data platform architecture for effectively governing and managing data within a decentralized data setup.We can balance decentralization and maintaining data quality by imposing a few constraints.The takeaways of this talk are drawn from the data platform at Enza Zaden.The concept of Data Mesh has gained significant attention in recent years, offering a promising approach to managing data at scale.However, as organizations embrace the decentralized approach to data, they often encounter unforeseen challenges and potential chaos that can arise within a Data Mesh implementation.This talk delves into the topic of minimizing the Data Mesh mess.We will explore practical strategies and a data platform architecture for effectively governing and managing data within a decentralized data setup.We can balance decentralization with maintaining data quality by imposing a few constraints.During the session, we will address key questions and considerations such as:\r\n- What constraints can we enforce to increase the quality of shared data while remaining adaptable to the data teams?- How can we effectively manage data ownership, access controls, and security across diverse data domains?- What are the recommended approaches for metadata management?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "182123cd-68a9-41f3-9655-fb8d24bbb0e1": {"__data__": {"id_": "182123cd-68a9-41f3-9655-fb8d24bbb0e1", "embedding": null, "metadata": {"title": "Minimizing the Data Mesh Mess"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7afe76a0-a34b-5e97-95b5-c3192c87e543", "node_type": null, "metadata": {"title": "Minimizing the Data Mesh Mess"}, "hash": "5c390712491156260e22672e5af10724f9bd8761444b76e8b5c66f99542cca4f"}, "2": {"node_id": "4b310b5f-430d-4b3a-b664-1dd04b2e798d", "node_type": null, "metadata": {"title": "Minimizing the Data Mesh Mess"}, "hash": "e05fd62714765a240edebe0bb4e1d31596fa469e4b8cf375e76d187405b3f0e5"}}, "hash": "cbc12527ed187edaad4764b7a34eb09382b1b106e3ad7b69ae60eaac4ccf314e", "text": "- How can we modify the level of flexibility and ownership for data teams depending on their level of experience and readiness?Drawing from a real-world data platform at Enza Zaden (https://www.enzazaden.com/), we will discuss successful strategies and best practices for taming the inherent complexity of a Data Mesh.This talk aims to provide practical insights and actionable steps to help you create a data platform that minimizes the Data Mesh mess.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4b93c82-f872-4e29-a54f-e7e50b98601f": {"__data__": {"id_": "c4b93c82-f872-4e29-a54f-e7e50b98601f", "embedding": null, "metadata": {"title": "Return to Data's Inferno: are the 7 layers of data testing hell still relevant?"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31d15b15-0304-58da-91d1-947c6fcb9995", "node_type": null, "metadata": {"title": "Return to Data's Inferno: are the 7 layers of data testing hell still relevant?"}, "hash": "966e707ea325d8a441b3c8b32021b6680a12340b49c674beb9026ad979f6ce22"}, "3": {"node_id": "5d5ade97-5f47-4b27-8f75-35490c5c2fbe", "node_type": null, "metadata": {"title": "Return to Data's Inferno: are the 7 layers of data testing hell still relevant?"}, "hash": "31edd295eb4831a8455ffc73b4ababafb7cf62179daddae8a73f60566ef53e16"}}, "hash": "3865be6f606d936e4a5dff44194a254f1d836584f62ceb17384fda88db57dad3", "text": "Return to Data's Inferno: are the 7 layers of data testing hell still relevant?Back in 2018, a blogpost titled \"Data's Inferno: 7 circles of data testing hell with Airflow\" presented a layered approach to data quality checks in data applications and pipelines.Now, 5 years later, this talk looks back at Data's Inferno and surveys what has changed but also what hasn't in the space of ensuring high data quality.5 years ago a blog post called \"Data's Inferno\" (https://medium.com/wbaa/datas-inferno-7-circles-of-data-testing-hell-with-airflow-cef4adff58d8) was written about how to ensure high data quality with Apache Airflow.It suggested using different types of tests as layers to catch issues lurking within the data.These layers included tests for Airflow DAG integrity, mock data pipelines, production data tests, and more.Combining these layers made for a reliable way to filter out incorrect data.Despite the blogpost's age, the ideas are still relevant today.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5d5ade97-5f47-4b27-8f75-35490c5c2fbe": {"__data__": {"id_": "5d5ade97-5f47-4b27-8f75-35490c5c2fbe", "embedding": null, "metadata": {"title": "Return to Data's Inferno: are the 7 layers of data testing hell still relevant?"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31d15b15-0304-58da-91d1-947c6fcb9995", "node_type": null, "metadata": {"title": "Return to Data's Inferno: are the 7 layers of data testing hell still relevant?"}, "hash": "966e707ea325d8a441b3c8b32021b6680a12340b49c674beb9026ad979f6ce22"}, "2": {"node_id": "c4b93c82-f872-4e29-a54f-e7e50b98601f", "node_type": null, "metadata": {"title": "Return to Data's Inferno: are the 7 layers of data testing hell still relevant?"}, "hash": "3865be6f606d936e4a5dff44194a254f1d836584f62ceb17384fda88db57dad3"}}, "hash": "31edd295eb4831a8455ffc73b4ababafb7cf62179daddae8a73f60566ef53e16", "text": "New tools and applications have been developed to help improve data quality as well as new best practices.In this talk, we'll review the layers of Data's Inferno and how they contributed to improving data quality.We'll also look at how new tools address the same concerns.Finally, we'll discuss how we expect and hope the data quality landscape to evolve in the future.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6b9886e0-6c60-421d-a4ca-e636648bd176": {"__data__": {"id_": "6b9886e0-6c60-421d-a4ca-e636648bd176", "embedding": null, "metadata": {"title": "import full-focus as ff \u2013 How to reduce stress and pressure as a data specialist."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bf14af7-ada1-5696-86bb-294140a478eb", "node_type": null, "metadata": {"title": "import full-focus as ff \u2013 How to reduce stress and pressure as a data specialist."}, "hash": "2b3ae107831c629f4bcfcdaaa9eece1c2f18977b1e867a5f68a6822859657137"}, "3": {"node_id": "0e540f0f-63f1-47ac-ba90-e6cff70d04be", "node_type": null, "metadata": {"title": "import full-focus as ff \u2013 How to reduce stress and pressure as a data specialist."}, "hash": "65ce9e12040e63036f3bae120d728a30a7bcbdaeb2b6e1e96cc07ae4dae8b1f6"}}, "hash": "dbe07efd4a052e01b8faf3f60a3204a2017af2bbca311e057e40c115c76cc4d5", "text": "import full-focus as ff \u2013 How to reduce stress and pressure as a data specialist.Data science, IT and software development become more and more complex and are subject to increasing requirements and fast-paced business demand.Higher complexity, higher pace and higher quality requirements result in more pressure on our fellow data engineers and data scientists.More pressure, but are we resilient enough to withstand that increasing pressure?You have probably already seen its outcome.Unhappiness, stress or even burn-outs of co-workers, instead of creating cool code, great solutions and building a better world using your skills.How to change the pressure and stress you perceive as a data scientist, data engineer of ML-engineer?How to ensure that your brain\u2019s frontal lobe returns to a problem solving and decision-making state?**Target audience**\r\nAll experience levels data engineers, data scientists and analysts.For those who start hitting do\u2019s, don\u2019ts and other hard walls in real life companies and projects.Especially if you experience a drain of energy and focus from those pressure and constrains.Senior or junior, there is much to learn and experience.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0e540f0f-63f1-47ac-ba90-e6cff70d04be": {"__data__": {"id_": "0e540f0f-63f1-47ac-ba90-e6cff70d04be", "embedding": null, "metadata": {"title": "import full-focus as ff \u2013 How to reduce stress and pressure as a data specialist."}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bf14af7-ada1-5696-86bb-294140a478eb", "node_type": null, "metadata": {"title": "import full-focus as ff \u2013 How to reduce stress and pressure as a data specialist."}, "hash": "2b3ae107831c629f4bcfcdaaa9eece1c2f18977b1e867a5f68a6822859657137"}, "2": {"node_id": "6b9886e0-6c60-421d-a4ca-e636648bd176", "node_type": null, "metadata": {"title": "import full-focus as ff \u2013 How to reduce stress and pressure as a data specialist."}, "hash": "dbe07efd4a052e01b8faf3f60a3204a2017af2bbca311e057e40c115c76cc4d5"}}, "hash": "65ce9e12040e63036f3bae120d728a30a7bcbdaeb2b6e1e96cc07ae4dae8b1f6", "text": "**Takeaway**\r\nLearn and experience 3 great tools to change your resilience instantly and consistently towards pressure and stress.Not just for yourself, but also be able to see and assist co-workers, family, or other loved ones if they experience stress.**Background knowledge needed**\r\nNone.Just be sure to bring both your head and body to this workshop to experience how quickly these tools work for you.**Time**\r\n\u2022\t 0 \u2013 5 Intro and experience tool #1\r\n\u2022\t 5 \u2013 15 Control your nervous system and work-related stress\r\n\u2022\t15 \u2013 20 experience tool #2\r\n\u2022\t20 \u2013 25 Your stress and social states (based on polyvagal theory)\r\n\u2022\t25 \u2013 30 experience tool #3", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ae913d8e-bd36-4b2f-baa1-57a2d8ff97a1": {"__data__": {"id_": "ae913d8e-bd36-4b2f-baa1-57a2d8ff97a1", "embedding": null, "metadata": {"title": "Keynote Katharine Jarmul"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8e06482-a55b-5837-adc3-f7bc91be7eb0", "node_type": null, "metadata": {"title": "Keynote Katharine Jarmul"}, "hash": "fd2bc27a258976ac222d509d0e6950481e2059a460e78f2a7c8b1efeca80526c"}}, "hash": "fd2bc27a258976ac222d509d0e6950481e2059a460e78f2a7c8b1efeca80526c", "text": "Keynote Katharine Jarmul\n\nLorem ipsum dolor\n\nLorem ipsum dolor", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cb845b37-2146-4f82-894b-670f16762cfc": {"__data__": {"id_": "cb845b37-2146-4f82-894b-670f16762cfc", "embedding": null, "metadata": {"title": "Personalization at Uber scale via causal-driven machine learning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c7480aa-cb34-5f97-bc1a-af7186c3eadb", "node_type": null, "metadata": {"title": "Personalization at Uber scale via causal-driven machine learning"}, "hash": "bfd591d42ad8b7ca4acd4e740db20085c117a8538d111df1c26d9ed957770249"}, "3": {"node_id": "8da78579-8972-49a3-809f-369e4c72e54a", "node_type": null, "metadata": {"title": "Personalization at Uber scale via causal-driven machine learning"}, "hash": "f1b428ede45c1e00d93bcdead819e727d263d8b6984f2469a3d6c26d756e6155"}}, "hash": "ce330e0d8de395515241cf109595c94f6706ff21da4a28f5d2f74bb437ea2d10", "text": "Personalization at Uber scale via causal-driven machine learning\n\nIn this talk, we outline how we introduced causality into our machine learning models within the core checkout and onboarding experiences globally, thereby strongly improving our key business metrics.We discuss case studies, where experimental data were combined with machine learning in order to create value for our users and personalize their experiences, and we share our lessons learned with the goal to inspire attendees to start incorporating causality into their machine learning solutions.Additionally, we explain how the open source Python package developed at Uber, CausalML, can help others in successfully making the transition from correlation-driven machine learning to causal-driven machine learning.In this talk, we outline how we introduced causality into our machine learning models within the core checkout and onboarding experiences globally, thereby strongly improving our key business metrics.We discuss case studies, where experimental data were combined with machine learning in order to create value for our users and personalize their experiences, and we share our lessons learned with the goal to inspire attendees to start incorporating causality into their machine learning solutions.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8da78579-8972-49a3-809f-369e4c72e54a": {"__data__": {"id_": "8da78579-8972-49a3-809f-369e4c72e54a", "embedding": null, "metadata": {"title": "Personalization at Uber scale via causal-driven machine learning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c7480aa-cb34-5f97-bc1a-af7186c3eadb", "node_type": null, "metadata": {"title": "Personalization at Uber scale via causal-driven machine learning"}, "hash": "bfd591d42ad8b7ca4acd4e740db20085c117a8538d111df1c26d9ed957770249"}, "2": {"node_id": "cb845b37-2146-4f82-894b-670f16762cfc", "node_type": null, "metadata": {"title": "Personalization at Uber scale via causal-driven machine learning"}, "hash": "ce330e0d8de395515241cf109595c94f6706ff21da4a28f5d2f74bb437ea2d10"}}, "hash": "f1b428ede45c1e00d93bcdead819e727d263d8b6984f2469a3d6c26d756e6155", "text": "Additionally, we explain how the open source Python package developed at Uber, CausalML, can help others in successfully making the transition from correlation-driven machine learning to causal-driven machine learning.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "058883b1-2f80-4a63-80f4-e9bc83a98dd1": {"__data__": {"id_": "058883b1-2f80-4a63-80f4-e9bc83a98dd1", "embedding": null, "metadata": {"title": "MLOps on the fly: Optimizing a feature store with DuckDB and ArrowFlight"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a325da5-4ccc-5d5d-ab16-858cd62b8c87", "node_type": null, "metadata": {"title": "MLOps on the fly: Optimizing a feature store with DuckDB and ArrowFlight"}, "hash": "1882776d2b3139510aa96ee84dcd2fb180c02c4c7458d3901b3378ccef3b0a01"}, "3": {"node_id": "15085444-2c42-4e6f-bdea-7ae31cb08b08", "node_type": null, "metadata": {"title": "MLOps on the fly: Optimizing a feature store with DuckDB and ArrowFlight"}, "hash": "c6344890bf813fef5bdcaad6372cdb6629e1440d4e6568f557b0bcba836e7c43"}}, "hash": "846629df9680afd610845f2597846d05fadbf3e081a521f174d6273c2e8e0a69", "text": "MLOps on the fly: Optimizing a feature store with DuckDB and ArrowFlight\n\nFeature Stores are a vital part of the MLOps stack for managing machine learning features and ensuring data consistency.This talk introduces Feature Stores and the underlying data management architecture.We\u2019ll then discuss the challenges and learnings of integrating DuckDB and Arrow Flight into the our Feature Store platform, and share benchmarks showing up to 30x speedups compared to Spark/Hive.Discover how DuckDB and ArrowFlight can also speedup your data management and machine learning pipelines.In this talk, we will cover the following topics:\r\n\r\n\u2022\tIntroduction to Machine Learning Feature Stores (5 min): Understanding the role of feature stores in the MLOps stack and their significance in managing machine learning features within organizations.\u2022\tData management architecture behind Feature Stores (2-3 min): Exploring the underlying mechanisms and data management components employed in feature stores.\u2022\tIntroduction to DuckDB and Arrow Flight (5 min): Highlighting the integration of DuckDB and Arrow Flight into the PyData ecosystem, leveraging the capabilities of Arrow.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15085444-2c42-4e6f-bdea-7ae31cb08b08": {"__data__": {"id_": "15085444-2c42-4e6f-bdea-7ae31cb08b08", "embedding": null, "metadata": {"title": "MLOps on the fly: Optimizing a feature store with DuckDB and ArrowFlight"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a325da5-4ccc-5d5d-ab16-858cd62b8c87", "node_type": null, "metadata": {"title": "MLOps on the fly: Optimizing a feature store with DuckDB and ArrowFlight"}, "hash": "1882776d2b3139510aa96ee84dcd2fb180c02c4c7458d3901b3378ccef3b0a01"}, "2": {"node_id": "058883b1-2f80-4a63-80f4-e9bc83a98dd1", "node_type": null, "metadata": {"title": "MLOps on the fly: Optimizing a feature store with DuckDB and ArrowFlight"}, "hash": "846629df9680afd610845f2597846d05fadbf3e081a521f174d6273c2e8e0a69"}}, "hash": "c6344890bf813fef5bdcaad6372cdb6629e1440d4e6568f557b0bcba836e7c43", "text": "\u2022\tThe journey of integrating DuckDB and Arrow Flight into our Feature Store platform (12 min): Sharing our experiences and insights on integrating DuckDB and Arrow Flight into the Hudi-based Lakehouse platform that powers our (offline) feature store, discussing challenges and successes encountered along the way.\u2022\tBenchmarks (5 min): Presenting a benchmark comparing the performance of DuckDB/Arrow Flight vs Spark/HiveServer2, in particular for small to medium sized data.Attendees will gain a deeper understanding of feature stores, insights into the integration of DuckDB and ArrowFlight into the PyData ecosystem, and practical knowledge on enhancing the performance of machine learning pipelines.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3aa39d86-22c4-4215-8263-d4fee2f3a041": {"__data__": {"id_": "3aa39d86-22c4-4215-8263-d4fee2f3a041", "embedding": null, "metadata": {"title": "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8", "node_type": null, "metadata": {"title": "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls"}, "hash": "09d3ccd7fff13f4a78184786a5d02a94ebb301f0e20b6ad873d1d070d4066989"}, "3": {"node_id": "d68f7a36-fc89-44f1-bb4e-41095158b490", "node_type": null, "metadata": {"title": "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls"}, "hash": "c15d2084f0b21e8e4a21b5ded4791bc95fcdc1c399e31eb370cf170423d6d13b"}}, "hash": "c742c9696f5116d28be0742413be8c5802df9b578d6152546874cd13b0dd1b80", "text": "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls\n\nData scientists and analysts are using quasi-experimental methods to make recommendations based on causality instead of randomized control trials.While these methods are easy to use, their assumptions can be complex to explain.This talk will explain these assumptions for data scientists and analysts without in-depth training of causal inference so they can use and explain these methods more confidently to change people's minds using data.Instead of relying solely on randomized control trials (also known as A/B tests), which are considered the gold standard for inferring causality, data scientists and analysts are increasingly turning to quasi-experimental methods to make recommendations based on causality.These methods, including open-source libraries such as CausalImpact (originally an R package but with numerous Python ports), are easy to use, but their assumptions can be complex to explain.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d68f7a36-fc89-44f1-bb4e-41095158b490": {"__data__": {"id_": "d68f7a36-fc89-44f1-bb4e-41095158b490", "embedding": null, "metadata": {"title": "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8", "node_type": null, "metadata": {"title": "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls"}, "hash": "09d3ccd7fff13f4a78184786a5d02a94ebb301f0e20b6ad873d1d070d4066989"}, "2": {"node_id": "3aa39d86-22c4-4215-8263-d4fee2f3a041", "node_type": null, "metadata": {"title": "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls"}, "hash": "c742c9696f5116d28be0742413be8c5802df9b578d6152546874cd13b0dd1b80"}}, "hash": "c15d2084f0b21e8e4a21b5ded4791bc95fcdc1c399e31eb370cf170423d6d13b", "text": "I will break down these assumptions and explain how they can help practitioners determine when to use these methods (and when not to use them), using examples from the world of digital language learning.The key takeaway is that when it comes to changing people's minds using data, explaining assumptions to decision-makers is just as important as understanding the underlying statistics.**Outline**\r\n- Minute 0-5: Introduction and Motivation\r\n- Minute 5-10: Difference-in-Difference / Bayesian Structural Time-Series\r\n- Minute 10-15: Case - Conversion effects of content changes based language-pair specific releases at Babbel\r\n- Minute 15-20: Regression Discontinuity Design\r\n- Minute 20-25: Case - Estimating motivational effects of language assessment\r\n- Minute 25-30: Wrap-up / Take-Aways\r\n\r\nAttendees should have basic knowledge of statistics and causality to get the most out of this talk.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "91be920b-9b95-4166-8430-01297783f4a2": {"__data__": {"id_": "91be920b-9b95-4166-8430-01297783f4a2", "embedding": null, "metadata": {"title": "Ok, doomer"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac0cd2f6-d8ea-5e5f-8cec-57631b7fc9c8", "node_type": null, "metadata": {"title": "Ok, doomer"}, "hash": "dbbf51179a8ba90dcf625fee9932c88d8bad61bf2f510e49c36465cc2828f6f5"}, "3": {"node_id": "0682a5ba-45e7-4d57-9ad4-615a741dd16e", "node_type": null, "metadata": {"title": "Ok, doomer"}, "hash": "3c12fd753b918831ca269d989ba8fade19e33768afacaa2325c31630b83647fc"}}, "hash": "73f0a70835386487b7aa95742387dfa7ac5e0c72fdf68f58d0e4ecd1d89c005c", "text": "Ok, doomer\n\nAI won't end the world, but it can and is making life miserable for plenty of folks.Instead of engaging with the AI overlords, let's explore a pragmatic set of design choices that all Data Scientists and ML devs can implement right now, to reduce the risks of deploying AI systems in the real world.Leave the AI boomers to grumble amongst themselves about x-risk and the singularity.Instead let's focus-in on how we can alleviate the real-world harms happening right now.Too often attempts to identify risks and respond to failure modes of ML and automated systems dive straight into the specifics of model, stack, and implementation.Or worse, add further impenetrable layers of abstraction - the \"more models, more problems\" syndrome.While it's encouraging to see the ecosystem of explainability tools and ML ops surging, as developers and pragmatists we should always prefer the simplest and cheapest tool in our toolkit which is fit for purpose.This talk calls attention to a number of existing simple, cheap and effective levers for flagging and reducing risk that are often overlooked.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0682a5ba-45e7-4d57-9ad4-615a741dd16e": {"__data__": {"id_": "0682a5ba-45e7-4d57-9ad4-615a741dd16e", "embedding": null, "metadata": {"title": "Ok, doomer"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac0cd2f6-d8ea-5e5f-8cec-57631b7fc9c8", "node_type": null, "metadata": {"title": "Ok, doomer"}, "hash": "dbbf51179a8ba90dcf625fee9932c88d8bad61bf2f510e49c36465cc2828f6f5"}, "2": {"node_id": "91be920b-9b95-4166-8430-01297783f4a2", "node_type": null, "metadata": {"title": "Ok, doomer"}, "hash": "73f0a70835386487b7aa95742387dfa7ac5e0c72fdf68f58d0e4ecd1d89c005c"}}, "hash": "3c12fd753b918831ca269d989ba8fade19e33768afacaa2325c31630b83647fc", "text": "These are software design fundamentals like timely and contextual feedback loops, or graceful degradation, that are easily forgotten in the rush to market.These pragmatic tools and product design choices can immediately improve visibility, safety and reduce reputational risk for any team implementing AI.P.S.Better oversight and tooling for our current tech will, by definition, improve our chances of being alerted if an existentially risky intelligence did happen to emerge from the silicon ether, one day.So it's a win win, really.\ud83e\udd37\u200d\u2640\ufe0f", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7af708cb-4979-4dd4-8df9-698f2c47d474": {"__data__": {"id_": "7af708cb-4979-4dd4-8df9-698f2c47d474", "embedding": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d38c367-284b-546b-ab29-f5fb5089392d", "node_type": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "hash": "f9bc479c8f36b2483cbc1dc2006ce06b2b1d73b7679e7e2ca1b33c96bdf30642"}, "3": {"node_id": "3e5eecc9-639a-4822-9f76-199f31e9eeeb", "node_type": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "hash": "b6fc04801d82f94af7cebf9a2ccc8aaec256a270ffd857e86ff811d43171d1a1"}}, "hash": "383ce495df1c57b975c2aa9763e17d61252309228058f21d279f480c2049e773", "text": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List\n\nChatGPT is a fantastic assistant, but it cannot do everything yet.For example, it cannot automatically manage my calendar, update my to-do list, or do anything that requires it to perform actions.However, what would it take to make this reality?I decided to put it to the test by allowing ChatGPT to manage my to-do list for me.During this presentation, I will first introduce you to the concepts of LLM-based agents and how they work.After this introduction, I will demo my agent-based to-do list manager.After this demo, we will dive into the implementation challenges, such as handling hallucinations, parsing actions, etc.I discovered some very clever engineering solutions and tricks to solve these problems, which I will share with you.By the end of the presentation, you will know how LLM-based agents work, how well they work, when they do not, and how to start implementing them yourself.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3e5eecc9-639a-4822-9f76-199f31e9eeeb": {"__data__": {"id_": "3e5eecc9-639a-4822-9f76-199f31e9eeeb", "embedding": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d38c367-284b-546b-ab29-f5fb5089392d", "node_type": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "hash": "f9bc479c8f36b2483cbc1dc2006ce06b2b1d73b7679e7e2ca1b33c96bdf30642"}, "2": {"node_id": "7af708cb-4979-4dd4-8df9-698f2c47d474", "node_type": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "hash": "383ce495df1c57b975c2aa9763e17d61252309228058f21d279f480c2049e773"}, "3": {"node_id": "258cdc1e-10b0-4ee8-9f64-396740ea3025", "node_type": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "hash": "d9fb12eaf11842af9574e8808a9c5e42f75547b36a939b2d25b32a69b0388417"}}, "hash": "b6fc04801d82f94af7cebf9a2ccc8aaec256a270ffd857e86ff811d43171d1a1", "text": "This talk is for people who want to learn how to build their first LLM-based agent.Familiarity with Python, PyDantic, and LMMs is nice during this presentation but not essential.As long as you love overengineered solutions to a basic to-do list, you will like this presentation.ChatGPT is a fantastic assistant, but it cannot do anything that requires it to perform actions.In this talk, we explore how we can solve this issue with LLM-based agents.I will cover topics such as:\r\n- A basic introduction to LLMs and the OpenAI API\r\n- An overview of LLM agents, the REACT framework, and tools.- A demonstration of an agent I built to manage my to-do list.- A discussion on implementation challenges, such as handling hallucinations, parsing actions, adhering to formatting rules, and managing apologetic LLMs.- Ingenious solutions and tricks for overcoming these challenges, like using JSON schema to describe formatting rules, designing effective tools, and how to enable the agent to fix its own mistakes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "258cdc1e-10b0-4ee8-9f64-396740ea3025": {"__data__": {"id_": "258cdc1e-10b0-4ee8-9f64-396740ea3025", "embedding": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d38c367-284b-546b-ab29-f5fb5089392d", "node_type": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "hash": "f9bc479c8f36b2483cbc1dc2006ce06b2b1d73b7679e7e2ca1b33c96bdf30642"}, "2": {"node_id": "3e5eecc9-639a-4822-9f76-199f31e9eeeb", "node_type": null, "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}, "hash": "b6fc04801d82f94af7cebf9a2ccc8aaec256a270ffd857e86ff811d43171d1a1"}}, "hash": "d9fb12eaf11842af9574e8808a9c5e42f75547b36a939b2d25b32a69b0388417", "text": "We'll showcase these solutions through amusing moments and challenges encountered during the development of my to-do list agent.- A summary of what works well and what still needs improvement\r\n\r\nThis talk is designed as an introduction to LLM agents.Throughout the presentation, I will aim to maintain a high-level perspective to ensure that even less technical audience members can grasp the concepts.To achieve this, I will share entertaining situations where my agent did something unexpected and how I resolved those issues.However, the presentation will still be practical enough for technical people to know all the concepts and techniques to develop their LLM agents.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9a27b557-69ae-4008-9ceb-0c3f9367f62a": {"__data__": {"id_": "9a27b557-69ae-4008-9ceb-0c3f9367f62a", "embedding": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "851e5c4780ccfa93c1772e2b2b7737ce5f0c3c3bf7adee866d9d6174018a4ec9"}, "3": {"node_id": "6576c8f8-422f-4a55-a149-b4e1d7b6f9d1", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "ab548d772e086766fd1cbdc6f216417c648a8eb926f39f50c1d96bf16d9a6024"}}, "hash": "78d9617dfbf0e0906cffad16944a3617d41b497806386c6ca4812a2cdd295fcb", "text": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data\n\nThe continued success of large language models (LLMs) hinges upon accurate, diverse, and well-labeled data.However, getting this data with the right context provides another set of challenges.This talk will explore how to facilitate a data-centric workflow for LLM development.We\u2019ll cover the importance of data annotation, the process of developing and curating your data pipeline, a four-step workflow to generate your own fine-tuned LLM, and the pros and cons of this approach.Lastly, we\u2019ll cover the importance of continuous learning and how to future-proof your newly generated fine-tuned LLM, emphasizing speed and observability.If you\u2019re curious about the next wave of generative AI and how to make the most of it through well-curated, context-specific data, this talk is for you.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6576c8f8-422f-4a55-a149-b4e1d7b6f9d1": {"__data__": {"id_": "6576c8f8-422f-4a55-a149-b4e1d7b6f9d1", "embedding": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "851e5c4780ccfa93c1772e2b2b7737ce5f0c3c3bf7adee866d9d6174018a4ec9"}, "2": {"node_id": "9a27b557-69ae-4008-9ceb-0c3f9367f62a", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "78d9617dfbf0e0906cffad16944a3617d41b497806386c6ca4812a2cdd295fcb"}, "3": {"node_id": "9113c3dd-6cb1-4bb9-8068-7ad0bfe785bb", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "0cae1bb46173748440b918999243cfb09bd59df8eb7cb4c967b8c059feed7b03"}}, "hash": "ab548d772e086766fd1cbdc6f216417c648a8eb926f39f50c1d96bf16d9a6024", "text": "In this presentation, machine learning enthusiasts, data scientists, and the generative AI curious will explore how to leverage the latest advancements in large language models (LLMs) for context-specific use cases.With all of the recent developments and rapid advancements around generative AI, it\u2019s important to separate the signal from the noise by demonstrating the value that these generative models can bring, especially when they\u2019re fine-tuned to specific parameters or datasets with specific use cases.This talk is an informative, guided exploration of not only why it\u2019s important to build a data-centric workflow for LLMs, but how exactly to do so and the multitude of tools that are out there that make this more accessible than ever.Featuring seasoned experts in the field of open source machine learning and data science, this talk will not only explain how it works but inspire curiosity in a hands-on manner, leaving attendees with open-source resources to test out this workflow in an accessible manner (through a CoLab notebook to be exact).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9113c3dd-6cb1-4bb9-8068-7ad0bfe785bb": {"__data__": {"id_": "9113c3dd-6cb1-4bb9-8068-7ad0bfe785bb", "embedding": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "851e5c4780ccfa93c1772e2b2b7737ce5f0c3c3bf7adee866d9d6174018a4ec9"}, "2": {"node_id": "6576c8f8-422f-4a55-a149-b4e1d7b6f9d1", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "ab548d772e086766fd1cbdc6f216417c648a8eb926f39f50c1d96bf16d9a6024"}, "3": {"node_id": "150168c1-0721-4cd9-b7a8-69f188bf9391", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "fdf7b9e4dfdf7d887b2236145622decfa14bd4debb9a56570a2582e91806b8fa"}}, "hash": "0cae1bb46173748440b918999243cfb09bd59df8eb7cb4c967b8c059feed7b03", "text": "We\u2019ll set the stage by providing context; sharing why a data-centric approach is essential when fine-tuning and finessing LLMs.Next, we\u2019ll share what a data-centric workflow looks like and the tools and resources used to develop it.Once the stage has been set, we\u2019ll start to cover how to begin the fine-tuning process.Attendees will be walked through a guided tour of how exactly to fine-tune LLMs with finesse through well-curated training data.We\u2019ll start by breaking down the four stages of learning, where each stage strategically and mindfully injects human signals, converging the model to perform to the specific use case provided.In each of these steps, we\u2019ll center the conversation around the tools and tactics used and the importance of context and curation when it comes to the datasets employed.We\u2019ll also break down the Parameter-Efficient FineTuning (PEFT) approach, which makes this work executable on such large foundational models.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "150168c1-0721-4cd9-b7a8-69f188bf9391": {"__data__": {"id_": "150168c1-0721-4cd9-b7a8-69f188bf9391", "embedding": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "851e5c4780ccfa93c1772e2b2b7737ce5f0c3c3bf7adee866d9d6174018a4ec9"}, "2": {"node_id": "9113c3dd-6cb1-4bb9-8068-7ad0bfe785bb", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "0cae1bb46173748440b918999243cfb09bd59df8eb7cb4c967b8c059feed7b03"}, "3": {"node_id": "083b9719-ba34-408d-af6f-7a9af11d0a87", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "e6a7719bfd7f1edc262f15043dc5717f945bd876e82cbfd82c8be1abf7c7fb4d"}}, "hash": "fdf7b9e4dfdf7d887b2236145622decfa14bd4debb9a56570a2582e91806b8fa", "text": "As the training walkthrough ends, we\u2019ll wrap up the conversation and demonstration by sharing just how to improve efficiency for the long haul by highlighting the importance of continuous learning.We\u2019ll offer a brief walkthrough of how continuous model fine-tuning works and how to make the most of it for the long haul.This includes a quick discussion on how to enable continuous model fine-tuning with an emphasis on speed and observability.The talk will conclude with links and resources to inspire further exploration and curiosity.Attendees will leave with a deeper understanding of how exactly to leverage these tools and materials, and resources to make the most of them.This includes a notebook with a guided tutorial and demonstration and additional links and references used to develop this workflow.Our goal is to ensure attendees are informed and empowered with the skills and resources to make the most of their learning after this demonstration.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "083b9719-ba34-408d-af6f-7a9af11d0a87": {"__data__": {"id_": "083b9719-ba34-408d-af6f-7a9af11d0a87", "embedding": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "851e5c4780ccfa93c1772e2b2b7737ce5f0c3c3bf7adee866d9d6174018a4ec9"}, "2": {"node_id": "150168c1-0721-4cd9-b7a8-69f188bf9391", "node_type": null, "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}, "hash": "fdf7b9e4dfdf7d887b2236145622decfa14bd4debb9a56570a2582e91806b8fa"}}, "hash": "e6a7719bfd7f1edc262f15043dc5717f945bd876e82cbfd82c8be1abf7c7fb4d", "text": "Outline of Time: 30 minute talk\r\n\r\n12 minutes: \r\n- Introduction of topic and technology\r\n- Introduction of the problems that this solves in the industry\r\n- Importance of data-centric model retraining\r\n- Tools and techniques used\r\n - Theoretical walkthrough \r\n10 minutes: \r\n- Technical demonstration and walkthrough\r\n5 minutes: \r\n- Factors to consider when fine-tuning LLMs\r\n- Continuous learning and deployment\r\n3 minutes: \r\n- Wrap up and additional time for buffer\r\n- Share where to find resources and references to continue curiosity.Core takeaways: \r\nWhy does a datacentric workflow for fine-tuning LLMs matter?What goes into building a datacentric workflow for fine-tuning foundational models?How to build and maintain a datacentric workflow for foundational model retraining.What core steps are important when fine-tuning foundational models?Why is context and human signal important to consider when fine-tuning LLMs?How to build a sustainable, continuous learning workflow.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "76f33570-5547-499c-be8c-59038873bb3e": {"__data__": {"id_": "76f33570-5547-499c-be8c-59038873bb3e", "embedding": null, "metadata": {"title": "Keynote Thomas Wolf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63aa42ea-2fc6-5bfb-92ac-11e83a7f45f8", "node_type": null, "metadata": {"title": "Keynote Thomas Wolf"}, "hash": "1506fb1d0688060897b51bc9c2ec92df5f3a16be837cab27919d1b97feb6e003"}}, "hash": "1506fb1d0688060897b51bc9c2ec92df5f3a16be837cab27919d1b97feb6e003", "text": "Keynote Thomas Wolf\n\nKeynote by Thomas Wolf. He will be accompanied on stage by Alessandro Cappelli, Julien Launay & Guilherme Penedo, all members of the Hugging Face team in Amsterdam working on large model training.\n\nLorum ipsum dolor", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26c19831-c98f-4435-b165-c82427c12766": {"__data__": {"id_": "26c19831-c98f-4435-b165-c82427c12766", "embedding": null, "metadata": {"title": "Unlocking the Black Box: A practical guide to finding an alibi for machine learning models"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9ad798a-cb3b-5209-9d45-7cef2617937f", "node_type": null, "metadata": {"title": "Unlocking the Black Box: A practical guide to finding an alibi for machine learning models"}, "hash": "4feaed964b69829a3dd81d699f608a2a783d492e7a34fd1966150eacfb1b1322"}, "3": {"node_id": "b9440585-20c6-4ebf-a222-c95195f805bd", "node_type": null, "metadata": {"title": "Unlocking the Black Box: A practical guide to finding an alibi for machine learning models"}, "hash": "b23961b4f0d2893635c392f00ab75f95d429cd44eff3cbe6733d953f05b77e65"}}, "hash": "e7b6ea5d96348335bcbaaaed598edcc603afa0ca0f54db729afbb358d8e209c0", "text": "Unlocking the Black Box: A practical guide to finding an alibi for machine learning models\n\nKnowledge work is undergoing a transformative journey with machine learning (ML) but the interpretability of the models we interact with is still lagging behind the coolness and hype of the technologies using ML.This workshop seeks to address the gap between the speed at which we use and adopt ML and the pace at which we understand it.During the workshop, we will cover fundamental concepts and techniques of interpretable machine learning, and explore various explainability methods supported by the Alibi Explain library so that you can get started explaining your models.If you've been meaning to dive deeper into the field of interpretable ML, add interpretability to your workflows, find an alibi for your models, or are simply curious about the field, come and join us for a fun 90-minute interactive session on interpretable ML.In the era of complex and powerful machine learning (ML) models, understanding the decision-making process of these models has become a challenge, and different interpretability methods can help us build trust, address bias, and ensure compliance with different standards.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b9440585-20c6-4ebf-a222-c95195f805bd": {"__data__": {"id_": "b9440585-20c6-4ebf-a222-c95195f805bd", "embedding": null, "metadata": {"title": "Unlocking the Black Box: A practical guide to finding an alibi for machine learning models"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9ad798a-cb3b-5209-9d45-7cef2617937f", "node_type": null, "metadata": {"title": "Unlocking the Black Box: A practical guide to finding an alibi for machine learning models"}, "hash": "4feaed964b69829a3dd81d699f608a2a783d492e7a34fd1966150eacfb1b1322"}, "2": {"node_id": "26c19831-c98f-4435-b165-c82427c12766", "node_type": null, "metadata": {"title": "Unlocking the Black Box: A practical guide to finding an alibi for machine learning models"}, "hash": "e7b6ea5d96348335bcbaaaed598edcc603afa0ca0f54db729afbb358d8e209c0"}}, "hash": "b23961b4f0d2893635c392f00ab75f95d429cd44eff3cbe6733d953f05b77e65", "text": "The Alibi Explain library offers a comprehensive toolkit for interpreting machine learning models and shedding light on their inner workings.No prior experience with the library is required, but some knowledge of machine learning is expected.During the workshop, we will cover the fundamental concepts and techniques of interpretable machine learning, exploring various explainability methods supported by Alibi Explain.We will discuss strategies such as rule-based explanations, feature importance, and counterfactual explanations.Through hands-on exercises, participants will gain practical experience in interpreting models and understanding their predictions.Throughout the workshop, we will emphasize real-world applications and use cases to demonstrate the relevance and importance of interpretable machine learning.We will discuss how interpretability can enable better decision-making in finance, healthcare, and retail domains.By the end of the workshop, attendees will have a good understanding of interpretable machine learning concepts and practical skills for finding an alibi for their ML models.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "daecb07c-9247-4bca-9dc8-89cfc0250d93": {"__data__": {"id_": "daecb07c-9247-4bca-9dc8-89cfc0250d93", "embedding": null, "metadata": {"title": "Building a personal search engine with llama-index"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f71f1eae-c12e-5b4f-8902-b8d0b8a38d72", "node_type": null, "metadata": {"title": "Building a personal search engine with llama-index"}, "hash": "3048db44e02fa581744056cb7a31a225dfdd5eb0ef6d99ecc4c82dc85e0e2872"}, "3": {"node_id": "e6dd1229-39dd-40db-b8cf-56d1c0bd49b1", "node_type": null, "metadata": {"title": "Building a personal search engine with llama-index"}, "hash": "e6b09c77ade2a5bb7b6c5afc3c9851bc0689fa2625cf85be8831fcc85a0e85ec"}}, "hash": "18dddb7f30eb0d9e79afef5ea3a3fd74a04a8353cbc332d0f5009af31813fdab", "text": "Building a personal search engine with llama-index\n\nWouldn\u2019t it be great to have a Google-like search engine, but then for your own text files and completely private?In this tutorial we\u2019ll build a small personal search engine using open source library llama-index.In this tutorial we will build a small personal search engine using open source library `llama-index`.Llama-index provides utility functions for ingesting various kinds of data, breaking the data up in chunks, building an index of that data using vector embeddings, and retrieving data from the index based on queries.We can even use llama-index to post-process the retrieval results for us using large language models such as GPT.The target audience is people that are already familiar with Python.Participants will experience working with unstructured data, vector embeddings, and explore the possibilities of the recent developments in natural language processing.Workshop materials will be provided via Github before the start of the workshop.For the demo application, we will only use open-source software and models that are light enough to run on an average laptop without a GPU.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e6dd1229-39dd-40db-b8cf-56d1c0bd49b1": {"__data__": {"id_": "e6dd1229-39dd-40db-b8cf-56d1c0bd49b1", "embedding": null, "metadata": {"title": "Building a personal search engine with llama-index"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f71f1eae-c12e-5b4f-8902-b8d0b8a38d72", "node_type": null, "metadata": {"title": "Building a personal search engine with llama-index"}, "hash": "3048db44e02fa581744056cb7a31a225dfdd5eb0ef6d99ecc4c82dc85e0e2872"}, "2": {"node_id": "daecb07c-9247-4bca-9dc8-89cfc0250d93", "node_type": null, "metadata": {"title": "Building a personal search engine with llama-index"}, "hash": "18dddb7f30eb0d9e79afef5ea3a3fd74a04a8353cbc332d0f5009af31813fdab"}}, "hash": "e6b09c77ade2a5bb7b6c5afc3c9851bc0689fa2625cf85be8831fcc85a0e85ec", "text": "Using llama-index with OpenAI\u2019s API is optional.If you want to explore postprocessing your results with GPT-3.5, we recommend registering an OpenAI account and making sure you have your OpenAI API key ready.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f1ed581d-81a0-4248-86c1-ce82e7d01cd7": {"__data__": {"id_": "f1ed581d-81a0-4248-86c1-ce82e7d01cd7", "embedding": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66510cf5-c4a9-5cf2-94c1-863495919590", "node_type": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "hash": "ec05d58f529fb77bb8afd6850243f28b08273477109c49defebfe58d9640619f"}, "3": {"node_id": "9b3c6264-77b5-41d4-838b-c1d110270fd2", "node_type": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "hash": "e25254af0bca1f4b89cd5c0f72542ffe1b4a117c11cc9821a572cff41834b7d0"}}, "hash": "37e3937f38f3fcb44d5ab8cbdefdcc6d2784a9b1f96d8e4c4e31094c7cd17705", "text": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial\n\nThis hands-on tutorial introduces participants to knowledge graph modeling using Neo4j, a popular graph database.Suitable for beginners and those seeking to enhance their knowledge, the tutorial will help attendees to learn the fundamentals of knowledge graphs, gain insights into Neo4j's modeling capabilities, and acquire practical skills in designing effective knowledge graph models.In this tutorial, we'll explore knowledge graphs and their modeling using Neo4j, a popular graph database.Participants will learn effective modeling techniques and how to leverage this technology for their own projects.The tutorial is for data professionals, data scientists, software engineers, and anyone interested in knowledge graphs.No prior experience with Neo4j or knowledge graph modeling is required, making it suitable for beginners and those looking to expand their knowledge.The tutorial will be interactive, combining theory and practical exercises to provide a comprehensive understanding of knowledge graph modeling.The tone will be informative and engaging, encouraging active learning and collaboration.By the end of this tutorial, participants will:\r\n\r\n1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9b3c6264-77b5-41d4-838b-c1d110270fd2": {"__data__": {"id_": "9b3c6264-77b5-41d4-838b-c1d110270fd2", "embedding": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66510cf5-c4a9-5cf2-94c1-863495919590", "node_type": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "hash": "ec05d58f529fb77bb8afd6850243f28b08273477109c49defebfe58d9640619f"}, "2": {"node_id": "f1ed581d-81a0-4248-86c1-ce82e7d01cd7", "node_type": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "hash": "37e3937f38f3fcb44d5ab8cbdefdcc6d2784a9b1f96d8e4c4e31094c7cd17705"}, "3": {"node_id": "9c6d6801-bd7c-4447-9877-493db9fe92da", "node_type": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "hash": "6a53f1987313a750b4f7abb7c4c92f41d3d284f6fe9e40da520fde08cf6b00a7"}}, "hash": "e25254af0bca1f4b89cd5c0f72542ffe1b4a117c11cc9821a572cff41834b7d0", "text": "Understand the concept of knowledge graphs and their significance in representing complex relationships and interconnected data.2.Gain familiarity with Neo4j and its capabilities for knowledge graph modeling.3.Acquire knowledge of representation patterns, best practices, pitfalls to avoid, and trade-offs to consider when modeling knowledge graphs.4.Develop practical skills through hands-on exercises and examples, enabling them to apply knowledge graph modeling techniques to their own projects.Tentative schedule:\r\n\r\n1.Introduction to Knowledge Graphs (20 minutes)\r\n   - Definition and real-world applications of knowledge graphs\r\n   - General and common knowledge graph elements\r\n\r\n2.Introduction to Neo4j (15 minutes)\r\n   - Overview of Neo4j as a graph database\r\n   - Understanding Neo4J\u2019s graph data model and the Cypher language\r\n\r\n3.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9c6d6801-bd7c-4447-9877-493db9fe92da": {"__data__": {"id_": "9c6d6801-bd7c-4447-9877-493db9fe92da", "embedding": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66510cf5-c4a9-5cf2-94c1-863495919590", "node_type": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "hash": "ec05d58f529fb77bb8afd6850243f28b08273477109c49defebfe58d9640619f"}, "2": {"node_id": "9b3c6264-77b5-41d4-838b-c1d110270fd2", "node_type": null, "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}, "hash": "e25254af0bca1f4b89cd5c0f72542ffe1b4a117c11cc9821a572cff41834b7d0"}}, "hash": "6a53f1987313a750b4f7abb7c4c92f41d3d284f6fe9e40da520fde08cf6b00a7", "text": "Representing knowledge graphs in Neo4J (20 minutes)\r\n   - Understanding different patterns for modeling entities, relations and other knowledge graph elements\r\n   - Choosing the appropriate pattern based on data and application requirements\r\n   - Avoiding common mistakes and pitfalls\r\n   - Making informed decisions about common dilemmas and trade-offs\r\n\r\n4.Hands-on Exercise (20 minutes)\r\n   - Guided exercise to model a knowledge graph using Neo4j\r\n\r\n5.Q&A and Discussion (10 minutes)\r\n   - Addressing participant questions and engaging in interactive discussion\r\n\r\n6.Wrap-up and Conclusion (5 minutes)\r\n   - Recap of key concepts and takeaways from the tutorial\r\n   - Suggestions for further learning and resources\r\n\r\nAll materials, including code examples, exercises, and supplementary resources, will be made available through a dedicated GitHub repository.Participants can access and download these materials to continue their learning journey beyond the tutorial session.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f7736f86-c42f-42d4-89b1-22e85500bac8": {"__data__": {"id_": "f7736f86-c42f-42d4-89b1-22e85500bac8", "embedding": null, "metadata": {"title": "Unconference #1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b32bca8-108d-5c64-9dd4-d577b0d284f6", "node_type": null, "metadata": {"title": "Unconference #1"}, "hash": "e439270c239d60a2e04fe706d08a5fd5758a10f6ed982ffef726d16b243d0064"}}, "hash": "e439270c239d60a2e04fe706d08a5fd5758a10f6ed982ffef726d16b243d0064", "text": "Unconference #1\n\nLorem ipsum dolor\n\n1143", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ffec7b14-342c-4e0e-9fa4-ec7a01a69c1b": {"__data__": {"id_": "ffec7b14-342c-4e0e-9fa4-ec7a01a69c1b", "embedding": null, "metadata": {"title": "Unconference #4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89243294-cdfa-57b6-a06f-a150fc1b56b7", "node_type": null, "metadata": {"title": "Unconference #4"}, "hash": "e9a99ca852ed4fb373eec195989ce75498d899d38221523e10eb1a7a4ba3a0aa"}}, "hash": "e9a99ca852ed4fb373eec195989ce75498d899d38221523e10eb1a7a4ba3a0aa", "text": "Unconference #4\n\nLorem ipsum dolor\n\nLorem ipsum dolor", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7d37103-da06-446a-b7f8-1edf72487406": {"__data__": {"id_": "c7d37103-da06-446a-b7f8-1edf72487406", "embedding": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "791c9c2c-69d2-5f10-bf2f-707ed3992b42", "node_type": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "hash": "f4ccd5adbf55da98b6cac86faa000fd414a322048f1f5fc2226fd8db4c67c1c2"}, "3": {"node_id": "91b88d61-146b-414e-9d3d-b6fffac3a756", "node_type": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "hash": "acc9894eb3764fe107fd3db0c91147d7848da88dc51f962cdb833a36d3fefba5"}}, "hash": "286e965dfbed0635f1b3ecb21bd79d098c1b5cebcb392ad9f70f9033bc75f8c0", "text": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks\n\nThis talk explores distillation learning, a powerful technique for compressing and transferring knowledge from larger neural networks to smaller, more efficient ones.It delves into its core components and various applications such as model compression and transfer learning.The speaker aims to simplify the topic for all audiences and provides implementation, demonstrating how to apply distillation learning in real scenarios.Attendees will gain insights into developing efficient neural networks by reviewing the various examples of the complex model.The material will be accessible online for convenient access and understanding.As the field of artificial intelligence continues to advance, the demand for more efficient and compact neural network models has become increasingly vital.The ability to compress and transfer knowledge from larger, complex models to smaller, more efficient models has emerged as a powerful solution.In this talk, we aim to shed light on the significance of distillation learning and its applications across various domains.In an era where data sizes and computational requirements are escalating, distillation learning provides a compelling solution to address the challenges posed by these factors.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "91b88d61-146b-414e-9d3d-b6fffac3a756": {"__data__": {"id_": "91b88d61-146b-414e-9d3d-b6fffac3a756", "embedding": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "791c9c2c-69d2-5f10-bf2f-707ed3992b42", "node_type": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "hash": "f4ccd5adbf55da98b6cac86faa000fd414a322048f1f5fc2226fd8db4c67c1c2"}, "2": {"node_id": "c7d37103-da06-446a-b7f8-1edf72487406", "node_type": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "hash": "286e965dfbed0635f1b3ecb21bd79d098c1b5cebcb392ad9f70f9033bc75f8c0"}, "3": {"node_id": "1f14fe1f-3196-4465-92ce-ba6252be10a4", "node_type": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "hash": "549c4f2d7dee353a3cd983fb576729e6297a465630fcaddb5b29a141f8236948"}}, "hash": "acc9894eb3764fe107fd3db0c91147d7848da88dc51f962cdb833a36d3fefba5", "text": "By utilizing a teacher-student framework, this approach facilitates the transfer of knowledge from a larger, well-performing teacher model to a smaller student model.The student model is trained to mimic the behaviour and output of the teacher model, thereby inheriting its expertise.This process enables the creation of compact models that are not only efficient in terms of memory and inference speed but also capable of performing tasks with comparable proficiency.Distillation learning represents a breakthrough in model compression and transfer learning, revolutionizing the field of artificial intelligence and novel machine learning utilising deep neural networks.In this talk, we will provide a comprehensive overview of distillation learning, covering its core components.We will explore the definition and motivation behind, highlighting the role of the teacher model in guiding the student model and the objective of the student model to replicate the teacher model's output.Additionally, we will discuss the diverse applications, including model compression, transfer learning, ensemble learning, multi-task learning, and language models.We will also delve into different types of this learning approach, such as model distillation, knowledge distillation, multi-task distillation, and transfer distillation.This talk facilitates knowledge exchange and inspires the development of efficient neural networks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1f14fe1f-3196-4465-92ce-ba6252be10a4": {"__data__": {"id_": "1f14fe1f-3196-4465-92ce-ba6252be10a4", "embedding": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "791c9c2c-69d2-5f10-bf2f-707ed3992b42", "node_type": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "hash": "f4ccd5adbf55da98b6cac86faa000fd414a322048f1f5fc2226fd8db4c67c1c2"}, "2": {"node_id": "91b88d61-146b-414e-9d3d-b6fffac3a756", "node_type": null, "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}, "hash": "acc9894eb3764fe107fd3db0c91147d7848da88dc51f962cdb833a36d3fefba5"}}, "hash": "549c4f2d7dee353a3cd983fb576729e6297a465630fcaddb5b29a141f8236948", "text": "The speaker simplifies the topic, making it accessible to all audiences.Simple practical implementation in TensorFlow will be demonstrated, showcasing how attendees can apply this technique in real scenarios.No expertise in complex models is required, and the material will be shared online for convenient access and comprehension.Highlighted References:\r\n- Mirzadeh, S., Farajtabar, M., Liang, D., & Ghasemzadeh, H. (2020).Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.- Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., & Doll\u00e1r, P. (2020).Designing network design spaces.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.- Malik, Shaiq Munir, et al.(2021) Teacher-Class Network: A Neural Network Compression Mechanism.In Proceedings of the British Machine Vision Conference.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3562a775-839c-40c1-b4e4-44a0b945b921": {"__data__": {"id_": "3562a775-839c-40c1-b4e4-44a0b945b921", "embedding": null, "metadata": {"title": "Data Contracts in action powered by Python open source ecosystem"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6fbbf46-8263-5421-80ae-34972cf15788", "node_type": null, "metadata": {"title": "Data Contracts in action powered by Python open source ecosystem"}, "hash": "72c4e3bb0d31780ae4f782e51606c8e65ea87974038cd091e312d58e9875d3e5"}}, "hash": "72c4e3bb0d31780ae4f782e51606c8e65ea87974038cd091e312d58e9875d3e5", "text": "Data Contracts in action powered by Python open source ecosystem\n\nThis informative talk aims to close the gap between the theory of data contracts and their real-life implementations. It contains a few Python code snippets and is aimed primarily at data and software engineers. However, it could be food for thought for machine learning engineers, data scientists, and other data consumers.\n\nTopic: There are a lot of ongoing discussions happening about data contracts. I would like to share with you some lessons learned from data contract implementations and show you some Python examples.\r\n\r\nAudience: data and software engineers; potentially could be interesting for machine learning engineers, data scientists, and other data consumers. Some affinity with Pandas, Great Expectations, and Open Table Formats are desirable. \r\n\r\nType: Informative with some hands-on examples\r\n\r\nMain takeaways:\r\n- better understanding of the data contracts concept \r\n- tips for batch data contracts implementations\r\n- tips for streaming data contracts implementations", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c0e2ce9f-fb13-49b4-a0cf-3cc7f0f90d71": {"__data__": {"id_": "c0e2ce9f-fb13-49b4-a0cf-3cc7f0f90d71", "embedding": null, "metadata": {"title": "Production Data to the Model: \u201cAre You Getting My Drift?\u201d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b7e549a-0c06-502d-ad0f-ec3ed74ca80f", "node_type": null, "metadata": {"title": "Production Data to the Model: \u201cAre You Getting My Drift?\u201d"}, "hash": "e97a4bf15e314855e9a212a05921b3efcdf5cdfadff15131a08295cd85f4cf8c"}, "3": {"node_id": "e4a70cd9-08aa-4028-8809-d84bef981628", "node_type": null, "metadata": {"title": "Production Data to the Model: \u201cAre You Getting My Drift?\u201d"}, "hash": "f423b4a7024b8f284a5a508f4cd24838af1746a14f7b863eff1daec36370a146"}}, "hash": "28812916347a8b72c1d045d82b92c055180ebe7fee6dafe67e066db4e050d102", "text": "Production Data to the Model: \u201cAre You Getting My Drift?\u201d\n\nA shift is a poetic word for uncertainty.Winds shift, rivers and sands drift, and people change.Coming to the not-so-poetic world of data science, what about the data?Data comes from systems and people using them, so it is natural that data too will see the rigors of shift too.A model that was trained and tested for particular dynamics may assume the expected uncertainty in the data such as a shift in the user behavior.But what happens when the shift goes beyond expectations?How do teams detect the different types of data drift?More so, how do they tackle the detected drift?In this talk, I would gently introduce you to data drift and how the industry tackles this issue.<i>\u201cEverything you see has its roots in the unseen world.The forms may change, yet the essence remains the same.\u201d</i>\r\n<p>When Rumi wrote this quote, machine learning was not even an idea.Therefore we can safely assume that these soothing words do not apply to the probability distribution of predictors and targets.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e4a70cd9-08aa-4028-8809-d84bef981628": {"__data__": {"id_": "e4a70cd9-08aa-4028-8809-d84bef981628", "embedding": null, "metadata": {"title": "Production Data to the Model: \u201cAre You Getting My Drift?\u201d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b7e549a-0c06-502d-ad0f-ec3ed74ca80f", "node_type": null, "metadata": {"title": "Production Data to the Model: \u201cAre You Getting My Drift?\u201d"}, "hash": "e97a4bf15e314855e9a212a05921b3efcdf5cdfadff15131a08295cd85f4cf8c"}, "2": {"node_id": "c0e2ce9f-fb13-49b4-a0cf-3cc7f0f90d71", "node_type": null, "metadata": {"title": "Production Data to the Model: \u201cAre You Getting My Drift?\u201d"}, "hash": "28812916347a8b72c1d045d82b92c055180ebe7fee6dafe67e066db4e050d102"}}, "hash": "f423b4a7024b8f284a5a508f4cd24838af1746a14f7b863eff1daec36370a146", "text": "The said distribution can change from something trivial like a change of collection metric system to something as disruptive as a pandemic.Keeping track of data drift has become an essential part of industrializing the machine-learning process.A simple mix of understanding the kind of data the model would encounter, mathematics, and a suitable detection strategy can help teams watch out for model performance decay.Additionally, it is important for data science practitioners to understand the types of drifts to devise the best detection strategy.My talk will focus on the following:\r\nIntroduction to data drift and the cost of ignoring it\r\nTypes of data drift \r\nCommonly-used tests to detect drift in numerical and categorical data\r\nA short Python-based walkthrough of detection methods\r\nHow is drift detected for unstructured data like text?Drift happened and we caught it, now what?</p>\r\n\r\nThe intended target audience is broad since anyone who has deployed or wishes to deploy their model needs to be aware of this issue.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c73a06fc-6db4-483d-b491-826f7b88bd91": {"__data__": {"id_": "c73a06fc-6db4-483d-b491-826f7b88bd91", "embedding": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56b64bc2-19f4-55f7-bed5-fbc4a5c50425", "node_type": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "hash": "751d8f430469b8122aa659594f018a80a32e9aad2727eae7527c45b9ba24337c"}, "3": {"node_id": "41910b9a-478c-44eb-879c-b12a75e9c0ae", "node_type": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "hash": "1eec878a3af793985aab255ab5ed42b404ff50545bcd9cc2f032838a8d703708"}}, "hash": "39d905b3341d95224b186448b2d73982df7c4e1ca43c84330ae1bb51eeddf8d0", "text": "Lets do the time warp again: time series machine learning with distance functions\n\nMany algorithms for machine learning from time series are based on measuring the distance or similarity between series.The most popular distance measure is dynamic time warping, which attempts to optimally realign two series to compensate for offest.There are many others though.We present an overview of the most popular time series specific distance functions and describe their speed optimised implementations in aeon, a scikit-learn compatible time series machine learning toolkit.We demonstrate their application for clustering, classification and regression on a real world case study and highlight some of the latest distance based time series machine learning tools available in aeon.This talk introduces you to popular time series distance functions and demonstrates their usage in exploratory and predictive modelling of time series.Participants will come away with an idea of how to use the very latest research into time series distances for clustering, classification and regression using the aeon toolkit and scikit learn.The talk will be mostly practical and code based, with some algorithmic and mathematical notation.Distances are used in all forms of time series machine learning.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "41910b9a-478c-44eb-879c-b12a75e9c0ae": {"__data__": {"id_": "41910b9a-478c-44eb-879c-b12a75e9c0ae", "embedding": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56b64bc2-19f4-55f7-bed5-fbc4a5c50425", "node_type": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "hash": "751d8f430469b8122aa659594f018a80a32e9aad2727eae7527c45b9ba24337c"}, "2": {"node_id": "c73a06fc-6db4-483d-b491-826f7b88bd91", "node_type": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "hash": "39d905b3341d95224b186448b2d73982df7c4e1ca43c84330ae1bb51eeddf8d0"}, "3": {"node_id": "cc33d7ce-ed84-4b88-b3ca-fe565545de34", "node_type": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "hash": "b4e8a641314a55e6de901225deda91c6387a1abaf0edda12ffc3491496058419"}}, "hash": "1eec878a3af793985aab255ab5ed42b404ff50545bcd9cc2f032838a8d703708", "text": "They can help explore collections of time series through clustering, reduce dimensionality by averaging and be used with instance based or kernel based classifiers and regressors.They are used in streaming based anomaly detection and change point detection and have been embedded within tree based ensembles for classification.The basic problem in specifying a distance function is to quantify how dissimilar two series are.Elastic distances attempt to compensate for small mis-alignments caused by offset that would make similar series look very different to measures such as Euclidean distance or correlation.There have been many different algorithms that combine forms of time warping (stretching the indexes to realign series) and editing (removing time points from one of the series to improve alignment).In the first part of the talk we will provide a high level overview and visualisation of the differences between these algorithms before describing the aeon toolkit, which contains the most comprehensive and fastest library of elastic distances that we are aware of.aeon distances can be used directly with sklearn distance based algorithms and with the many time series specific algorithms for classification, clustering and regression available in aeon.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cc33d7ce-ed84-4b88-b3ca-fe565545de34": {"__data__": {"id_": "cc33d7ce-ed84-4b88-b3ca-fe565545de34", "embedding": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56b64bc2-19f4-55f7-bed5-fbc4a5c50425", "node_type": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "hash": "751d8f430469b8122aa659594f018a80a32e9aad2727eae7527c45b9ba24337c"}, "2": {"node_id": "41910b9a-478c-44eb-879c-b12a75e9c0ae", "node_type": null, "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}, "hash": "1eec878a3af793985aab255ab5ed42b404ff50545bcd9cc2f032838a8d703708"}}, "hash": "b4e8a641314a55e6de901225deda91c6387a1abaf0edda12ffc3491496058419", "text": "In the the middle section of the tutorial we will use a real world industrial dataset to demonstrate use cases in clustering, classification and regression.We will end with some pointers to the very latest research into using distance functions.We will require attendees to have a basic knowledge of scikit-learn and standard machine learning algorithms.This should appeal to anyone interested in machine learning from time series.It will focus on practical application and algorithm comprehension rather than maths, and will identify the very latest research into algorithm development to suggest further reading.We will provide easy to follow notbooks prior to the talk and all examples will be freely available.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dd4673c0-a2dc-4e1e-9cf8-01ada0da8b8a": {"__data__": {"id_": "dd4673c0-a2dc-4e1e-9cf8-01ada0da8b8a", "embedding": null, "metadata": {"title": "Our journey using data and AI to help monitor wildlife in parks in Africa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86f6e0f4-9664-5e9c-af9b-addbb9375abf", "node_type": null, "metadata": {"title": "Our journey using data and AI to help monitor wildlife in parks in Africa"}, "hash": "9488c6f69ad407604dffe93ffea1cd092bb566da6598fd7280c1d2d9b0638950"}}, "hash": "9488c6f69ad407604dffe93ffea1cd092bb566da6598fd7280c1d2d9b0638950", "text": "Our journey using data and AI to help monitor wildlife in parks in Africa\n\nExploration of the intersection between data, AI, and environmental conservation. In this talk, we will share our experiences and practical insights during our journey trying to develop a system using Python, camera traps and data-driven techniques to help detect poachers in Africa.\n\nIn this storytelling and informative talk, we will delve into our experience of data and AI to monitor wildlife in parks in Africa. Our objective is to provide attendees with a comprehensive understanding of the applications, challenges, and opportunities of leveraging data-driven techniques in environmental conservation.\r\n\r\nAudience : individuals interested in leveraging data for positive impact. \r\n\r\nThe talk is accessible to a non-technical audience in its story-telling part, but also contains technical parts and details, as well as a live demonstration of the developed and open-sourced solution. Knowledge of Python and cloud infrastructures may be useful. \r\nTechnologies explored : Python, Node-RED, Streamlit, Google Cloud Platform, Google Vision API, Zamba, Earth Rangers.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "88100b2e-1b46-45e4-958e-a01eda029839": {"__data__": {"id_": "88100b2e-1b46-45e4-958e-a01eda029839", "embedding": null, "metadata": {"title": "Standby detection with a human in the loop"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c8b6f48-94a7-579a-9920-00350de3f2e0", "node_type": null, "metadata": {"title": "Standby detection with a human in the loop"}, "hash": "4133a2d40bcde4f04a14f4d4ffa9e05948f13beb572842caca05165de796f21a"}, "3": {"node_id": "9a35f25a-4ff1-46e9-94b8-c33568e70981", "node_type": null, "metadata": {"title": "Standby detection with a human in the loop"}, "hash": "dc64223ef5d38a80bbaeb09f7c77cc65a1e1caa154563efb109ed179daed3946"}}, "hash": "75ccead6d4fad93739fa72a67eeebc2e21ab074100dcb236cebdc883cc207b5e", "text": "Standby detection with a human in the loop\n\nIn the Netherlands a large share of energy is used by industry.By measuring the energy usage of individual machines in real time it is possible to pinpoint when machines are operating inefficiently and help factories take measures to reduce energy waste.It turns out that in most factories, the biggest source of energy waste comes from idling machines.To be able to give valuable insights and provide relevant alerts to our customers, we set up a machine learning system for standby detection with a \u201chuman in the loop\u201d.In this talk we will go over the considerations that go into setting up a machine learning system with a human in the loop and showcase our approach to the problem.No background knowledge is required for this talk.In the Netherlands a large share of energy is used by industry (>40% compared to only 14% used by households*).Eliminating energy waste in this sector is a big step forward towards a greener future.Therefore, Sensorfact made it its mission to eliminate all industrial energy waste.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9a35f25a-4ff1-46e9-94b8-c33568e70981": {"__data__": {"id_": "9a35f25a-4ff1-46e9-94b8-c33568e70981", "embedding": null, "metadata": {"title": "Standby detection with a human in the loop"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c8b6f48-94a7-579a-9920-00350de3f2e0", "node_type": null, "metadata": {"title": "Standby detection with a human in the loop"}, "hash": "4133a2d40bcde4f04a14f4d4ffa9e05948f13beb572842caca05165de796f21a"}, "2": {"node_id": "88100b2e-1b46-45e4-958e-a01eda029839", "node_type": null, "metadata": {"title": "Standby detection with a human in the loop"}, "hash": "75ccead6d4fad93739fa72a67eeebc2e21ab074100dcb236cebdc883cc207b5e"}, "3": {"node_id": "ae01cd15-adc8-456d-99a9-3d68cf7566ef", "node_type": null, "metadata": {"title": "Standby detection with a human in the loop"}, "hash": "d53134e246352b8d865626ff462065c472efcfef46dd8b351572676183a71a6d"}}, "hash": "dc64223ef5d38a80bbaeb09f7c77cc65a1e1caa154563efb109ed179daed3946", "text": "By measuring the energy usage (electricity or gas) of individual machines in real time it is possible to pinpoint when machines are operating inefficiently and help factories take measures to reduce energy waste.It turns out that in most factories, the biggest source of energy waste comes from forgetting to turn off machines when they are not used.Flagging idling machines based on their electricity usage may seem like a trivial problem at first, however the large variety in machines and production processes makes this a lot harder than you would expect.To be able to give valuable insights on idling machines and provide relevant alerts to our customers, we set up a machine learning system with a \u201chuman in the loop\u201d.In many settings it is perfectly fine to embed a machine learning model in a process without any human interference.However, there are cases where it is better to keep a human in the loop.The most obvious use cases are those where there is simply no room for error, for example in medical applications.However, also in less life threatening it can be beneficial to have a human act as gatekeeper ensuring high quality outputs.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ae01cd15-adc8-456d-99a9-3d68cf7566ef": {"__data__": {"id_": "ae01cd15-adc8-456d-99a9-3d68cf7566ef", "embedding": null, "metadata": {"title": "Standby detection with a human in the loop"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c8b6f48-94a7-579a-9920-00350de3f2e0", "node_type": null, "metadata": {"title": "Standby detection with a human in the loop"}, "hash": "4133a2d40bcde4f04a14f4d4ffa9e05948f13beb572842caca05165de796f21a"}, "2": {"node_id": "9a35f25a-4ff1-46e9-94b8-c33568e70981", "node_type": null, "metadata": {"title": "Standby detection with a human in the loop"}, "hash": "dc64223ef5d38a80bbaeb09f7c77cc65a1e1caa154563efb109ed179daed3946"}}, "hash": "d53134e246352b8d865626ff462065c472efcfef46dd8b351572676183a71a6d", "text": "In this talk we will go over the considerations that go into setting up a machine learning system with a human in the loop and showcase our approach to the problem, using the case of standby detection.We will share learnings from our own experience and along the way give you an overview of the (open source) tools we chose to use for the different facets of the project.No background knowledge is required for this talk.If you are looking for inspiration on how to build a machine learning system with a human in the loop or if you are curious about sustainability use cases this talk may be interesting for you.*https://www.clo.nl/indicatoren/nl0052-energieverbruik-per-sector", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e4c9d236-16eb-4fab-9abb-fe422d1cd8be": {"__data__": {"id_": "e4c9d236-16eb-4fab-9abb-fe422d1cd8be", "embedding": null, "metadata": {"title": "Building True Machine Learning MVPs: Avoiding Pitfalls and Validating the Value Chain"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "569b4008-5ff9-5ec2-8c88-6dfca3e583cd", "node_type": null, "metadata": {"title": "Building True Machine Learning MVPs: Avoiding Pitfalls and Validating the Value Chain"}, "hash": "7b06c59603a33f33918b9a1f3e1c6351b48feb3c2bf845ef8bde01f284ad4d8b"}, "3": {"node_id": "b760ef53-aab3-4d87-98d0-c76ba277e96f", "node_type": null, "metadata": {"title": "Building True Machine Learning MVPs: Avoiding Pitfalls and Validating the Value Chain"}, "hash": "154f42af2dcd0b850891340e7257de2692cd0ab4e9bcb134ead2398c0a1dca83"}}, "hash": "a835b7584569c4d48f9dc61e16ba95595f8efd2a0c9e1dedbc23e8f3b770bceb", "text": "Building True Machine Learning MVPs: Avoiding Pitfalls and Validating the Value Chain\n\nLearn how to build true Machine Learning Minimum Viable Products (MVPs) and avoid common pitfalls!MVPs are not just about testing performance; they allow you to validate the entire value chain, assess feasibility, and gauge desirability.In this talk, we will share practical insights and strategies from our experience of predicting customer intent for the ING mobile banking app.Discover how to rapidly iterate, leverage existing resources, and ensure your MVPs validate the right assumptions.The talk aims to equip attendees with practical knowledge and strategies to build true machine learning MVPs and avoid common pitfalls.It will inspire data science teams to approach MVP development as a holistic process that validates the value chain and aligns with business objectives, showcasing how to hack the system with existing resources and how to find the complete list of assumptions to validate.The talk will cover the following key points:\r\n1.Understanding True MVPs: (5 minutes)\r\n- Explore the concept of true machine learning MVPs beyond model testing.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b760ef53-aab3-4d87-98d0-c76ba277e96f": {"__data__": {"id_": "b760ef53-aab3-4d87-98d0-c76ba277e96f", "embedding": null, "metadata": {"title": "Building True Machine Learning MVPs: Avoiding Pitfalls and Validating the Value Chain"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "569b4008-5ff9-5ec2-8c88-6dfca3e583cd", "node_type": null, "metadata": {"title": "Building True Machine Learning MVPs: Avoiding Pitfalls and Validating the Value Chain"}, "hash": "7b06c59603a33f33918b9a1f3e1c6351b48feb3c2bf845ef8bde01f284ad4d8b"}, "2": {"node_id": "e4c9d236-16eb-4fab-9abb-fe422d1cd8be", "node_type": null, "metadata": {"title": "Building True Machine Learning MVPs: Avoiding Pitfalls and Validating the Value Chain"}, "hash": "a835b7584569c4d48f9dc61e16ba95595f8efd2a0c9e1dedbc23e8f3b770bceb"}}, "hash": "154f42af2dcd0b850891340e7257de2692cd0ab4e9bcb134ead2398c0a1dca83", "text": "- Topics covered: validating the entire value chain, assessing feasibility and desirability.2.Avoiding Pitfalls: (10 minutes)\r\n- Discuss common pitfalls and misconceptions in building MVPs.- Provide some examples of how these pitfalls invalidated projects in the past (e.g.discovering sleeping dogs in mortgages)\r\n3.Lessons from Predicting Customer Intent: (10 minutes)\r\n- Share insights from our experience of predicting customer intent for the ING mobile banking app.- Highlight how we leveraged existing resources by reusing front-end components and utilizing a simple SQL query to drive rapid iteration.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2d099a71-5487-444c-83d8-7d8dc6569ad4": {"__data__": {"id_": "2d099a71-5487-444c-83d8-7d8dc6569ad4", "embedding": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffdd7835-3743-5f4b-99ca-ebe6cfbb21ee", "node_type": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "hash": "d67d8c18c2a0772208be9893f38e451d51522aeaca85f3098d7c133ed3265449"}, "3": {"node_id": "604919f9-c6c8-4222-bc6f-3dfcd5299be6", "node_type": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "hash": "cac2335232851d7dc766e69ad0a53fc64f67c636518d80376d06fac3c608ad94"}}, "hash": "6236c38f4426d9072e6b75046d51645341c1cb97df6612ee6a54f1b774a85923", "text": "Bayesian ranking for tennis players in PyMC\n\nIn this talk, we will explore the Bayesian Bradley Terry model implemented in PyMC.We will focus on its application for ranking tennis players, demonstrating how this probabilistic approach can provide an accurate and robust rankings, arguably better than the ATP ranking itself and the Elo rating system.By leveraging the power of Bayesian statistics, we can incorporate prior knowledge, handle uncertainty, and make better inferences about player abilities.Join us to learn how to implement the Bayesian Bradley Terry model in PyMC and discover its advantages for ranking tennis players.The Bradley Terry model is a powerful model to predict the outcome of a paired comparison, as a by-product we will be able to rank players based on their hidden (latent) ability scores.Traditionally, rankings have been based on simple win-loss records, which may not capture the true abilities of players due to variations in competition quality and sample size.By adopting a Bayesian framework, we can overcome these limitations and obtain more reliable rankings.In this talk, we will introduce the Bayesian Bradley Terry model and its underlying principles.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "604919f9-c6c8-4222-bc6f-3dfcd5299be6": {"__data__": {"id_": "604919f9-c6c8-4222-bc6f-3dfcd5299be6", "embedding": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffdd7835-3743-5f4b-99ca-ebe6cfbb21ee", "node_type": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "hash": "d67d8c18c2a0772208be9893f38e451d51522aeaca85f3098d7c133ed3265449"}, "2": {"node_id": "2d099a71-5487-444c-83d8-7d8dc6569ad4", "node_type": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "hash": "6236c38f4426d9072e6b75046d51645341c1cb97df6612ee6a54f1b774a85923"}, "3": {"node_id": "7c68894e-4f93-44a9-b983-6a36984222a0", "node_type": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "hash": "ad2d6ba727797e7562d745d2473169174d9927efb57fd406596c6c52783d24cd"}}, "hash": "cac2335232851d7dc766e69ad0a53fc64f67c636518d80376d06fac3c608ad94", "text": "We will explore how to encode the model in Python using the PyMC library.We will walk through the step-by-step implementation, highlighting key considerations and practical tips.To illustrate the model's effectiveness, we will showcase its application to ranking tennis players, and compare it with both the official ATP ranking and the ELO ranking system.Tennis provides an ideal domain for this analysis, as it involves head-to-head matches between players, allowing us to directly compare their abilities.By applying the Bayesian Bradley Terry model to historical tennis match data, we can generate rankings that better reflect players' true skills, accounting for factors such as opponent strength and match surface.Throughout the talk, we will emphasize a hands-on approach, providing code examples and demonstrations.Attendees will gain a solid understanding of the model, learn how to implement it using PyMC, a practical application, possible extensions and maybe a few PyMC tricks along the way.### Outline\r\n\r\n* What's wrong with current tennis ranking.* Introduction to the Bayesian Bradley Terry model.* Implementation of the model in PyMC.* Application to ranking tennis players by latent ability score.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7c68894e-4f93-44a9-b983-6a36984222a0": {"__data__": {"id_": "7c68894e-4f93-44a9-b983-6a36984222a0", "embedding": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffdd7835-3743-5f4b-99ca-ebe6cfbb21ee", "node_type": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "hash": "d67d8c18c2a0772208be9893f38e451d51522aeaca85f3098d7c133ed3265449"}, "2": {"node_id": "604919f9-c6c8-4222-bc6f-3dfcd5299be6", "node_type": null, "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}, "hash": "cac2335232851d7dc766e69ad0a53fc64f67c636518d80376d06fac3c608ad94"}}, "hash": "ad2d6ba727797e7562d745d2473169174d9927efb57fd406596c6c52783d24cd", "text": "* Comparison with ATP ranking and ELO rating system.* Possible extensions and other applications.### Prerequisites\r\n\r\nBasic knowledge of Python, PyMC and probability concepts will be helpful.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "197cca09-5692-4951-80c3-e5ee9742ac36": {"__data__": {"id_": "197cca09-5692-4951-80c3-e5ee9742ac36", "embedding": null, "metadata": {"title": "Keynote \"Natural Intelligence is All You Need [tm]\""}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0485fc1-fcb3-535a-98fd-85efd758a50e", "node_type": null, "metadata": {"title": "Keynote \"Natural Intelligence is All You Need [tm]\""}, "hash": "1584232fcfdb1a3a6aaa233d93a8a8dc01b181c4b85fdb4569ad58ecb213f12a"}, "3": {"node_id": "b59e8c16-5a9a-449c-bbb6-ec3ab66b6bc9", "node_type": null, "metadata": {"title": "Keynote \"Natural Intelligence is All You Need [tm]\""}, "hash": "07abf7e9fe9c55f80440b49ab9da9548aee1d3459970f6067618ecb5cf8c9b52"}}, "hash": "45d80980d544e707006c6906aee81ef9a00c439f9a2d67f3ec9559921a7ce05c", "text": "Keynote \"Natural Intelligence is All You Need [tm]\"\n\nIn this talk I will try to show you what might happen if you allow yourself the creative **freedom** to rethink and reinvent common practices once in a while.As it turns out, in order to do that, natural intelligence is all you need.And we may start needing a lot of it in the near future\n\nI\u2019ve met a lot of authoritative people in my field who pass out advise that sounds like this:\r\n\r\nWorking on recommenders?Collect all the data!Sessions!Working on text classification?That\u2019s a solved problem!Bert!Working with embeddings?There\u2019s a library for that already!Working on tabular data?XGBoost for the win!GridSearch!In short: \u201cthis is how you do data science, don\u2019t go and reinvent the wheel\u201d.If you spend 5 minutes thinking about \u201cthe invention of the wheel\u201d though, then you may start to rethink.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b59e8c16-5a9a-449c-bbb6-ec3ab66b6bc9": {"__data__": {"id_": "b59e8c16-5a9a-449c-bbb6-ec3ab66b6bc9", "embedding": null, "metadata": {"title": "Keynote \"Natural Intelligence is All You Need [tm]\""}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0485fc1-fcb3-535a-98fd-85efd758a50e", "node_type": null, "metadata": {"title": "Keynote \"Natural Intelligence is All You Need [tm]\""}, "hash": "1584232fcfdb1a3a6aaa233d93a8a8dc01b181c4b85fdb4569ad58ecb213f12a"}, "2": {"node_id": "197cca09-5692-4951-80c3-e5ee9742ac36", "node_type": null, "metadata": {"title": "Keynote \"Natural Intelligence is All You Need [tm]\""}, "hash": "45d80980d544e707006c6906aee81ef9a00c439f9a2d67f3ec9559921a7ce05c"}}, "hash": "07abf7e9fe9c55f80440b49ab9da9548aee1d3459970f6067618ecb5cf8c9b52", "text": "After all: the wheels on a bike are different from the wheels on an airplane, just like the wheels of a tractor.And for Pete\u2019s sake: that\u2019s a good thing!If we hadn\u2019t reinvented those wheels, we\u2019re be stuck with wooden horse carts.So is it really a bad thing if we try to rethink our tools?What if, as a field, we\u2019re not moving because we\u2019re too afraid of reinventing the wheel?In this talk I will try to show you what might happen if you allow yourself the creative **freedom** to rethink and reinvent common practices once in a while.As it turns out, in order to do that, natural intelligence is all you need.And we may start needing a lot of it in the near future", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6d590827-52d5-45df-9791-451711fc524e": {"__data__": {"id_": "6d590827-52d5-45df-9791-451711fc524e", "embedding": null, "metadata": {"title": "Transfer Learning in Boosting Models"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce9ef2b8-e1bc-59de-8de7-8d5809253967", "node_type": null, "metadata": {"title": "Transfer Learning in Boosting Models"}, "hash": "21f139bcea12a14ae8c1c65839519ba551bd02131c59fe421d156c0d8f8dba00"}, "3": {"node_id": "209ef90a-650a-4767-bed4-3ce70b9a86d4", "node_type": null, "metadata": {"title": "Transfer Learning in Boosting Models"}, "hash": "8cb3e1353ed68248a1930eb111a8c80b738af50e902509fd3186af78cced3e6b"}}, "hash": "a08d789a33018ff7b914fb141f681714e93d931e9e30412962a8d7b03fa90e3d", "text": "Transfer Learning in Boosting Models\n\nDid you know that you could do transfer learning on boosted forests too?Even in current days, we face business cases where the modelling sample is very low.This brings an uncertainty to the modelling results and in some cases no ability to model at all.To counter it, we investigated the ability to use transfer learning approaches on boosting models.In this talk, we would like to show the methods used and results from a real case example applied to the credit risk domain.Transfer learning (TL), a form of machine learning, involves leveraging knowledge acquired while addressing one task and applying it to a related task.While TL is mainly associated with deep learning tasks, it is also applicable to boosting algorithms which are commonly used in advanced credit risk modelling.During the talk, we present a real use-case involving building a probability of default (PD) model for a customer segment with small data history within the bank.There can be several ways to benefit from data coming from other customer segments with already rich data available within the bank.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "209ef90a-650a-4767-bed4-3ce70b9a86d4": {"__data__": {"id_": "209ef90a-650a-4767-bed4-3ce70b9a86d4", "embedding": null, "metadata": {"title": "Transfer Learning in Boosting Models"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce9ef2b8-e1bc-59de-8de7-8d5809253967", "node_type": null, "metadata": {"title": "Transfer Learning in Boosting Models"}, "hash": "21f139bcea12a14ae8c1c65839519ba551bd02131c59fe421d156c0d8f8dba00"}, "2": {"node_id": "6d590827-52d5-45df-9791-451711fc524e", "node_type": null, "metadata": {"title": "Transfer Learning in Boosting Models"}, "hash": "a08d789a33018ff7b914fb141f681714e93d931e9e30412962a8d7b03fa90e3d"}}, "hash": "8cb3e1353ed68248a1930eb111a8c80b738af50e902509fd3186af78cced3e6b", "text": "Simple approaches would be:  \r\n- Fit a model on only rich data & just apply to the limited data  \r\n- Fit a model on both data sets, but tune it on the limited data  \r\n\r\nMore complex (TL) approaches:  \r\n- Fit a model on rich data with sample weights come from resemblance analysis to calculate similarity between these two data sources.- Use refitting with the limited data on the model trained on rich data  \r\n- Start with an initial pre-trained model while modelling on the limited data  \r\n\r\nJoin us for an engaging session where we will share the outcomes of our experiments and lessons learned, as we address these approaches that hold relevance beyond the presented use-case, offering practical applicability for similar scenarios in your own domain.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b3f98ffe-a897-4d5a-8fca-6721fd7035f6": {"__data__": {"id_": "b3f98ffe-a897-4d5a-8fca-6721fd7035f6", "embedding": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f11b211-2573-5bfb-bf12-33dfea2ab2a7", "node_type": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "hash": "2d9117c247274db8b4b5f1046f64323227d4b35b5da67131fb7bccb1d9de6250"}, "3": {"node_id": "73bb4c05-757c-4eb4-919e-bd7147de6206", "node_type": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "hash": "a8318e59cf451657a0ecd44d0ceac062b6a98483e2018dd5f7596a900117e201"}}, "hash": "bfab3eb6db4efe2ccc7adddf7c69294cce8112cafb234290c603247f1401119f", "text": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization\n\nOptimizing machine learning models using regular metrics is a common practice in the industry.However, aligning model optimization with business metrics is closely tied to the objectives of the business and is highly valued by product managers and other stakeholders.This talk delves into the process of training machine learning models based on business metrics in order to enhance economic outcomes.With a primary focus on data scientists and machine learning practitioners, this talk explores techniques, methodologies, and real-world applications that harness the power of business metrics to propel machine learning models and foster business success.We will present a specific case study that demonstrates how we utilized business metrics at Booking.com that brought significant impact on model performance on business outcomes.Specifically, we will discuss our approaches to leveraging business metrics for hyperparameter tuning and reducing model complexity, which instill greater confidence within our team when deploying improved models to production.Description\r\nThis talk aims to equip data scientists and machine learning practitioners with the knowledge and tools to train machine learning models on business metrics effectively.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "73bb4c05-757c-4eb4-919e-bd7147de6206": {"__data__": {"id_": "73bb4c05-757c-4eb4-919e-bd7147de6206", "embedding": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f11b211-2573-5bfb-bf12-33dfea2ab2a7", "node_type": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "hash": "2d9117c247274db8b4b5f1046f64323227d4b35b5da67131fb7bccb1d9de6250"}, "2": {"node_id": "b3f98ffe-a897-4d5a-8fca-6721fd7035f6", "node_type": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "hash": "bfab3eb6db4efe2ccc7adddf7c69294cce8112cafb234290c603247f1401119f"}, "3": {"node_id": "7657eefb-fbf1-4fdd-9518-2392754c936e", "node_type": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "hash": "ef50e90ba2734cdfc931b01620c5821532f063d5d32d3c08dfd6da7007725bbf"}}, "hash": "a8318e59cf451657a0ecd44d0ceac062b6a98483e2018dd5f7596a900117e201", "text": "We will delve into the process of hyperparameter tuning, algorithm selection, and model evaluation specifically tailored for optimizing economic outcomes.A real-world use case at Booking.com will demonstrate the transformative power of this approach.Outline\r\n- Introduction to training machine learning models on machine learning metrics versus business metrics \r\n- Overview of the significance of leveraging business metrics to improve machine learning models' performance on business metrics \r\n- Introduction to machine learning algorithms suitable for modeling business metrics to drive economic optimizations \r\n- Metrics and evaluation, and training techniques specific to assessing the business impact of machine learning models\r\n- Showcasing practical use case at Booking.com where training models on business metrics has led to significant improvements in economic outcomes\r\n\r\nCentral Focus\r\nTraining machine learning models on business metrics present a powerful methodology for optimizing economic outcomes.By incorporating relevant business data and metrics into the modeling process, data scientists and machine learning practitioners can drive substantial improvements in economic performance.This talk will provide attendees with the necessary insights and techniques to apply this approach successfully.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7657eefb-fbf1-4fdd-9518-2392754c936e": {"__data__": {"id_": "7657eefb-fbf1-4fdd-9518-2392754c936e", "embedding": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f11b211-2573-5bfb-bf12-33dfea2ab2a7", "node_type": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "hash": "2d9117c247274db8b4b5f1046f64323227d4b35b5da67131fb7bccb1d9de6250"}, "2": {"node_id": "73bb4c05-757c-4eb4-919e-bd7147de6206", "node_type": null, "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}, "hash": "a8318e59cf451657a0ecd44d0ceac062b6a98483e2018dd5f7596a900117e201"}}, "hash": "ef50e90ba2734cdfc931b01620c5821532f063d5d32d3c08dfd6da7007725bbf", "text": "Key Takeaways\r\n- Understanding the importance of training machine learning models on business metrics for economic optimizations\r\n- Familiarity with machine learning algorithms suitable for modeling business metrics and driving economic outcomes\r\n- Strategies for evaluating and quantifying the economic impact of machine learning models\r\nReal-world inspiration and practical insights for applying this approach to boost economic outcomes\r\n\r\nExpected Background Knowledge\r\nAttendees should have a foundation in machine learning concepts and practical experience with data science techniques, in particular, knowledge of Gradient Boosting Machine (GBM) models.Familiarity with business metrics, economic principles, and optimization objectives will be beneficial but not required.We aim to deliver an informative and practical talk that caters to data scientists and machine learning practitioners.Attendees will gain actionable insights, methodologies, and real-world examples to effectively train machine learning models on business metrics, leading to enhanced economic outcomes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f0833169-e7c7-433b-8e28-7da0c3924cc4": {"__data__": {"id_": "f0833169-e7c7-433b-8e28-7da0c3924cc4", "embedding": null, "metadata": {"title": "Polars and a peek in the expression engine"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89e8ef97-184b-5aad-9054-447f5410a089", "node_type": null, "metadata": {"title": "Polars and a peek in the expression engine"}, "hash": "db6e17fca12bf054d9837c98ea22f9b8b8bb642a43d231448d832b946d087599"}}, "hash": "db6e17fca12bf054d9837c98ea22f9b8b8bb642a43d231448d832b946d087599", "text": "Polars and a peek in the expression engine\n\nThis talk we will see why the expression engine in polars is so versatile and fast.\r\nWe will look at them in the perspective of the optimizer as well as the physical engine.\n\nPolars expressions are a DSL to a very powerful vectorized engine. They make it very easy to write parallel, efficient and readable code. \r\n\r\nThis talk we will see why the expression engine in polars is so versatile and fast.\r\nWe will look at them in the perspective of the optimizer as well as the physical engine.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8aebd03a-c249-4c02-9e06-730da20a5a0e": {"__data__": {"id_": "8aebd03a-c249-4c02-9e06-730da20a5a0e", "embedding": null, "metadata": {"title": "Using AI to make Amsterdam greener, safer and more accessible"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73f28a4c-f282-5739-9cc5-d517ef4870d8", "node_type": null, "metadata": {"title": "Using AI to make Amsterdam greener, safer and more accessible"}, "hash": "615cdab0bced221235adda41f63bfbf07f1b05ef0658f6af78925bfa1164daea"}, "3": {"node_id": "f14d2c0b-5e9d-48e3-a1bf-e9fb3bfd54a4", "node_type": null, "metadata": {"title": "Using AI to make Amsterdam greener, safer and more accessible"}, "hash": "1569462aba5ceaf4726a4c7272f740de47e3b5b0d8736f3f293612c6df5b3342"}}, "hash": "f888830e4c4bd952916d799fee6794a13570446fcee976e766bee232a737c306", "text": "Using AI to make Amsterdam greener, safer and more accessible\n\nIn this talk, we would like to introduce you to the urban challenges that the City of Amsterdam is trying to solve using AI.We will walk you through the technical details behind one of our projects and invite you to join us in the ethical development of cool AI applications for social good.The City of Amsterdam has the mission of promoting the development of artificial intelligence to improve the lives of Amsterdam\u2019s residents.We conduct cutting-edge research into the analysis of text, images, and point cloud data, all with the aim of solving the urban challenges of our generation and the ones to come.Recently, we\u2019ve been working on making our city more inclusive by mapping accessibility infrastructure in the public space.We\u2019ve been also working on making the city safer by localizing all street lights and automatically extracting some of their characteristics.Finally, our analysis of trees and greenery in the city can help increase the city's biodiversity and also help us reach our climate goals.Working in the public sector means that technology itself is only a part of our job.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f14d2c0b-5e9d-48e3-a1bf-e9fb3bfd54a4": {"__data__": {"id_": "f14d2c0b-5e9d-48e3-a1bf-e9fb3bfd54a4", "embedding": null, "metadata": {"title": "Using AI to make Amsterdam greener, safer and more accessible"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73f28a4c-f282-5739-9cc5-d517ef4870d8", "node_type": null, "metadata": {"title": "Using AI to make Amsterdam greener, safer and more accessible"}, "hash": "615cdab0bced221235adda41f63bfbf07f1b05ef0658f6af78925bfa1164daea"}, "2": {"node_id": "8aebd03a-c249-4c02-9e06-730da20a5a0e", "node_type": null, "metadata": {"title": "Using AI to make Amsterdam greener, safer and more accessible"}, "hash": "f888830e4c4bd952916d799fee6794a13570446fcee976e766bee232a737c306"}}, "hash": "1569462aba5ceaf4726a4c7272f740de47e3b5b0d8736f3f293612c6df5b3342", "text": "On a daily basis, we also need to ensure that all development is done according to our city\u2019s values \u2013 for example, that applications benefit everyone, that we are open and transparent, and that we give citizens a say in shaping their (digital) city.This means (at the very least) that open-source development and the publication of methodology, data, and insights for all of our algorithms are an inseparable part of work.In this talk, we would like to introduce you to the challenges that we face, walk you through the technical details behind one of our projects, and share the related open-source materials that can be reused by the PyData community.Finally, we hope to inspire you to join us in the ethical development of cool AI applications for social good.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ab2dade4-0372-4ef3-9d0f-fc3e1f92fba8": {"__data__": {"id_": "ab2dade4-0372-4ef3-9d0f-fc3e1f92fba8", "embedding": null, "metadata": {"title": "Generating Data Frames for your test - using Pandas stratgies in Hypothesis"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9cfdb94-6920-5b26-b93c-f9511d7aec1f", "node_type": null, "metadata": {"title": "Generating Data Frames for your test - using Pandas stratgies in Hypothesis"}, "hash": "c22b92a1f1adb4798289c619a54fcf1fbd52dc282781771c822d2cbde68798ba"}, "3": {"node_id": "aa636989-22d8-4fb8-8619-61fe6e8ffa7b", "node_type": null, "metadata": {"title": "Generating Data Frames for your test - using Pandas stratgies in Hypothesis"}, "hash": "ec0ac3ed3351796e43f346b39b6085559d32798e323e9971c78e2b6f7ae3e599"}}, "hash": "d87dbef339dd91a1633a88e571c716b164430978e2d6a822750b346a1e73df09", "text": "Generating Data Frames for your test - using Pandas stratgies in Hypothesis\n\nDo you test your data pipeline?Do you use Hypothesis?In this workshop, we will use Hypothesis - a property-based testing framework to generate Pandas DataFrame for your tests, without involving any real data.In this short 90 mins workshop, we will first go through the basics of hypothesis and what is property-based testing.After that, we will introduce the strategies for Pandas objects - available via the extras in Hypothesis.We will have a glimpse of what the strategies are doing to generate the testing object, including Pandas Series and DataFrames.In the end, we will apply what we learn in real testing applications - testing a data pipeline that involves DataFrames.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aa636989-22d8-4fb8-8619-61fe6e8ffa7b": {"__data__": {"id_": "aa636989-22d8-4fb8-8619-61fe6e8ffa7b", "embedding": null, "metadata": {"title": "Generating Data Frames for your test - using Pandas stratgies in Hypothesis"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9cfdb94-6920-5b26-b93c-f9511d7aec1f", "node_type": null, "metadata": {"title": "Generating Data Frames for your test - using Pandas stratgies in Hypothesis"}, "hash": "c22b92a1f1adb4798289c619a54fcf1fbd52dc282781771c822d2cbde68798ba"}, "2": {"node_id": "ab2dade4-0372-4ef3-9d0f-fc3e1f92fba8", "node_type": null, "metadata": {"title": "Generating Data Frames for your test - using Pandas stratgies in Hypothesis"}, "hash": "d87dbef339dd91a1633a88e571c716b164430978e2d6a822750b346a1e73df09"}}, "hash": "ec0ac3ed3351796e43f346b39b6085559d32798e323e9971c78e2b6f7ae3e599", "text": "## Outline\r\n- Introduction of Property-based testing (15 mins)\r\n- Introduction and basic use of Hypothesis exercises (30 mins)\r\n- Deep dive into Pandas strategies (20 mins)\r\n- Do it yourself - apply property-based testing to data pipelines (20 mins)\r\n- Conclusion (5 mins)\r\n\r\n## Prerequisits\r\nNo prior knowledge of property-based testing or hypothesis is required.However, we assume the attendee has experience using Pandas and has a basic understanding of Pandas objects.Knowledge about Numpy array and typing would also be beneficial in understanding the Pandas Strategies.## Goal\r\nWe hope the attendee will learn about property-based testing and see how it can benefit their work involved data - especially those that use Pandas.After the workshop, attendees should be able to understand how the Pandas strategies in Hypothesis works and to use Hypotheses to test codes that involve Pandas Series or DataFrame input.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e63cad79-5195-44db-b730-34c9b94785cf": {"__data__": {"id_": "e63cad79-5195-44db-b730-34c9b94785cf", "embedding": null, "metadata": {"title": "There are no bad labels, only happy accidents"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28ea4e75-e11b-56a7-b889-08724577cd95", "node_type": null, "metadata": {"title": "There are no bad labels, only happy accidents"}, "hash": "13cb367eb18224c50642e6d148d679fa7f92888488c2b65bd049a5ad64943730"}, "3": {"node_id": "e0cab5ab-e70e-47e2-80aa-e3879037e8d5", "node_type": null, "metadata": {"title": "There are no bad labels, only happy accidents"}, "hash": "aac82dd7d95bd154a0cda33a7d16148ef6885cd9fc5f6c7de50e940ece6743cd"}}, "hash": "78266db3a4c108a0cd348d59418d53715e579f7ddb5707209849ca11d9cca863", "text": "There are no bad labels, only happy accidents\n\nAre you 100% sure that you can trust your labels?Imagine spending a company credit card worth of compute on getting the best model statistics ever.Would that be money well spent if your dataset has some labeling issues?More often than not, \"bad labels\" are great because they can tell you how to improve the machine learning model before even training it.But it only works only if you actually spend the time being confronted with your own dataset.In this workshop, we'll annotate our own data while we leverage techniques to find happy accidents.To solve specific problems, you don't need loads of data anymore \u2013 you just need good data.This workshop is split up into two segments.First, we will dive into some data quality issues of some well-known datasets.We will have prepared some datasets along with code, tricks, and Python tools to help you understand the data quality (or the lack thereof).While doing this, we will also discuss some theories behind annotator agreement metrics and show how they might help you make decisions.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e0cab5ab-e70e-47e2-80aa-e3879037e8d5": {"__data__": {"id_": "e0cab5ab-e70e-47e2-80aa-e3879037e8d5", "embedding": null, "metadata": {"title": "There are no bad labels, only happy accidents"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28ea4e75-e11b-56a7-b889-08724577cd95", "node_type": null, "metadata": {"title": "There are no bad labels, only happy accidents"}, "hash": "13cb367eb18224c50642e6d148d679fa7f92888488c2b65bd049a5ad64943730"}, "2": {"node_id": "e63cad79-5195-44db-b730-34c9b94785cf", "node_type": null, "metadata": {"title": "There are no bad labels, only happy accidents"}, "hash": "78266db3a4c108a0cd348d59418d53715e579f7ddb5707209849ca11d9cca863"}}, "hash": "aac82dd7d95bd154a0cda33a7d16148ef6885cd9fc5f6c7de50e940ece6743cd", "text": "Once we have some experience with these techniques, we will annotate some data as a group, after which we will share the dataset so that the group may analyze and be confronted with their own annotations.The goal of the workshop is to give people the ability to detect a \"data smell\".Through hands-on experimentation with data quality challenges, you'll learn how to better reason about and iterate on your own data.Data quality really matters, and confronting the labeling process yourself can help you better your machine learning pipeline and evaluation process.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"0028b7eb-2e98-56ec-9fed-e4cd83f4d118": {"node_ids": ["605a748e-e813-4a4f-993b-daa7fe1fd9f6", "d9457ff1-153d-41cd-8c3a-1a8ec98e0e0c", "07818f93-ad74-4701-8db9-28af07b448a0", "4def9880-4c0c-45df-8c34-c03845d9231c"], "metadata": {"title": "Forecasting Customer Lifetime Value (CLTV) for Marketing Campaigns under Uncertainty with PySTAN"}}, "0b21795f-fb6f-583d-a77d-bceee45fd768": {"node_ids": ["8e793421-25f9-40f0-9919-8d54dbc4a67e", "12285416-baab-407e-b1e8-45626a98d769", "dea587a7-93d4-4061-afa7-12856d76e08b"], "metadata": {"title": "Causal Inference Libraries: What They Do, What I'd Like Them To Do"}}, "7b6015d4-f08c-5df1-a904-965815a3180b": {"node_ids": ["b8f4b6b8-aa9b-422d-8606-1c0cfc1dd375", "9928fea1-99d4-4b5c-a4f8-dcdde8a96878", "4666beb1-d82c-4952-a8c1-94c19c4a8716"], "metadata": {"title": "Staggered Difference-in-Differences in Practice: Causal Insights from the Music Industry"}}, "64283b01-5050-5b2a-bfa7-67044ee6b343": {"node_ids": ["6d5d25f3-351a-4e7f-aeb5-a5edb2d07f33", "681576f0-41c0-44c1-b318-d79dfc64e7a5"], "metadata": {"title": "Don\u2019t judge a book by its cover: Using LLM created datasets to train models that detect literary features"}}, "df2190cb-ce2b-5155-bf89-47ef2a5a8594": {"node_ids": ["35344444-233c-4e22-a2fc-b121373345be", "4f467eb7-fbb8-4779-9ada-2f64b8703207"], "metadata": {"title": "Mind the language: how to monitor NLP and LLM in production"}}, "7097b087-3738-56ca-b572-6026df468b73": {"node_ids": ["bb24db79-e3c5-4770-ba8b-703596e6aea8", "8e156afc-147e-4d3e-bb68-d6741225a052"], "metadata": {"title": "Revealing the True Motives of News Readers"}}, "e5266795-c955-5045-bfbe-9b7206a3da31": {"node_ids": ["a53add3d-456a-4158-bb5d-59e83f82015e", "716a3d0c-dbba-47f7-b9a3-9df4c1bcd9cb"], "metadata": {"title": "Extend your scikit-learn workflow with Hugging Face and skorch"}}, "32b0fc9d-f825-59d5-87bf-1865861452e6": {"node_ids": ["f1af43b7-3872-4265-98ef-4c114ebbcde4", "997ebb8a-e46f-424d-8a94-e8894ca44dd0"], "metadata": {"title": "From Vision to Action: Designing and Deploying Effective Computer Vision Pipelines"}}, "80309313-5f6a-58dd-b81f-730483b2dd6b": {"node_ids": ["ec0155d6-a3fd-4772-852e-501fdabec2e1"], "metadata": {"title": "Online ML Serving best practices"}}, "e5441b76-3ff7-596b-9e16-45abb13d0a77": {"node_ids": ["b596bea5-ef57-4ac1-aca6-93cd0d9b837b", "68ad0027-e543-400d-b8a9-c1821ba70eb7", "c64baa1b-48fc-4917-bf79-0529a6354f0f", "233508d3-608f-426d-9fe4-213b3fbd2256"], "metadata": {"title": "In-Process Analytical Data Management with DuckDB"}}, "c450cf56-9c5f-5fa4-8e9d-d05c0a5af981": {"node_ids": ["14acdc8e-4682-4f6f-9515-14f59d314acd", "d2124416-9247-428b-b1a6-0878552f97e3", "1ed03643-25db-445b-b684-e1efefe280bf"], "metadata": {"title": "Declarative data manipulation pipeline with Dagster"}}, "e9d34ab2-6fdf-5b41-82c0-2365a11d012e": {"node_ids": ["c349aed2-5b64-4672-a89c-1f9a4e2bbbf2", "6f8eaa7a-34dc-4855-b8a5-4bcf54471140"], "metadata": {"title": "PyIceberg: Tipping your toes into the petabyte data-lake"}}, "e82f37c8-03f9-5cb5-92f2-8f157924b59d": {"node_ids": ["0a06b37c-6100-42ad-8866-768322a148c5"], "metadata": {"title": "Keynote Vicki Boykis"}}, "21e0cc6f-bb25-5f30-8afd-896a68790937": {"node_ids": ["64132b3c-fa33-4ae2-82df-aaf1f029018f", "c6d66437-122d-47d3-8a8e-64ed191b5c11"], "metadata": {"title": "What the PDEP? An overview of some upcoming pandas changes"}}, "82e3db42-5170-5eac-ad21-977d67e729b8": {"node_ids": ["a287e9e2-37a0-47ad-a4ad-a74ff94ba760", "7a1b2479-89c5-4a64-a63c-8574a1241093"], "metadata": {"title": "Turning your Data/AI algorithms into full web applications in no time with Taipy"}}, "9a85e18c-c167-5721-994b-8a15333fe5af": {"node_ids": ["6ba97669-ec4a-44ac-a3b3-e87483d02ef2", "29840ed9-38e3-4d5c-af5e-494a344b3690", "4ee063a6-5de0-4993-9f65-771d9ad95928", "ffd95951-7d70-4bdd-9bd6-4e562e7b6e9f"], "metadata": {"title": "Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com"}}, "f8de93a7-8a01-5b3c-ab6a-bafed8754db1": {"node_ids": ["0c68890f-2a56-43d8-8b8f-2cca9543bc77", "6bd2fe90-317c-4662-8c2f-1c3f46926175"], "metadata": {"title": "Graph Neural Networks for Real World Fraud Detection"}}, "37b038f8-5842-5f59-b441-c26b96cd2690": {"node_ids": ["fbea8871-0243-4c4a-923f-4b750eafcb29"], "metadata": {"title": "Promtly Evaluating Prompts with Bayesian Tournaments"}}, "4219e333-9d0f-595f-a7ef-8ef7d4cdea8a": {"node_ids": ["0f94b288-176a-41eb-a2e3-36c3b1e50f0c", "01042a01-bc0b-437e-92e1-7261b83c7ba9", "edc76a48-3fb9-49c8-955a-c3b407523cb1"], "metadata": {"title": "Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy"}}, "7896c0f3-e1e9-5103-a9af-2976ed5118d5": {"node_ids": ["fdcc5a66-1c5a-4f53-b785-51910876a809"], "metadata": {"title": "Lightning talks"}}, "68f57a2f-125a-57f2-ae71-5ae344efa647": {"node_ids": ["0c8f746e-7b94-46bd-93cf-a2fcd56b200b"], "metadata": {"title": "Kickstart AI sponsored drinks [time & location TBD]"}}, "1796a41f-f4b9-5498-b08a-069ce827c06d": {"node_ids": ["5095e989-9f90-46c6-be52-3708616d7e91", "c075a1e6-91e5-4d09-8800-dfcf062314e8"], "metadata": {"title": "Designing a Machine Learning System"}}, "73e8f0d5-c586-5bb5-866f-9071f5bc2d40": {"node_ids": ["f3a42fba-5390-4b51-860e-569b73ff61c8"], "metadata": {"title": "Uncertainty visualization with ArviZ"}}, "e054832c-e8fd-56bb-987b-1ce5eb94d7dd": {"node_ids": ["e85aa15b-8cff-4976-8fa3-7c34d3a4ab20", "80819620-31b8-43a0-829b-89e7e35cbad8", "c0c578e3-4bcc-4253-8b63-335f2f6fa4f3", "872e3b26-8321-4c01-a794-8da24fec212e"], "metadata": {"title": "Probabilistic predictions: probabilistic forecasting with sktime and probabilistic regression with skpro"}}, "20780d65-1ec2-51f9-b7e2-ab25c930becc": {"node_ids": ["c89e546e-c997-47aa-8130-30c8c6f4d7a4", "6867abd6-99eb-480b-be4c-feea03f9a951"], "metadata": {"title": "Unconference: Interviews: Tips and Stories from Both Sides"}}, "38fcc911-f1bf-53f2-a217-fa7979592794": {"node_ids": ["26b6170b-5340-4e52-8e2c-8971d0ff2c33"], "metadata": {"title": "Unconference #2"}}, "b1316bcc-7a02-5d20-83ac-77806aa91efd": {"node_ids": ["a53a0321-3d18-470e-8e31-004ed623593f", "4cd10bd5-f299-42bf-846e-383912adc23e"], "metadata": {"title": "Innovation in the Age of Regulation: Federated Learning with Flower"}}, "8f739f49-0ed3-5db9-99bc-b4d02ae9ad97": {"node_ids": ["9ed7246f-859a-4e3c-b853-ffbc7e254760"], "metadata": {"title": "Using Kubernetes as your multi-tenant orchestration layer"}}, "4edb2500-cc63-510c-9d30-5dd259022b4a": {"node_ids": ["cccf5f12-66b2-46a0-8e7a-bc5b23a0db36", "29b4ac38-bbf7-4b0b-9baa-9098daf45d9d"], "metadata": {"title": "Let\u2019s exploit pickle, and `skops` to the rescue!"}}, "cd77e690-a63f-564c-8a15-5da291b8040f": {"node_ids": ["ee7a64f4-fc28-400e-87a9-47b14dc64c81", "775d5324-9cac-49f3-b393-d2b5a73bc687"], "metadata": {"title": "Mastering Recommendation Systems Evaluation: An A/B Testing Approach with Insights from the Industry"}}, "f418d8b0-4976-5fb0-ab84-49ddfeff58d2": {"node_ids": ["5db5678f-2e9a-4956-a4e8-ad0ec4e92d96", "2263dc47-4263-42df-8e6d-63aa2810c9df"], "metadata": {"title": "To One-Hot or Not: A guide to feature encoding and when to use what"}}, "da6b1185-bfaf-568b-a2dd-2e5236440ea2": {"node_ids": ["0a815d19-1f1f-404a-a926-81fd70e92597", "b2426976-f96a-4fae-bdc5-44e2d52fa83d", "804ef235-d6d9-46b3-9111-9b39f5f468ec"], "metadata": {"title": "Survival Analysis: a deep dive"}}, "cbb0b87d-a8ba-536e-b126-878f6600ff69": {"node_ids": ["84e65ea3-0818-4784-b4e2-22a08e59ac17", "634fd595-6c9a-4f98-b5ba-a6ea0b2a0fad"], "metadata": {"title": "Deep look into Deepfakes: Mastering Creation, Impact, and Detection"}}, "79731e0e-a1d2-597c-88a9-69920acd5d09": {"node_ids": ["c6431e49-1b5d-48e9-84d5-1be2ee34500b", "ec6b2bc7-f9e7-4ff2-9be3-5d1bd4e8e84c"], "metadata": {"title": "A data-driven approach for distributing scarce goods within the REWE retail supply chain"}}, "c2604d42-c404-581f-a4f5-b1b2cc1ebca5": {"node_ids": ["3f29108f-7f5d-47b1-b5a2-6f864b66b4b2", "63d9510b-9880-4415-85fe-56ef411c8c26"], "metadata": {"title": "Balancing the electricity grid with multi-level forecasting models"}}, "7c5cd37a-c881-51b5-ad88-bad2af81cbb7": {"node_ids": ["80204bd9-b8c7-4947-8896-96e1b26606df", "d91faaa8-b1dc-40e5-b632-f329e6644003", "18a0c84c-a5ea-4045-aab7-e46cbc156777", "42b9a7a5-57d2-4c75-a708-4970e5a3cdc4"], "metadata": {"title": "Multimodal Product Demand Forecasting: From pixels on your screen to a meal on your plate"}}, "7afe76a0-a34b-5e97-95b5-c3192c87e543": {"node_ids": ["4b310b5f-430d-4b3a-b664-1dd04b2e798d", "182123cd-68a9-41f3-9655-fb8d24bbb0e1"], "metadata": {"title": "Minimizing the Data Mesh Mess"}}, "31d15b15-0304-58da-91d1-947c6fcb9995": {"node_ids": ["c4b93c82-f872-4e29-a54f-e7e50b98601f", "5d5ade97-5f47-4b27-8f75-35490c5c2fbe"], "metadata": {"title": "Return to Data's Inferno: are the 7 layers of data testing hell still relevant?"}}, "7bf14af7-ada1-5696-86bb-294140a478eb": {"node_ids": ["6b9886e0-6c60-421d-a4ca-e636648bd176", "0e540f0f-63f1-47ac-ba90-e6cff70d04be"], "metadata": {"title": "import full-focus as ff \u2013 How to reduce stress and pressure as a data specialist."}}, "e8e06482-a55b-5837-adc3-f7bc91be7eb0": {"node_ids": ["ae913d8e-bd36-4b2f-baa1-57a2d8ff97a1"], "metadata": {"title": "Keynote Katharine Jarmul"}}, "3c7480aa-cb34-5f97-bc1a-af7186c3eadb": {"node_ids": ["cb845b37-2146-4f82-894b-670f16762cfc", "8da78579-8972-49a3-809f-369e4c72e54a"], "metadata": {"title": "Personalization at Uber scale via causal-driven machine learning"}}, "4a325da5-4ccc-5d5d-ab16-858cd62b8c87": {"node_ids": ["058883b1-2f80-4a63-80f4-e9bc83a98dd1", "15085444-2c42-4e6f-bdea-7ae31cb08b08"], "metadata": {"title": "MLOps on the fly: Optimizing a feature store with DuckDB and ArrowFlight"}}, "e5be9c09-4c7c-5a3f-b1f8-2c7ecec74cd8": {"node_ids": ["3aa39d86-22c4-4215-8263-d4fee2f3a041", "d68f7a36-fc89-44f1-bb4e-41095158b490"], "metadata": {"title": "The proof of the pudding is in the (way of) eating: quasi-experimental methods of causal inference and their practical pitfalls"}}, "ac0cd2f6-d8ea-5e5f-8cec-57631b7fc9c8": {"node_ids": ["91be920b-9b95-4166-8430-01297783f4a2", "0682a5ba-45e7-4d57-9ad4-615a741dd16e"], "metadata": {"title": "Ok, doomer"}}, "9d38c367-284b-546b-ab29-f5fb5089392d": {"node_ids": ["7af708cb-4979-4dd4-8df9-698f2c47d474", "3e5eecc9-639a-4822-9f76-199f31e9eeeb", "258cdc1e-10b0-4ee8-9f64-396740ea3025"], "metadata": {"title": "LLM Agents 101: How I Gave ChatGPT Access to My To-Do List"}}, "4fd749c4-0c64-50f5-be8d-498d1ea5e0ea": {"node_ids": ["9a27b557-69ae-4008-9ceb-0c3f9367f62a", "6576c8f8-422f-4a55-a149-b4e1d7b6f9d1", "9113c3dd-6cb1-4bb9-8068-7ad0bfe785bb", "150168c1-0721-4cd9-b7a8-69f188bf9391", "083b9719-ba34-408d-af6f-7a9af11d0a87"], "metadata": {"title": "Fine-tuning with Finesse: Tuning LLMs with Well-curated Training Data"}}, "63aa42ea-2fc6-5bfb-92ac-11e83a7f45f8": {"node_ids": ["76f33570-5547-499c-be8c-59038873bb3e"], "metadata": {"title": "Keynote Thomas Wolf"}}, "d9ad798a-cb3b-5209-9d45-7cef2617937f": {"node_ids": ["26c19831-c98f-4435-b165-c82427c12766", "b9440585-20c6-4ebf-a222-c95195f805bd"], "metadata": {"title": "Unlocking the Black Box: A practical guide to finding an alibi for machine learning models"}}, "f71f1eae-c12e-5b4f-8902-b8d0b8a38d72": {"node_ids": ["daecb07c-9247-4bca-9dc8-89cfc0250d93", "e6dd1229-39dd-40db-b8cf-56d1c0bd49b1"], "metadata": {"title": "Building a personal search engine with llama-index"}}, "66510cf5-c4a9-5cf2-94c1-863495919590": {"node_ids": ["f1ed581d-81a0-4248-86c1-ce82e7d01cd7", "9b3c6264-77b5-41d4-838b-c1d110270fd2", "9c6d6801-bd7c-4447-9877-493db9fe92da"], "metadata": {"title": "Mastering Knowledge Graph Modeling with Neo4j: A Practical Tutorial"}}, "3b32bca8-108d-5c64-9dd4-d577b0d284f6": {"node_ids": ["f7736f86-c42f-42d4-89b1-22e85500bac8"], "metadata": {"title": "Unconference #1"}}, "89243294-cdfa-57b6-a06f-a150fc1b56b7": {"node_ids": ["ffec7b14-342c-4e0e-9fa4-ec7a01a69c1b"], "metadata": {"title": "Unconference #4"}}, "791c9c2c-69d2-5f10-bf2f-707ed3992b42": {"node_ids": ["c7d37103-da06-446a-b7f8-1edf72487406", "91b88d61-146b-414e-9d3d-b6fffac3a756", "1f14fe1f-3196-4465-92ce-ba6252be10a4"], "metadata": {"title": "Distillation Unleashed: Domain Knowledge Transfer with Compact Neural Networks"}}, "d6fbbf46-8263-5421-80ae-34972cf15788": {"node_ids": ["3562a775-839c-40c1-b4e4-44a0b945b921"], "metadata": {"title": "Data Contracts in action powered by Python open source ecosystem"}}, "0b7e549a-0c06-502d-ad0f-ec3ed74ca80f": {"node_ids": ["c0e2ce9f-fb13-49b4-a0cf-3cc7f0f90d71", "e4a70cd9-08aa-4028-8809-d84bef981628"], "metadata": {"title": "Production Data to the Model: \u201cAre You Getting My Drift?\u201d"}}, "56b64bc2-19f4-55f7-bed5-fbc4a5c50425": {"node_ids": ["c73a06fc-6db4-483d-b491-826f7b88bd91", "41910b9a-478c-44eb-879c-b12a75e9c0ae", "cc33d7ce-ed84-4b88-b3ca-fe565545de34"], "metadata": {"title": "Lets do the time warp again: time series machine learning with distance functions"}}, "86f6e0f4-9664-5e9c-af9b-addbb9375abf": {"node_ids": ["dd4673c0-a2dc-4e1e-9cf8-01ada0da8b8a"], "metadata": {"title": "Our journey using data and AI to help monitor wildlife in parks in Africa"}}, "0c8b6f48-94a7-579a-9920-00350de3f2e0": {"node_ids": ["88100b2e-1b46-45e4-958e-a01eda029839", "9a35f25a-4ff1-46e9-94b8-c33568e70981", "ae01cd15-adc8-456d-99a9-3d68cf7566ef"], "metadata": {"title": "Standby detection with a human in the loop"}}, "569b4008-5ff9-5ec2-8c88-6dfca3e583cd": {"node_ids": ["e4c9d236-16eb-4fab-9abb-fe422d1cd8be", "b760ef53-aab3-4d87-98d0-c76ba277e96f"], "metadata": {"title": "Building True Machine Learning MVPs: Avoiding Pitfalls and Validating the Value Chain"}}, "ffdd7835-3743-5f4b-99ca-ebe6cfbb21ee": {"node_ids": ["2d099a71-5487-444c-83d8-7d8dc6569ad4", "604919f9-c6c8-4222-bc6f-3dfcd5299be6", "7c68894e-4f93-44a9-b983-6a36984222a0"], "metadata": {"title": "Bayesian ranking for tennis players in PyMC"}}, "e0485fc1-fcb3-535a-98fd-85efd758a50e": {"node_ids": ["197cca09-5692-4951-80c3-e5ee9742ac36", "b59e8c16-5a9a-449c-bbb6-ec3ab66b6bc9"], "metadata": {"title": "Keynote \"Natural Intelligence is All You Need [tm]\""}}, "ce9ef2b8-e1bc-59de-8de7-8d5809253967": {"node_ids": ["6d590827-52d5-45df-9791-451711fc524e", "209ef90a-650a-4767-bed4-3ce70b9a86d4"], "metadata": {"title": "Transfer Learning in Boosting Models"}}, "3f11b211-2573-5bfb-bf12-33dfea2ab2a7": {"node_ids": ["b3f98ffe-a897-4d5a-8fca-6721fd7035f6", "73bb4c05-757c-4eb4-919e-bd7147de6206", "7657eefb-fbf1-4fdd-9518-2392754c936e"], "metadata": {"title": "Enhancing Economic Outcomes: Leveraging Business Metrics for Machine Learning Model Optimization"}}, "89e8ef97-184b-5aad-9054-447f5410a089": {"node_ids": ["f0833169-e7c7-433b-8e28-7da0c3924cc4"], "metadata": {"title": "Polars and a peek in the expression engine"}}, "73f28a4c-f282-5739-9cc5-d517ef4870d8": {"node_ids": ["8aebd03a-c249-4c02-9e06-730da20a5a0e", "f14d2c0b-5e9d-48e3-a1bf-e9fb3bfd54a4"], "metadata": {"title": "Using AI to make Amsterdam greener, safer and more accessible"}}, "a9cfdb94-6920-5b26-b93c-f9511d7aec1f": {"node_ids": ["ab2dade4-0372-4ef3-9d0f-fc3e1f92fba8", "aa636989-22d8-4fb8-8619-61fe6e8ffa7b"], "metadata": {"title": "Generating Data Frames for your test - using Pandas stratgies in Hypothesis"}}, "28ea4e75-e11b-56a7-b889-08724577cd95": {"node_ids": ["e63cad79-5195-44db-b730-34c9b94785cf", "e0cab5ab-e70e-47e2-80aa-e3879037e8d5"], "metadata": {"title": "There are no bad labels, only happy accidents"}}}}