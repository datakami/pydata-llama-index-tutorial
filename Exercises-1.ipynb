{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913d2a2-f87f-4505-abb4-49e6b2291e52",
   "metadata": {},
   "source": [
    "# Part 1: Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e084dce-bd60-43c1-a13d-cd9636ea2856",
   "metadata": {},
   "source": [
    "If you want to use the OpenAPI API during this tutorial, make a file `secret.py` with `openai_api_key = \"YOURKEYHERE\"`.\n",
    "\n",
    "How to use this notebook: \n",
    "1. Execute all cells under \"Setup\"\n",
    "2. Fill in the exercises below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb43be-6be1-4cd1-933f-cd3a77052e33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb21f09-827a-412c-8b19-c40fd14825a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd39ab-b6ba-4d69-8e8f-39f382d7712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# logging for lazy people :)\n",
    "from loguru import logger\n",
    "\n",
    "# we're not importing specific methods or classes so it's clear when we actually call llama_index!\n",
    "import llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b056809-10bc-4f4d-90bf-a9b8e8acd34e",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44daa1-a3e6-45bf-bc6e-8c84fad646b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stdout, format=\"{time} - {level} - {message}\", level=\"DEBUG\")\n",
    "logger.add(\"tutorial_part_1.log\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee221890-7faa-4b1a-b593-ffe15d5976dc",
   "metadata": {},
   "source": [
    "### Tell the notebook where files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e8b1-5b94-4dcf-b109-9e8351b087b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/pydata/schedule.json\")\n",
    "INDEX_PATH = Path(\"indices/pydata_schedule_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f10b1-759c-459b-bece-1495c099cf62",
   "metadata": {},
   "source": [
    "### Tell `llama-index` to use a local embeddings model for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420c723-b6f0-4c1f-8a5f-9da6b875747b",
   "metadata": {},
   "source": [
    "More information about this embeddings model: https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "\n",
    "Note that all-minilm-l6-v2 has a maximum size of 256 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b220f-e86c-48e6-a069-ec4d84a2b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"local:sentence-transformers/all-minilm-l6-v2\"\n",
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531cae-9ec3-47f3-b4c1-e9df4cea39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=embed_model, chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b317df6-0c8c-4ec8-beb9-ce932bc34a50",
   "metadata": {},
   "source": [
    "### Load a vector index with the PyData Amsterdam 2023 schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81545f8f-f540-4b5a-bc39-eac70da0771a",
   "metadata": {},
   "source": [
    "Load the index from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bc3e5-6bc7-4970-9a1f-c722ba931956",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede9a40-f6e1-42f3-92bf-4519bfad87ef",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b73e6",
   "metadata": {},
   "source": [
    "### Create a retriever from the vector index `index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99828da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a3b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83ba90ab",
   "metadata": {},
   "source": [
    "### Retrieve chunks that mention llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc5a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2ba35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18ae7223",
   "metadata": {},
   "source": [
    "### Retrieve chunks related to causal machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095ac6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdbaf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bfb44ff",
   "metadata": {},
   "source": [
    "### Print the talk title and speaker for results related to causal machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a26425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa7326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c955a628",
   "metadata": {},
   "source": [
    "### Create a new retriever that retrieves more than 2 results. Hint: [llama-index Retriever documentation](https://gpt-index.readthedocs.io/en/v0.8.5.post2/api_reference/query/retrievers/vector_store.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62558f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff8d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43a015c6",
   "metadata": {},
   "source": [
    "### Find all talks about causal inference at PyData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40adf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f0d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f81ee400-d0c5-4ad2-b68e-09c84ce4044c",
   "metadata": {},
   "source": [
    "## Querying the vector index with an external LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ced37-8de0-4d5e-8abb-6fae18bf1803",
   "metadata": {},
   "source": [
    "### Set OpenAI API key (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5b922-be03-435f-b581-053dfb47e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret import openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fc70c-f18f-412c-bae5-3ac06a090a92",
   "metadata": {},
   "source": [
    "### Use OpenAI's gpt-3.5-turbo for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6ce8a-85d1-4d29-a182-a749782d14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llama_index.llms.OpenAI(model=\"gpt-3.5-turbo\", api_key=openai_api_key)\n",
    "service_context = llama_index.ServiceContext.from_defaults(\n",
    "  embed_model=embed_model, chunk_size=256, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c91e61-7b94-4a1c-a68b-b22553e8b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(INDEX_PATH):\n",
    "    logger.error(\"Index file for part 1 does not exist on disk. :(\")\n",
    "else:\n",
    "    try:                                                                             \n",
    "        storage_context = llama_index.StorageContext.from_defaults(persist_dir=INDEX_PATH)\n",
    "        index = llama_index.load_index_from_storage(storage_context, service_context=service_context)\n",
    "        logger.info(\"Loaded index from local storage\")                               \n",
    "    except Exception as e:                                                           \n",
    "        logger.error(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43eb07",
   "metadata": {},
   "source": [
    "### Create a query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d3724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4d28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35bac008",
   "metadata": {},
   "source": [
    "### Find talks that might be interesting for startup founders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2cdad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508751e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
